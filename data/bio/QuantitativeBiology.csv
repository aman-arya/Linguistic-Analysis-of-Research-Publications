Title,Abstract
$\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity,"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
"
Deciphering noise amplification and reduction in open chemical reaction networks,"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
"
An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data,"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
"
Competing evolutionary paths in growing populations with applications to multidrug resistance,"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
"
Gene regulatory network inference: an introductory survey,"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
"
Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations,"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
"
Not even wrong: The spurious link between biodiversity and ecosystem functioning,"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a naïve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
"
Modeling of drug diffusion in a solid tumor leading to tumor cell death,"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
"
Blockchain and human episodic memory,"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
"
Epidemic Spreading and Aging in Temporal Networks with Memory,"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
"
Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan,"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
"
Objective Procedure for Reconstructing Couplings in Complex Systems,"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
"
Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains,"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
"
From Natural to Artificial Camouflage: Components and Systems,"  We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.
"
Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration,"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
"
Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models,"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
"
Contribution of Data Categories to Readmission Prediction Accuracy,"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
"
When Streams of Optofluidics Meet the Sea of Life,"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
"
Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs,"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
"
Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer,"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
"
Resting-state ASL : Toward an optimal sequence duration,"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
"
Unveiling ADP-binding sites and channels in respiratory complexes: Validation of Murburn concept as a holistic explanation for oxidative phosphorylation,"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
"
Highly accurate model for prediction of lung nodule malignancy with CT scans,"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
"
A global model for predicting the arrival of imported dengue infections,"  With approximately half of the world's population at risk of contracting
dengue, this mosquito-borne disease is of global concern. International
travellers significantly contribute to dengue's rapid and large-scale spread by
importing the disease from endemic into non-endemic countries. To prevent
future outbreaks and dengue from establishing in non-endemic countries,
knowledge about the arrival time and location of infected travellers is
crucial. We propose a network model that predicts the monthly number of dengue
infected air passengers arriving at any given airport. We consider
international air travel volumes, monthly dengue incidence rates and temporal
infection dynamics. Our findings shed light onto dengue importation routes and
reveal country-specific reporting rates that have been until now largely
unknown.
"
Mean squared displacement and sinuosity of three-dimensional random search movements,"  Correlated random walks (CRW) have been used for a long time as a null model
for animal's random search movement in two dimensions (2D). An increasing
number of studies focus on animals' movement in three dimensions (3D), but the
key properties of CRW, such as the way the mean squared displacement is related
to the path length, are well known only in 1D and 2D. In this paper I derive
such properties for 3D CRW, in a consistent way with the expression of these
properties in 2D. This should allow 3D CRW to act as a null model when
analyzing actual 3D movements similarly to what is done in 2D
"
Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM,"  Many real-world data sets, especially in biology, are produced by highly
multivariate and nonlinear complex dynamical systems. In this paper, we focus
on brain imaging data, including both calcium imaging and functional MRI data.
Standard vector-autoregressive models are limited by their linearity
assumptions, while nonlinear general-purpose, large-scale temporal models, such
as LSTM networks, typically require large amounts of training data, not always
readily available in biological applications; furthermore, such models have
limited interpretability. We introduce here a novel approach for learning a
nonlinear differential equation model aimed at capturing brain dynamics.
Specifically, we propose a variable-projection optimization approach to
estimate the parameters of the multivariate (coupled) van der Pol oscillator,
and demonstrate that such a model can accurately represent nonlinear dynamics
of the brain data. Furthermore, in order to improve the predictive accuracy
when forecasting future brain-activity time series, we use this analytical
model as an unlimited source of simulated data for pretraining LSTM; such
model-specific data augmentation approach consistently improves LSTM
performance on both calcium and fMRI imaging data.
"
Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field,"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
"
ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"  Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.
"
Human peripheral blur is optimal for object recognition,"  Our eyes sample a disproportionately large amount of information at the
centre of gaze with increasingly sparse sampling into the periphery. This
sampling scheme is widely believed to be a wiring constraint whereby high
resolution at the centre is achieved by sacrificing spatial acuity in the
periphery. Here we propose that this sampling scheme may be optimal for object
recognition because the relevant spatial content is dense near an object and
sparse in the surrounding vicinity. We tested this hypothesis by training deep
convolutional neural networks on full-resolution and foveated images. Our main
finding is that networks trained on images with foveated sampling show better
object classification compared to networks trained on full resolution images.
Importantly, blurring images according to the human blur function yielded the
best performance compared to images with shallower or steeper blurring. Taken
together our results suggest that, peripheral blurring in our eyes may have
evolved for optimal object recognition, rather than merely to satisfy wiring
constraints.
"
Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment,"  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
"
Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca,"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
"
A reproducible effect size is more useful than an irreproducible hypothesis test to analyze high throughput sequencing datasets,"  Motivation: P values derived from the null hypothesis significance testing
framework are strongly affected by sample size, and are known to be
irreproducible in underpowered studies, yet no suitable replacement has been
proposed. Results: Here we present implementations of non-parametric
standardized median effect size estimates, dNEF, for high-throughput sequencing
datasets. Case studies are shown for transcriptome and tag-sequencing datasets.
The dNEF measure is shown to be more repro- ducible and robust than P values
and requires sample sizes as small as 3 to reproducibly identify differentially
abundant features. Availability: Source code and binaries freely available at:
this https URL, omicplotR, and
this https URL.
"
Temporal processing and context dependency in C. elegans mechanosensation,"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
"
Density estimation on small datasets,"  How might a smooth probability distribution be estimated, with accurately
quantified uncertainty, from a limited amount of sampled data? Here we describe
a field-theoretic approach that addresses this problem remarkably well in one
dimension, providing an exact nonparametric Bayesian posterior without relying
on tunable parameters or large-data approximations. Strong non-Gaussian
constraints, which require a non-perturbative treatment, are found to play a
major role in reducing distribution uncertainty. A software implementation of
this method is provided.
"
Functional importance of noise in neuronal information processing,"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
"
Exact partial information decompositions for Gaussian systems based on dependency constraints,"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
"
Landau-Ginzburg theory of cortex dynamics: Scale-free avalanches emerge at the edge of synchronization,"  Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
"
How Many Subpopulations is Too Many? Exponential Lower Bounds for Inferring Population Histories,"  Reconstruction of population histories is a central problem in population
genetics. Existing coalescent-based methods, like the seminal work of Li and
Durbin (Nature, 2011), attempt to solve this problem using sequence data but
have no rigorous guarantees. Determining the amount of data needed to correctly
reconstruct population histories is a major challenge. Using a variety of tools
from information theory, the theory of extremal polynomials, and approximation
theory, we prove new sharp information-theoretic lower bounds on the problem of
reconstructing population structure -- the history of multiple subpopulations
that merge, split and change sizes over time. Our lower bounds are exponential
in the number of subpopulations, even when reconstructing recent histories. We
demonstrate the sharpness of our lower bounds by providing algorithms for
distinguishing and learning population histories with matching dependence on
the number of subpopulations.
"
Learning to attend in a brain-inspired deep neural network,"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
"
Multiplex core-periphery organization of the human connectome,"  The behavior of many complex systems is determined by a core of densely
interconnected units. While many methods are available to identify the core of
a network when connections between nodes are all of the same type, a principled
approach to define the core when multiple types of connectivity are allowed is
still lacking. Here we introduce a general framework to define and extract the
core-periphery structure of multi-layer networks by explicitly taking into
account the connectivity of the nodes at each layer. We show how our method
works on synthetic networks with different size, density, and overlap between
the cores at the different layers. We then apply the method to multiplex brain
networks whose layers encode information both on the anatomical and the
functional connectivity among regions of the human cortex. Results confirm the
presence of the main known hubs, but also suggest the existence of novel brain
core regions that have been discarded by previous analysis which focused
exclusively on the structural layer. Our work is a step forward in the
identification of the core of the human connectome, and contributes to shed
light to a fundamental question in modern neuroscience.
"
BOLD5000: A public fMRI dataset of 5000 images,"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
"
Protein Folding and Machine Learning: Fundamentals,"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
"
Community structure detection and evaluation during the pre- and post-ictal hippocampal depth recordings,"  Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
"
Solving constraint-satisfaction problems with distributed neocortical-like neuronal networks,"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
"
Estimation of Relationship between Stimulation Current and Force Exerted during Isometric Contraction,"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
"
Kinetics of Protein-DNA Interactions: First-Passage Analysis,"  All living systems can function only far away from equilibrium, and for this
reason chemical kinetic methods are critically important for uncovering the
mechanisms of biological processes. Here we present a new theoretical method of
investigating dynamics of protein-DNA interactions, which govern all major
biological processes. It is based on a first-passage analysis of biochemical
and biophysical transitions, and it provides a fully analytic description of
the processes. Our approach is explained for the case of a single protein
searching for a specific binding site on DNA. In addition, the application of
the method to investigations of the effect of DNA sequence heterogeneity, and
the role multiple targets and traps in the protein search dynamics are
discussed.
"
Exact Inference of Causal Relations in Dynamical Systems,"  From philosophers of ancient times to modern economists, biologists and other
researchers are engaged in revealing causal relations. The most challenging
problem is inferring the type of the causal relationship: whether it is uni- or
bi-directional or only apparent - implied by a hidden common cause only. Modern
technology provides us tools to record data from complex systems such as the
ecosystem of our planet or the human brain, but understanding their functioning
needs detection and distinction of causal relationships of the system
components without interventions. Here we present a new method, which
distinguishes and assigns probabilities to the presence of all the possible
causal relations between two or more time series from dynamical systems. The
new method is validated on synthetic datasets and applied to EEG
(electroencephalographic) data recorded in epileptic patients. Given the
universality of our method, it may find application in many fields of science.
"
"Thermoregulation in mice, rats and humans: An insight into the evolution of human hairlessness","  The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
"
Higher order molecular organisation as a source of biological function,"  Molecular interactions have widely been modelled as networks. The local
wiring patterns around molecules in molecular networks are linked with their
biological functions. However, networks model only pairwise interactions
between molecules and cannot explicitly and directly capture the higher order
molecular organisation, such as protein complexes and pathways. Hence, we ask
if hypergraphs (hypernetworks), that directly capture entire complexes and
pathways along with protein-protein interactions (PPIs), carry additional
functional information beyond what can be uncovered from networks of pairwise
molecular interactions. The mathematical formalism of a hypergraph has long
been known, but not often used in studying molecular networks due to the lack
of sophisticated algorithms for mining the underlying biological information
hidden in the wiring patterns of molecular systems modelled as hypernetworks.
We propose a new, multi-scale, protein interaction hypernetwork model that
utilizes hypergraphs to capture different scales of protein organization,
including PPIs, protein complexes and pathways. In analogy to graphlets, we
introduce hypergraphlets, small, connected, non-isomorphic, induced
sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these
multi-scale molecular hypergraphs and to mine them for new biological
information. We apply them to model the multi-scale protein networks of baker
yeast and human and show that the higher order molecular organisation captured
by these hypergraphs is strongly related to the underlying biology.
Importantly, we demonstrate that our new models and data mining tools reveal
different, but complementary biological information compared to classical PPI
networks. We apply our hypergraphlets to successfully predict biological
functions of uncharacterised proteins.
"
Why Abeta42 Is Much More Toxic Than Abeta40,"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
"
FRET-based nanocommunication with luciferase and channelrhodopsin molecules for in-body medical systems,"  The paper is concerned with an in-body system gathering data for medical
purposes. It is focused on communication between the following two components
of the system: liposomes gathering the data inside human veins and a detector
collecting the data from liposomes. Foerster Resonance Energy Transfer (FRET)
is considered as a mechanism for communication between the system components.
The usage of bioluminescent molecules as an energy source for generating FRET
signals is suggested and the performance evaluation of this approach is given.
FRET transmission may be initiated without an aid of an external laser, which
is crucial in case of communication taking place inside of human body. It is
also shown how to solve the problem of FRET signals recording. The usage of
channelrhodopsin molecules, able to receive FRET signals and convert them into
voltage, is proposed. The communication system is modelled with molecular
structures and spectral characteristics of the proposed molecules and further
validated by using Monte Carlo computer simulations, calculating the data
throughput and the bit error rate.
"
Gene regulatory networks: a primer in biological processes and statistical modelling,"  Modelling gene regulatory networks not only requires a thorough understanding
of the biological system depicted but also the ability to accurately represent
this system from a mathematical perspective. Throughout this chapter, we aim to
familiarise the reader with the biological processes and molecular factors at
play in the process of gene expression regulation.We first describe the
different interactions controlling each step of the expression process, from
transcription to mRNA and protein decay. In the second section, we provide
statistical tools to accurately represent this biological complexity in the
form of mathematical models. Amongst other considerations, we discuss the
topological properties of biological networks, the application of deterministic
and stochastic frameworks and the quantitative modelling of regulation. We
particularly focus on the use of such models for the simulation of expression
data that can serve as a benchmark for the testing of network inference
algorithms.
"
Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study,"  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the ""RelATive cEntrality"" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other ""black box"" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
"
Activit{é} motrice des truies en groupes dans les diff{é}rents syst{è}mes de logement,"  Assessment of the motor activity of group-housed sows in commercial farms.
The objective of this study was to specify the level of motor activity of
pregnant sows housed in groups in different housing systems. Eleven commercial
farms were selected for this study. Four housing systems were represented:
small groups of five to seven sows (SG), free access stalls (FS) with exercise
area, electronic sow feeder with a stable group (ESFsta) or a dynamic group
(ESFdyn). Ten sows in mid-gestation were observed in each farm. The
observations of motor activity were made for 6 hours at the first meal or at
the start of the feeding sequence, two consecutive days and at regular
intervals of 4 minutes. The results show that the motor activity of
group-housed sows depends on the housing system. The activity is higher with
the ESFdyn system (standing: 55.7%), sows are less active in the SG system
(standing: 26.5%), and FS system is intermediate. The distance traveled by sows
in ESF system is linked to a larger area available. Thus, sows travel an
average of 362 m $\pm$ 167 m in the ESFdyn system with an average available
surface of 446 m${}^2$ whereas sows in small groups travel 50 m $\pm$ 15 m for
15 m${}^2$ available.
"
Phylogenetic networks that are their own fold-ups,"  Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
"
A Lattice Model of Charge-Pattern-Dependent Polyampholyte Phase Separation,"  In view of recent intense experimental and theoretical interests in the
biophysics of liquid-liquid phase separation (LLPS) of intrinsically disordered
proteins (IDPs), heteropolymer models with chain molecules configured as
self-avoiding walks on the simple cubic lattice are constructed to study how
phase behaviors depend on the sequence of monomers along the chains. To address
pertinent general principles, we focus primarily on two fully charged
50-monomer sequences with significantly different charge patterns. Each monomer
in our models occupies a single lattice site and all monomers interact via a
screened pairwise Coulomb potential. Phase diagrams are obtained by extensive
Monte Carlo sampling performed at multiple temperatures on ensembles of 300
chains in boxes of sizes ranging from $52\times 52\times 52$ to $246\times
246\times 246$ to simulate a large number of different systems with the overall
polymer volume fraction $\phi$ in each system varying from $0.001$ to $0.1$.
Phase separation in the model systems is characterized by the emergence of a
large cluster connected by inter-monomer nearest-neighbor lattice contacts and
by large fluctuations in local polymer density. The simulated critical
temperatures, $T_{\rm cr}$, of phase separation for the two sequences differ
significantly, whereby the sequence with a more ""blocky"" charge pattern
exhibits a substantially higher propensity to phase separate. The trend is
consistent with our sequence-specific random-phase-approximation (RPA) polymer
theory, but the variation of the simulated $T_{\rm cr}$ with a previously
proposed ""sequence charge decoration"" pattern parameter is milder than that
predicted by RPA. Ramifications of our findings for the development of
analytical theory and simulation protocols of IDP LLPS are discussed.
"
Membrane Trafficking in the Yeast Saccharomyces cerevisiae Model,"  The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{ü}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
"
Quantum Structures in Human Decision-making: Towards Quantum Expected Utility,"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
"
Efficient algorithms to discover alterations with complementary functional association in cancer,"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
"
Causal Mediation Analysis Leveraging Multiple Types of Summary Statistics Data,"  Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails.
"
Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks,"  Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
"
Trail-Mediated Self-Interaction,"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
"
Dihedral angle prediction using generative adversarial networks,"  Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
"
"Pinned, locked, pushed, and pulled traveling waves in structured environments","  Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
"
Bounds on the expected size of the maximum agreement subtree for a given tree shape,"  We show that the expected size of the maximum agreement subtree of two
$n$-leaf trees, uniformly random among all trees with the shape, is
$\Theta(\sqrt{n})$. To derive the lower bound, we prove a global structural
result on a decomposition of rooted binary trees into subgroups of leaves
called blobs. To obtain the upper bound, we generalize a first moment argument
for random tree distributions that are exchangeable and not necessarily
sampling consistent.
"
"Neurofeedback: principles, appraisal and outstanding issues","  Neurofeedback is a form of brain training in which subjects are fed back
information about some measure of their brain activity which they are
instructed to modify in a way thought to be functionally advantageous. Over the
last twenty years, NF has been used to treat various neurological and
psychiatric conditions, and to improve cognitive function in various contexts.
However, despite its growing popularity, each of the main steps in NF comes
with its own set of often covert assumptions. Here we critically examine some
conceptual and methodological issues associated with the way general objectives
and neural targets of NF are defined, and review the neural mechanisms through
which NF may act, and the way its efficacy is gauged. The NF process is
characterised in terms of functional dynamics, and possible ways in which it
may be controlled are discussed. Finally, it is proposed that improving NF will
require better understanding of various fundamental aspects of brain dynamics
and a more precise definition of functional brain activity and brain-behaviour
relationships.
"
Linear compartmental models: input-output equations and operations that preserve identifiability,"  This work focuses on the question of how identifiability of a mathematical
model, that is, whether parameters can be recovered from data, is related to
identifiability of its submodels. We look specifically at linear compartmental
models and investigate when identifiability is preserved after adding or
removing model components. In particular, we examine whether identifiability is
preserved when an input, output, edge, or leak is added or deleted. Our
approach, via differential algebra, is to analyze specific input-output
equations of a model and the Jacobian of the associated coefficient map. We
clarify a prior determinantal formula for these equations, and then use it to
prove that, under some hypotheses, a model's input-output equations can be
understood in terms of certain submodels we call ""output-reachable"". Our proofs
use algebraic and combinatorial techniques.
"
Eigensolutions and spectral analysis of a model for vertical gene transfer of plasmids,"  Plasmids are autonomously replicating genetic elements in bacteria. At cell
division plasmids are distributed among the two daughter cells. This gene
transfer from one generation to the next is called vertical gene transfer. We
study the dynamics of a bacterial population carrying plasmids and are in
particular interested in the long-time distribution of plasmids. Starting with
a model for a bacterial population structured by the discrete number of
plasmids, we proceed to the continuum limit in order to derive a continuous
model. The model incorporates plasmid reproduction, division and death of
bacteria, and distribution of plasmids at cell division. It is a hyperbolic
integro-differential equation and a so-called growth-fragmentation-death model.
As we are interested in the long-time distribution of plasmids we study the
associated eigenproblem and show existence of eigensolutions. The stability of
this solution is studied by analyzing the spectrum of the integro-differential
operator given by the eigenproblem. By relating the spectrum with the spectrum
of an integral operator we find a simple real dominating eigenvalue with a
non-negative corresponding eigenfunction. Moreover, we describe an iterative
method for the numerical construction of the eigenfunction.
"
Spatial analysis of airborne laser scanning point clouds for predicting forest variables,"  With recent developments in remote sensing technologies, plot-level forest
resources can be predicted utilizing airborne laser scanning (ALS). The
prediction is often assisted by mostly vertical summaries of the ALS point
clouds. We present a spatial analysis of the point cloud by studying the
horizontal distribution of the pulse returns through canopy height models
thresholded at different height levels. The resulting patterns of patches of
vegetation and gabs on each layer are summarized to spatial ALS features. We
propose new features based on the Euler number, which is the number of patches
minus the number of gaps, and the empty-space function, which is a spatial
summary function of the gab space. The empty-space function is also used to
describe differences in the gab structure between two different layers. We
illustrate usefulness of the proposed spatial features for predicting different
forest variables that summarize the spatial structure of forests or their
breast height diameter distribution. We employ the proposed spatial features,
in addition to commonly used features from literature, in the well-known k-nn
estimation method to predict the forest variables. We present the methodology
on the example of a study site in Central Finland.
"
Flux cost functions and the choice of metabolic fluxes,"  Metabolic fluxes in cells are governed by physical, biochemical,
physiological, and economic principles. Cells may show ""economical"" behaviour,
trading metabolic performance against the costly side-effects of high enzyme or
metabolite concentrations. Some constraint-based flux prediction methods score
fluxes by heuristic flux costs as proxies of enzyme investments. However,
linear cost functions ignore enzyme kinetics and the tight coupling between
fluxes, metabolite levels and enzyme levels. To derive more realistic cost
functions, I define an apparent ""enzymatic flux cost"" as the minimal enzyme
cost at which the fluxes can be realised in a given kinetic model, and a
""kinetic flux cost"", which includes metabolite cost. I discuss the mathematical
properties of such flux cost functions, their usage for flux prediction, and
their importance for cells' metabolic strategies. The enzymatic flux cost
scales linearly with the fluxes and is a concave function on the flux polytope.
The costs of two flows are usually not additive, due to an additional
""compromise cost"". Between flux polytopes, where fluxes change their
directions, the enzymatic cost shows a jump. With strictly concave flux cost
functions, cells can reduce their enzymatic cost by running different fluxes in
different cell compartments or at different moments in time. The enzymactic
flux cost can be translated into an approximated cell growth rate, a convex
function on the flux polytope. Growth-maximising metabolic states can be
predicted by Flux Cost Minimisation (FCM), a variant of FBA based on general
flux cost functions. The solutions are flux distributions in corners of the
flux polytope, i.e. typically elementary flux modes. Enzymatic flux costs can
be linearly or nonlinearly approximated, providing model parameters for linear
FBA based on kinetic parameters and extracellular concentrations, and justified
by a kinetic model.
"
Self-sustained activity in balanced networks with low firing-rate,"  The brain can display self-sustained activity (SSA), which is the persistent
firing of neurons in the absence of external stimuli. This spontaneous activity
shows low neuronal firing rates and is observed in diverse in vitro and in vivo
situations. In this work, we study the influence of excitatory/inhibitory
balance, connection density, and network size on the self-sustained activity of
a neuronal network model. We build a random network of adaptive exponential
integrate-and-fire (AdEx) neuron models connected through inhibitory and
excitatory chemical synapses. The AdEx model mimics several behaviours of
biological neurons, such as spike initiation, adaptation, and bursting
patterns. In an excitation/inhibition balanced state, if the mean connection
degree (K) is fixed, the firing rate does not depend on the network size (N),
whereas for fixed N, the firing rate decreases when K increases. However, for
large K, SSA states can appear only for large N. We show the existence of SSA
states with similar behaviours to those observed in experimental recordings,
such as very low and irregular neuronal firing rates, and spike-train power
spectra with slow fluctuations, only for balanced networks of large size.
"
Global stability of a network-based SIRS epidemic model with nonmonotone incidence rate,"  This paper studies the dynamics of a network-based SIRS epidemic model with
vaccination and a nonmonotone incidence rate. This type of nonlinear incidence
can be used to describe the psychological or inhibitory effect from the
behavioral change of the susceptible individuals when the number of infective
individuals on heterogeneous networks is getting larger. Using the analytical
method, epidemic threshold $R_0$ is obtained. When $R_0$ is less than one, we
prove the disease-free equilibrium is globally asymptotically stable and the
disease dies out, while $R_0$ is greater than one, there exists a unique
endemic equilibrium. By constructing a suitable Lyapunov function, we also
prove the endemic equilibrium is globally asymptotically stable if the
inhibitory factor $\alpha$ is sufficiently large. Numerical experiments are
also given to support the theoretical results. It is shown both theoretically
and numerically a larger $\alpha$ can accelerate the extinction of the disease
and reduce the level of disease.
"
Generalizations of the 'Linear Chain Trick': Incorporating more flexible dwell time distributions into mean field ODE models,"  Mathematical modelers have long known of a ""rule of thumb"" referred to as the
Linear Chain Trick (LCT; aka the Gamma Chain Trick): a technique used to
construct mean field ODE models from continuous-time stochastic state
transition models where the time an individual spends in a given state (i.e.,
the dwell time) is Erlang distributed (i.e., gamma distributed with integer
shape parameter). Despite the LCT's widespread use, we lack general theory to
facilitate the easy application of this technique, especially for complex
models. This has forced modelers to choose between constructing ODE models
using heuristics with oversimplified dwell time assumptions, using time
consuming derivations from first principles, or to instead use non-ODE models
(like integro-differential equations or delay differential equations) which can
be cumbersome to derive and analyze. Here, we provide analytical results that
enable modelers to more efficiently construct ODE models using the LCT or
related extensions. Specifically, we 1) provide novel extensions of the LCT to
various scenarios found in applications; 2) provide formulations of the LCT and
it's extensions that bypass the need to derive ODEs from integral or stochastic
model equations; and 3) introduce a novel Generalized Linear Chain Trick (GLCT)
framework that extends the LCT to a much broader family of distributions,
including the flexible phase-type distributions which can approximate
distributions on $\mathbb{R}^+$ and be fit to data. These results give modelers
more flexibility to incorporate appropriate dwell time assumptions into mean
field ODEs, including conditional dwell time distributions, and these results
help clarify connections between individual-level stochastic model assumptions
and the structure of corresponding mean field ODEs.
"
Microfluidics for Chemical Synthesis: Flow Chemistry,"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
"
Global Sensitivity Analysis of High Dimensional Neuroscience Models: An Example of Neurovascular Coupling,"  The complexity and size of state-of-the-art cell models have significantly
increased in part due to the requirement that these models possess complex
cellular functions which are thought--but not necessarily proven--to be
important. Modern cell models often involve hundreds of parameters; the values
of these parameters come, more often than not, from animal experiments whose
relationship to the human physiology is weak with very little information on
the errors in these measurements. The concomitant uncertainties in parameter
values result in uncertainties in the model outputs or Quantities of Interest
(QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual
parameters (or sets of parameters) their relative contribution to output
uncertainty thereby introducing a measure of influence or importance of said
parameters. New GSA approaches are required to deal with increased model size
and complexity; a three stage methodology consisting of screening (dimension
reduction), surrogate modeling, and computing Sobol' indices, is presented. The
methodology is used to analyze a physiologically validated numerical model of
neurovascular coupling which possess 160 uncertain parameters. The sensitivity
analysis investigates three quantities of interest (QoIs), the average value of
$K^+$ in the extracellular space, the average volumetric flow rate through the
perfusing vessel, and the minimum value of the actin/myosin complex in the
smooth muscle cell. GSA provides a measure of the influence of each parameter,
for each of the three QoIs, giving insight into areas of possible physiological
dysfunction and areas of further investigation.
"
Classification of grasping tasks based on EEG-EMG coherence,"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
"
Methodological variations in lagged regression for detecting physiologic drug effects in EHR data,"  We studied how lagged linear regression can be used to detect the physiologic
effects of drugs from data in the electronic health record (EHR). We
systematically examined the effect of methodological variations ((i) time
series construction, (ii) temporal parameterization, (iii) intra-subject
normalization, (iv) differencing (lagged rates of change achieved by taking
differences between consecutive measurements), (v) explanatory variables, and
(vi) regression models) on performance of lagged linear methods in this
context. We generated two gold standards (one knowledge-base derived, one
expert-curated) for expected pairwise relationships between 7 drugs and 4 labs,
and evaluated how the 64 unique combinations of methodological perturbations
reproduce gold standards. Our 28 cohorts included patients in Columbia
University Medical Center/NewYork-Presbyterian Hospital clinical database. The
most accurate methods achieved AUROC of 0.794 for knowledge-base derived gold
standard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%
CI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],
expert-curated gold standard) across all methods that re-parameterize time
according to sequence and use either a joint autoregressive model with
differencing or an independent lag model without differencing. The complement
of this set of methods achieved a mean AUROC close to 0.5, indicating the
importance of these choices. We conclude that time- series analysis of EHR data
will likely rely on some of the beneficial pre-processing and modeling
methodologies identified, and will certainly benefit from continued careful
analysis of methodological perturbations. This study found that methodological
variations, such as pre-processing and representations, significantly affect
results, exposing the importance of evaluating these components when comparing
machine-learning methods.
"
Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification,"  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
"
Crawling migration under chemical signalling: a stochastic particle model,"  Cell migration is a fundamental process involved in physiological phenomena
such as the immune response and morphogenesis, but also in pathological
processes, such as the development of tumor metastasis. These functions are
effectively ensured because cells are active systems that adapt to their
environment. In this work, we consider a migrating cell as an active particle,
where its intracellular activity is responsible for motion. Such system was
already modeled in a previous model where the protrusion activity of the cell
was described by a stochastic Markovian jump process. The model was proven able
to capture the diversity in observed trajectories. Here, we add a description
of the effect of an external chemical attractive signal on the protrusion
dynamics, that may vary in time. We show that the resulting stochastic model is
a well-posed non-homogeneous Markovian process, and provide cell trajectories
in different settings, illustrating the effects of the signal on long-term
trajectories.
"
Universal kinetics for engagement of mechanosensing pathways in cell adhesion,"  When plated onto substrates, cell morphology and even stem cell
differentiation are influenced by the stiffness of their environment. Stiffer
substrates give strongly spread (eventually polarized) cells with strong focal
adhesions, and stress fibers; very soft substrates give a less developed
cytoskeleton, and much lower cell spreading. The kinetics of this process of
cell spreading is studied extensively, and important universal relationships
are established on how the cell area grows with time. Here we study the
population dynamics of spreading cells, investigating the characteristic
processes involved in cell response to the substrate. We show that unlike the
individual cell morphology, this population dynamics does not depend on the
substrate stiffness. Instead, a strong activation temperature dependence is
observed. Different cell lines on different substrates all have long-time
statistics controlled by the thermal activation over a single energy barrier
dG=19 kcal/mol, while the early-time kinetics follows a power law $t^5$. This
implies that the rate of spreading depends on an internal process of
adhesion-mechanosensing complex assembly and activation: the operational
complex must have 5 component proteins, and the last process in the sequence
(which we believe is the activation of focal adhesion kinase) is controlled by
the binding energy dG.
"
Computational and informatics advances for reproducible data analysis in neuroimaging,"  The reproducibility of scientific research has become a point of critical
concern. We argue that openness and transparency are critical for
reproducibility, and we outline an ecosystem for open and transparent science
that has emerged within the human neuroimaging community. We discuss the range
of open data sharing resources that have been developed for neuroimaging data,
and the role of data standards (particularly the Brain Imaging Data Structure)
in enabling the automated sharing, processing, and reuse of large neuroimaging
datasets. We outline how the open-source Python language has provided the basis
for a data science platform that enables reproducible data analysis and
visualization. We also discuss how new advances in software engineering, such
as containerization, provide the basis for greater reproducibility in data
analysis. The emergence of this new ecosystem provides an example for many
areas of science that are currently struggling with reproducibility.
"
Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography,"  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule
structure inside single cells. Macromolecule classification approaches based on
convolutional neural networks (CNN) were developed to separate millions of
macromolecules captured from ECT systematically. However, given the fast
accumulation of ECT data, it will soon become necessary to use CNN models to
efficiently and accurately separate substantially more macromolecules at the
prediction stage, which requires additional computational costs. To speed up
the prediction, we compress classification models into compact neural networks
with little in accuracy for deployment. Specifically, we propose to perform
model compression through knowledge distillation. Firstly, a complex teacher
network is trained to generate soft labels with better classification
feasibility followed by training of customized student networks with simple
architectures using the soft label to compress model complexity. Our tests
demonstrate that our compressed models significantly reduce the number of
parameters and time cost while maintaining similar classification accuracy.
"
Stochastic Chemical Reaction Networks for Robustly Approximating Arbitrary Probability Distributions,"  We show that discrete distributions on the $d$-dimensional non-negative
integer lattice can be approximated arbitrarily well via the marginals of
stationary distributions for various classes of stochastic chemical reaction
networks. We begin by providing a class of detailed balanced networks and prove
that they can approximate any discrete distribution to any desired accuracy.
However, these detailed balanced constructions rely on the ability to
initialize a system precisely, and are therefore susceptible to perturbations
in the initial conditions. We therefore provide another construction based on
the ability to approximate point mass distributions and prove that this
construction is capable of approximating arbitrary discrete distributions for
any choice of initial condition. In particular, the developed models are
ergodic, so their limit distributions are robust to a finite number of
perturbations over time in the counts of molecules.
"
Early Salient Region Selection Does Not Drive Rapid Visual Categorization,"  The current dominant visual processing paradigm in both human and machine
research is the feedforward, layered hierarchy of neural-like processing
elements. Within this paradigm, visual saliency is seen by many to have a
specific role, namely that of early selection. Early selection is thought to
enable very fast visual performance by limiting processing to only the most
relevant candidate portions of an image. Though this strategy has indeed led to
improved processing time efficiency in machine algorithms, at least one set of
critical tests of this idea has never been performed with respect to the role
of early selection in human vision. How would the best of the current saliency
models perform on the stimuli used by experimentalists who first provided
evidence for this visual processing paradigm? Would the algorithms really
provide correct candidate sub-images to enable fast categorization on those
same images? Here, we report on a new series of tests of these questions whose
results suggest that it is quite unlikely that such an early selection process
has any role in human rapid visual categorization.
"
A unifying framework for the modelling and analysis of STR DNA samples arising in forensic casework,"  This paper presents a new framework for analysing forensic DNA samples using
probabilistic genotyping. Specifically it presents a mathematical framework for
specifying and combining the steps in producing forensic casework
electropherograms of short tandem repeat loci from DNA samples. It is
applicable to both high and low template DNA samples, that is, samples
containing either high or low amounts DNA. A specific model is developed within
the framework, by way of particular modelling assumptions and approximations,
and its interpretive power presented on examples using simulated data and data
from a publicly available dataset. The framework relies heavily on the use of
univariate and multivariate probability generating functions. It is shown that
these provide a succinct and elegant mathematical scaffolding to model the key
steps in the process. A significant development in this paper is that of new
numerical methods for accurately and efficiently evaluating the probability
distribution of amplicons arising from the polymerase chain reaction process,
which is modelled as a discrete multi-type branching process. Source code in
the scripting languages Python, R and Julia is provided for illustration of
these methods. These new developments will be of general interest to persons
working outside the province of forensic DNA interpretation that this paper
focuses on.
"
Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction,"  The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
"
Towards Gene Expression Convolutions using Gene Interaction Graphs,"  We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
"
Dynamic constraints on activity and connectivity during the learning of value,"  Human learning is a complex process in which future behavior is altered via
the modulation of neural activity. Yet, the degree to which brain activity and
functional connectivity during learning is constrained across subjects, for
example by conserved anatomy and physiology or by the nature of the task,
remains unknown. Here, we measured brain activity and functional connectivity
in a longitudinal experiment in which healthy adult human participants learned
the values of novel objects over the course of four days. We assessed the
presence of constraints on activity and functional connectivity using an
inter-subject correlation approach. Constraints on activity and connectivity
were greater in magnitude than expected in a non-parametric permutation-based
null model, particularly in primary sensory and motor systems, as well as in
regions associated with the learning of value. Notably, inter-subject
connectivity in activity and connectivity displayed marked temporal variations,
with inter-subject correlations in activity exceeding those in connectivity
during early learning and \emph{visa versa} in later learning. Finally,
individual differences in performance accuracy tracked the degree to which a
subject's connectivity, but not activity, tracked subject-general patterns.
Taken together, our results support the notion that brain activity and
connectivity are constrained across subjects in early learning, with
constraints on activity, but not connectivity, decreasing in later learning.
"
Kinky DNA in solution: Small angle scattering study of a nucleosome positioning sequence,"  DNA is a flexible molecule, but the degree of its flexibility is subject to
debate. The commonly-accepted persistence length of $l_p \approx 500\,$\AA\ is
inconsistent with recent studies on short-chain DNA that show much greater
flexibility but do not probe its origin. We have performed X-ray and neutron
small-angle scattering on a short DNA sequence containing a strong nucleosome
positioning element, and analyzed the results using a modified Kratky-Porod
model to determine possible conformations. Our results support a hypothesis
from Crick and Klug in 1975 that some DNA sequences in solution can have sharp
kinks, potentially resolving the discrepancy. Our conclusions are supported by
measurements on a radiation-damaged sample, where single-strand breaks lead to
increased flexibility and by an analysis of data from another sequence, which
does not have kinks, but where our method can detect a locally enhanced
flexibility due to an $AT$-domain.
"
Susceptibility of Methicillin Resistant Staphylococcus aureus to Vancomycin using Liposomal Drug Delivery System,"  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
"
Fast Nonconvex Deconvolution of Calcium Imaging Data,"  Calcium imaging data promises to transform the field of neuroscience by
making it possible to record from large populations of neurons simultaneously.
However, determining the exact moment in time at which a neuron spikes, from a
calcium imaging data set, amounts to a non-trivial deconvolution problem which
is of critical importance for downstream analyses. While a number of
formulations have been proposed for this task in the recent literature, in this
paper we focus on a formulation recently proposed in Jewell and Witten (2017)
which has shown initial promising results. However, this proposal is slow to
run on fluorescence traces of hundreds of thousands of timesteps.
Here we develop a much faster online algorithm for solving the optimization
problem of Jewell and Witten (2017) that can be used to deconvolve a
fluorescence trace of 100,000 timesteps in less than a second. Furthermore,
this algorithm overcomes a technical challenge of Jewell and Witten (2017) by
avoiding the occurrence of so-called ""negative"" spikes. We demonstrate that
this algorithm has superior performance relative to existing methods for spike
deconvolution on calcium imaging datasets that were recently released as part
of the spikefinder challenge (this http URL).
Our C++ implementation, along with R and python wrappers, is publicly
available on Github at this https URL.
"
Interactions mediated by a public good transiently increase cooperativity in growing Pseudomonas putida metapopulations,"  Bacterial communities have rich social lives. A well-established interaction
involves the exchange of a public good in Pseudomonas populations, where the
iron-scavenging compound pyoverdine, synthesized by some cells, is shared with
the rest. Pyoverdine thus mediates interactions between producers and
non-producers and can constitute a public good. This interaction is often used
to test game theoretical predictions on the ""social dilemma"" of producers. Such
an approach, however, underestimates the impact of specific properties of the
public good, for example consequences of its accumulation in the environment.
Here, we experimentally quantify costs and benefits of pyoverdine production in
a specific environment, and build a model of population dynamics that
explicitly accounts for the changing significance of accumulating pyoverdine as
chemical mediator of social interactions. The model predicts that, in an
ensemble of growing populations (metapopulation) with different initial
producer fractions (and consequently pyoverdine contents), the global producer
fraction initially increases. Because the benefit of pyoverdine declines at
saturating concentrations, the increase need only be transient. Confirmed by
experiments on metapopulations, our results show how a changing benefit of a
public good can shape social interactions in a bacterial population.
"
Heterogeneous inputs to central pattern generators can shape insect gaits,"  In our previous work, we studied an interconnected bursting neuron model for
insect locomotion, and its corresponding phase oscillator model, which at high
speed can generate stable tripod gaits with three legs off the ground
simultaneously in swing, and at low speed can generate stable tetrapod gaits
with two legs off the ground simultaneously in swing. However, at low speed
several other stable locomotion patterns, that are not typically observed as
insect gaits, may coexist. In the present paper, by adding heterogeneous
external input to each oscillator, we modify the bursting neuron model so that
its corresponding phase oscillator model produces only one stable gait at each
speed, specifically: a unique stable tetrapod gait at low speed, a unique
stable tripod gait at high speed, and a unique branch of stable transition
gaits connecting them. This suggests that control signals originating in the
brain and central nervous system can modify gait patterns.
"
Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise,"  We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our general noise model allows to
perform ICA in settings where other noisy ICA procedures fail. Additionally, it
can be used for applications with grouped data by adjusting for different
stationary noise within each group. We show that the noise model has a natural
relation to causality and explain how it can be applied in the context of
causal inference. In addition to our theoretical framework, we provide an
efficient estimation procedure and prove identifiability of the unmixing matrix
under mild assumptions. Finally, we illustrate the performance and robustness
of our method on simulated data, provide audible and visual examples, and
demonstrate the applicability to real-world scenarios by experiments on
publicly available Antarctic ice core data as well as two EEG data sets. We
provide a scikit-learn compatible pip-installable Python package coroICA as
well as R and Matlab implementations accompanied by a documentation at
this https URL.
"
Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces,"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
"
Tonic activation of extrasynaptic NMDA receptors decreases intrinsic excitability and promotes bistability in a model of neuronal activity,"  NMDA receptors (NMDA-R) typically contribute to excitatory synaptic
transmission in the central nervous system. While calcium influx through NMDA-R
plays a critical role in synaptic plasticity, indirect experimental evidence
also exists demonstrating actions of NMDAR-mediated calcium influx on neuronal
excitability through the activation of calcium-activated potassium channels.
But, so far, this mechanism has not been studied theoretically. Our theoretical
model provide a simple description of neuronal electrical activity including
the tonic activity of NMDA receptors and a cytosolic calcium compartment. We
show that calcium influx through NMDA-R can directly be coupled to activation
of calcium-activated potassium channels providing an overall inhibitory effect
on neuronal excitability. Furthermore, the presence of tonic NMDA-R activity
promotes bistability in electrical activity by dramatically increasing the
stimulus interval where both a stable steady state and repetitive firing can
exist. This results could provide an intrinsic mechanism for the constitution
of memory traces in neuronal circuits. They also shed light on the way by which
beta-amyloids can decrease neuronal activity when interfering with NMDA-R in
Alzheimer's disease.
"
A Biologically Plausible Supervised Learning Method for Spiking Neural Networks Using the Symmetric STDP Rule,"  Spiking neural networks (SNNs) possess energy-efficient potential due to
event-based computation. However, supervised training of SNNs remains a
challenge as spike activities are non-differentiable. Previous SNNs training
methods can basically be categorized into two classes, backpropagation-like
training methods and plasticity-based learning methods. The former methods are
dependent on energy-inefficient real-valued computation and non-local
transmission, as also required in artificial neural networks (ANNs), while the
latter either be considered biologically implausible or exhibit poor
performance. Hence, biologically plausible (bio-plausible) high-performance
supervised learning (SL) methods for SNNs remain deficient. In this paper, we
proposed a novel bio-plausible SNN model for SL based on the symmetric
spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By
combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic
plasticity of the dynamic threshold, our SNN model implemented SL well and
achieved good performance in the benchmark recognition task (MNIST). To reveal
the underlying mechanism of our SL model, we visualized both layer-based
activities and synaptic weights using the t-distributed stochastic neighbor
embedding (t-SNE) method after training and found that they were well
clustered, thereby demonstrating excellent classification ability. As the
learning rules were bio-plausible and based purely on local spike events, our
model could be easily applied to neuromorphic hardware for online training and
may be helpful for understanding SL information processing at the synaptic
level in biological neural systems.
"
Rapid micro fluorescence in situ hybridization in tissue sections,"  This paper describes a micro fluorescence in situ hybridization
({\mu}FISH)-based rapid detection of cytogenetic biomarkers on formalin-fixed
paraffin embedded (FFPE) tissue sections. We demonstrated this method in the
context of detecting human epidermal growth factor 2 (HER2) in breast tissue
sections. This method uses a non-contact microfluidic scanning probe (MFP),
which localizes FISH probes at the micrometer length-scale to selected cells of
the tissue section. The scanning ability of the MFP allows for a versatile
implementation of FISH on tissue sections. We demonstrated the use of
oligonucleotide FISH probes in ethylene carbonate-based buffer enabling rapid
hybridization within < 1 min for chromosome enumeration and 10-15 min for
assessment of the HER2 status in FFPE sections. We further demonstrated
recycling of FISH probes for multiple sequential tests using a defined volume
of probes by forming hierarchical hydrodynamic flow confinements. This
microscale method is compatible with the standard FISH protocols and with the
Instant Quality (IQ) FISH assay, reduces the FISH probe consumption ~100-fold
and the hybridization time 4-fold, resulting in an assay turnaround time of < 3
h. We believe rapid {\mu}FISH has the potential of being used in pathology
workflows as a standalone method or in combination with other molecular methods
for diagnostic and prognostic analysis of FFPE sections.
"
A Novel Model of Cancer-Induced Peripheral Neuropathy and the Role of TRPA1 in Pain Transduction,"  Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
"
Deep Learning for micro-Electrocorticographic (μECoG) Data,"  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
"
Modularity Matters: Learning Invariant Relational Reasoning Tasks,"  We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.
"
Jensen's force and the statistical mechanics of cortical asynchronous states,"  The cortex exhibits self-sustained highly-irregular activity even under
resting conditions, whose origin and function need to be fully understood. It
is believed that this can be described as an ""asynchronous state"" stemming from
the balance between excitation and inhibition, with important consequences for
information-processing, though a competing hypothesis claims it stems from
critical dynamics. By analyzing a parsimonious neural-network model with
excitatory and inhibitory interactions, we elucidate a noise-induced mechanism
called ""Jensen's force"" responsible for the emergence of a novel phase of
arbitrarily-low but self-sustained activity, which reproduces all the
experimental features of asynchronous states. The simplicity of our framework
allows for a deep understanding of asynchronous states from a broad
statistical-mechanics perspective and of the phase transitions to other
standard phases it exhibits, opening the door to reconcile, asynchronous-state
and critical-state hypotheses. We argue that Jensen's forces are measurable
experimentally and might be relevant in contexts beyond neuroscience.
"
Nutritionally recommended food for semi- to strict vegetarian diets based on large-scale nutrient composition data,"  Diet design for vegetarian health is challenging due to the limited food
repertoire of vegetarians. This challenge can be partially overcome by
quantitative, data-driven approaches that utilise massive nutritional
information collected for many different foods. Based on large-scale data of
foods' nutrient compositions, the recent concept of nutritional fitness helps
quantify a nutrient balance within each food with regard to satisfying daily
nutritional requirements. Nutritional fitness offers prioritisation of
recommended foods using the foods' occurrence in nutritionally adequate food
combinations. Here, we systematically identify nutritionally recommendable
foods for semi- to strict vegetarian diets through the computation of
nutritional fitness. Along with commonly recommendable foods across different
diets, our analysis reveals favourable foods specific to each diet, such as
immature lima beans for a vegan diet as an amino acid and choline source, and
mushrooms for ovo-lacto vegetarian and vegan diets as a vitamin D source.
Furthermore, we find that selenium and other essential micronutrients can be
subject to deficiency in plant-based diets, and suggest nutritionally-desirable
dietary patterns. We extend our analysis to two hypothetical scenarios of
highly personalised, plant-based methionine-restricted diets. Our
nutrient-profiling approach may provide a useful guide for designing different
types of personalised vegetarian diets.
"
The Stretch to Stray on Time: Resonant Length of Random Walks in a Transient,"  First-passage times in random walks have a vast number of diverse
applications in physics, chemistry, biology, and finance. In general,
environmental conditions for a stochastic process are not constant on the time
scale of the average first-passage time, or control might be applied to reduce
noise. We investigate moments of the first-passage time distribution under a
transient describing relaxation of environmental conditions. We solve the
Laplace-transformed (generalized) master equation analytically using a novel
method that is applicable to general state schemes. The first-passage time from
one end to the other of a linear chain of states is our application for the
solutions. The dependence of its average on the relaxation rate obeys a power
law for slow transients. The exponent $\nu$ depends on the chain length $N$
like $\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the
noise of first-passage times expressed as the coefficient of variation (CV),
even if the average first-passage time is much longer than the transient. The
CV has a pronounced minimum for some lengths, which we call resonant lengths.
These results also suggest a simple and efficient noise control strategy, and
are closely related to the timing of repetitive excitations, coherence
resonance and information transmission by noisy excitable systems. A resonant
number of steps from the inhibited state to the excitation threshold and slow
recovery from negative feedback provide optimal timing noise reduction and
information transmission.
"
Species tree inference from genomic sequences using the log-det distance,"  The log-det distance between two aligned DNA sequences was introduced as a
tool for statistically consistent inference of a gene tree under simple
non-mixture models of sequence evolution. Here we prove that the log-det
distance, coupled with a distance-based tree construction method, also permits
consistent inference of species trees under mixture models appropriate to
aligned genomic-scale sequences data. Data may include sites from many genetic
loci, which evolved on different gene trees due to incomplete lineage sorting
on an ultrametric species tree, with different time-reversible substitution
processes. The simplicity and speed of distance-based inference suggests
log-det based methods should serve as benchmarks for judging more elaborate and
computationally-intensive species trees inference methods.
"
Explaining Parochialism: A Causal Account for Political Polarization in Changing Economic Environments,"  Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
"
Local White Matter Architecture Defines Functional Brain Dynamics,"  Large bundles of myelinated axons, called white matter, anatomically connect
disparate brain regions together and compose the structural core of the human
connectome. We recently proposed a method of measuring the local integrity
along the length of each white matter fascicle, termed the local connectome. If
communication efficiency is fundamentally constrained by the integrity along
the entire length of a white matter bundle, then variability in the functional
dynamics of brain networks should be associated with variability in the local
connectome. We test this prediction using two statistical approaches that are
capable of handling the high dimensionality of data. First, by performing
statistical inference on distance-based correlations, we show that similarity
in the local connectome between individuals is significantly correlated with
similarity in their patterns of functional connectivity. Second, by employing
variable selection using sparse canonical correlation analysis and
cross-validation, we show that segments of the local connectome are predictive
of certain patterns of functional brain dynamics. These results are consistent
with the hypothesis that structural variability along axon bundles constrains
communication between disparate brain regions.
"
Spatially-resolved Brillouin spectroscopy reveals biomechanical changes in early ectatic corneal disease and post-crosslinking in vivo,"  Mounting evidence connects the biomechanical properties of tissues to the
development of eye diseases such as keratoconus, a common disease in which the
cornea thins and bulges into a conical shape. However, measuring biomechanical
changes in vivo with sufficient sensitivity for disease detection has proved
challenging. Here, we present a first large-scale study (~200 subjects,
including normal and keratoconus patients) using Brillouin light-scattering
microscopy to measure longitudinal modulus in corneal tissues with high
sensitivity and spatial resolution. Our results in vivo provide evidence of
biomechanical inhomogeneity at the onset of keratoconus and suggest that
biomechanical asymmetry between the left and right eyes may presage disease
development. We additionally measure the stiffening effect of corneal
crosslinking treatment in vivo for the first time. Our results demonstrate the
promise of Brillouin microscopy for diagnosis and treatment of keratoconus, and
potentially other diseases.
"
Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary Computing,"  Point clouds obtained from photogrammetry are noisy and incomplete models of
reality. We propose an evolutionary optimization methodology that is able to
approximate the underlying object geometry on such point clouds. This approach
assumes a priori knowledge on the 3D structure modeled and enables the
identification of a collection of primitive shapes approximating the scene.
Built-in mechanisms that enforce high shape diversity and adaptive population
size make this method suitable to modeling both simple and complex scenes. We
focus here on the case of cylinder approximations and we describe, test, and
compare a set of mutation operators designed for optimal exploration of their
search space. We assess the robustness and limitations of this algorithm
through a series of synthetic examples, and we finally demonstrate its general
applicability on two real-life cases in vegetation and industrial settings.
"
How strong are correlations in strongly recurrent neuronal networks?,"  Cross-correlations in the activity in neural networks are commonly used to
characterize their dynamical states and their anatomical and functional
organizations. Yet, how these latter network features affect the spatiotemporal
structure of the correlations in recurrent networks is not fully understood.
Here, we develop a general theory for the emergence of correlated neuronal
activity from the dynamics in strongly recurrent networks consisting of several
populations of binary neurons. We apply this theory to the case in which the
connectivity depends on the anatomical or functional distance between the
neurons. We establish the architectural conditions under which the system
settles into a dynamical state where correlations are strong, highly robust and
spatially modulated. We show that such strong correlations arise if the network
exhibits an effective feedforward structure. We establish how this feedforward
structure determines the way correlations scale with the network size and the
degree of the connectivity. In networks lacking an effective feedforward
structure correlations are extremely small and only weakly depend on the number
of connections per neuron. Our work shows how strong correlations can be
consistent with highly irregular activity in recurrent networks, two key
features of neuronal dynamics in the central nervous system.
"
Particle-without-Particle: a practical pseudospectral collocation method for linear partial differential equations with distributional sources,"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
"
"Volvox barberi flocks, forming near-optimal, two-dimensional, polydisperse lattice packings","  Volvox barberi is a multicellular green alga forming spherical colonies of
10000-50000 differentiated somatic and germ cells. Here, I show that these
colonies actively self-organize over minutes into ""flocks"" that can contain
more than 100 colonies moving and rotating collectively for hours. The colonies
in flocks form two-dimensional, irregular, ""active crystals"", with lattice
angles and colony diameters both following log-normal distributions. Comparison
with a dynamical simulation of soft spheres with diameters matched to the
Volvox samples, and a weak long-range attractive force, show that the Volvox
flocks achieve optimal random close-packing. A dye tracer in the Volvox medium
revealed large hydrodynamic vortices generated by colony and flock rotations,
providing a likely source of the forces leading to flocking and optimal
packing.
"
Conformation Clustering of Long MD Protein Dynamics with an Adversarial Autoencoder,"  Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
"
Implementing a Concept Network Model,"  The same concept can mean different things or be instantiated in different
forms depending on context, suggesting a degree of flexibility within the
conceptual system. We propose that a compositional network model can be used to
capture and predict this flexibility. We modeled individual concepts (e.g.,
BANANA, BOTTLE) as graph-theoretical networks, in which properties (e.g.,
YELLOW, SWEET) were represented as nodes and their associations as edges. In
this framework, networks capture the within-concept statistics that reflect how
properties correlate with each other across instances of a concept. We ran a
classification analysis using graph eigendecomposition to validate these
models, and find that these models can successfully discriminate between object
concepts. We then computed formal measures from these concept networks and
explored their relationship to conceptual structure. We find that diversity
coefficients and core-periphery structure can be interpreted as network-based
measures of conceptual flexibility and stability, respectively. These results
support the feasibility of a concept network framework and highlight its
ability to formally capture important characteristics of the conceptual system.
"
The Moon Illusion explained by the Projective Consciousness Model,"  The Moon often appears larger near the perceptual horizon and smaller high in
the sky though the visual angle subtended is invariant. We show how this
illusion results from the optimization of a projective geometrical frame for
conscious perception through free energy minimization, as articulated in the
Projective Consciousness Model. The model accounts for all documented
modulations of the illusion without anomalies (e.g., the size-distance
paradox), surpasses other theories in explanatory power, makes sense of inter-
and intra-subjective variability vis-a-vis the illusion, and yields new
quantitative and qualitative predictions.
"
New indicators for assessing the quality of in silico produced biomolecules: the case study of the aptamer-Angiopoietin-2 complex,"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
"
Deep SNP: An End-to-end Deep Neural Network with Attention-based Localization for Break-point Detection in SNP Array Genomic data,"  Diagnosis and risk stratification of cancer and many other diseases require
the detection of genomic breakpoints as a prerequisite of calling copy number
alterations (CNA). This, however, is still challenging and requires
time-consuming manual curation. As deep-learning methods outperformed classical
state-of-the-art algorithms in various domains and have also been successfully
applied to life science problems including medicine and biology, we here
propose Deep SNP, a novel Deep Neural Network to learn from genomic data.
Specifically, we used a manually curated dataset from 12 genomic single
nucleotide polymorphism array (SNPa) profiles as truth-set and aimed at
predicting the presence or absence of genomic breakpoints, an indicator of
structural chromosomal variations, in windows of 40,000 probes. We compare our
results with well-known neural network models as well as Rawcopy though this
tool is designed to predict breakpoints and in addition genomic segments with
high sensitivity. We show, that Deep SNP is capable of successfully predicting
the presence or absence of a breakpoint in large genomic windows and
outperforms state-of-the-art neural network models. Qualitative examples
suggest that integration of a localization unit may enable breakpoint detection
and prediction of genomic segments, even if the breakpoint coordinates were not
provided for network training. These results warrant further evaluation of
DeepSNP for breakpoint localization and subsequent calling of genomic segments.
"
Gradient Sensing via Cell Communication,"  The chemotactic dynamics of cells and organisms that have no specialized
gradient sensing organelles is not well understood. In fact, chemotaxis of this
sort of organism is especially challenging to explain when the external
chemical gradient is so small as to make variations of concentrations minute
over the length of each of the organisms. Experimental evidence lends support
to the conjecture that chemotactic behavior of chains of cells can be achieved
via cell-to-cell communication. This is the chemotactic basis for the Local
Excitation, Global Inhibition (LEGI) model.
A generalization of the model for the communication component of the LEGI
model is proposed. Doing so permits us to study in detail how gradient sensing
changes as a function of the structure of the communication term. The key
findings of this study are, an accounting of how gradient sensing is affected
by the competition of communication and diffusive processes; the determination
of the scale dependence of the model outcomes; the sensitivity of communication
to parameters in the model. Together with an essential analysis of the dynamics
of the model, these findings can prove useful in suggesting experiments aimed
at determining the viability of a communication mechanism in chemotactic
dynamics of chains and networks of cells exposed to a chemical concentration
gradient.
"
Reconfiguration of Brain Network between Resting-state and Oddball Paradigm,"  The oddball paradigm is widely applied to the investigation of multiple
cognitive functions. Prior studies have explored the cortical oscillation and
power spectral differing from the resting-state conduction to oddball paradigm,
but whether brain networks existing the significant difference is still
unclear. Our study addressed how the brain reconfigures its architecture from a
resting-state condition (i.e., baseline) to P300 stimulus task in the visual
oddball paradigm. In this study, electroencephalogram (EEG) datasets were
collected from 24 postgraduate students, who were required to only mentally
count the number of target stimulus; afterwards the functional EEG networks
constructed in different frequency bands were compared between baseline and
oddball task conditions to evaluate the reconfiguration of functional network
in the brain. Compared to the baseline, our results showed the significantly (p
< 0.05) enhanced delta/theta EEG connectivity and decreased alpha default mode
network in the progress of brain reconfiguration to the P300 task. Furthermore,
the reconfigured coupling strengths were demonstrated to relate to P300
amplitudes, which were then regarded as input features to train a classifier to
differentiate the high and low P300 amplitudes groups with an accuracy of
77.78%. The findings of our study help us to understand the changes of
functional brain connectivity from resting-state to oddball stimulus task, and
the reconfigured network pattern has the potential for the selection of good
subjects for P300-based brain- computer interface.
"
ModelFactory: A Matlab/Octave based toolbox to create human body models,"  Background: Model-based analysis of movements can help better understand
human motor control. Here, the models represent the human body as an
articulated multi-body system that reflects the characteristics of the human
being studied.
Results: We present an open-source toolbox that allows for the creation of
human models with easy-to-setup, customizable configurations. The toolbox
scripts are written in Matlab/Octave and provide a command-based interface as
well as a graphical interface to construct, visualize and export models.
Built-in software modules provide functionalities such as automatic scaling of
models based on subject height and weight, custom scaling of segment lengths,
mass and inertia, addition of body landmarks, and addition of motion capture
markers. Users can set up custom definitions of joints, segments and other body
properties using the many included examples as templates. In addition to the
human, any number of objects (e.g. exoskeletons, orthoses, prostheses, boxes)
can be added to the modeling environment.
Conclusions: The ModelFactory toolbox is published as open-source software
under the permissive zLib license. The toolbox fulfills an important function
by making it easier to create human models, and should be of interest to human
movement researchers.
This document is the author's version of this article.
"
Interpretable LSTMs For Whole-Brain Neuroimaging Analyses,"  The analysis of neuroimaging data poses several strong challenges, in
particular, due to its high dimensionality, its strong spatio-temporal
correlation and the comparably small sample sizes of the respective datasets.
To address these challenges, conventional decoding approaches such as the
searchlight reduce the complexity of the decoding problem by considering local
clusters of voxels only. Thereby, neglecting the distributed spatial patterns
of brain activity underlying many cognitive states. In this work, we introduce
the DLight framework, which overcomes these challenges by utilizing a long
short-term memory unit (LSTM) based deep neural network architecture to analyze
the spatial dependency structure of whole-brain fMRI data. In order to maintain
interpretability of the neuroimaging data, we adapt the layer-wise relevance
propagation (LRP) method. Thereby, we enable the neuroscientist user to study
the learned association of the LSTM between the data and the cognitive state of
the individual. We demonstrate the versatility of DLight by applying it to a
large fMRI dataset of the Human Connectome Project. We show that the decoding
performance of our method scales better with large datasets, and moreover
outperforms conventional decoding approaches, while still detecting
physiologically appropriate brain areas for the cognitive states classified. We
also demonstrate that DLight is able to detect these areas on several levels of
data granularity (i.e., group, subject, trial, time point).
"
On the variance of internode distance under the multispecies coalescent,"  We consider the problem of estimating species trees from unrooted gene tree
topologies in the presence of incomplete lineage sorting, a common phenomenon
that creates gene tree heterogeneity in multilocus datasets. One popular class
of reconstruction methods in this setting is based on internode distances, i.e.
the average graph distance between pairs of species across gene trees. While
statistical consistency in the limit of large numbers of loci has been
established in some cases, little is known about the sample complexity of such
methods. Here we make progress on this question by deriving a lower bound on
the worst-case variance of internode distance which depends linearly on the
corresponding graph distance in the species tree. We also discuss some
algorithmic implications.
"
Network-based protein structural classification,"  Experimental determination of protein function is resource-consuming. As an
alternative, computational prediction of protein function has received
attention. In this context, protein structural classification (PSC) can help,
by allowing for determining structural classes of currently unclassified
proteins based on their features, and then relying on the fact that proteins
with similar structures have similar functions. Existing PSC approaches rely on
sequence-based or direct (""raw"") 3-dimensional (3D) structure-based protein
features. In contrast, we first model 3D structures as protein structure
networks (PSNs). Then, we use (""processed"") network-based features for PSC. We
propose the use of graphlets, state-of-the-art features in many domains of
network science, in the task of PSC. Moreover, because graphlets can deal only
with unweighted PSNs, and because accounting for edge weights when constructing
PSNs could improve PSC accuracy, we also propose a deep learning framework that
automatically learns network features from the weighted PSNs. When evaluated on
a large set of ~9,400 CATH and ~12,800 SCOP protein domains (spanning 36 PSN
sets), our proposed approaches are superior to existing PSC approaches in terms
of accuracy, with comparable running time.
"
A multiple timescales approach to bridging spiking- and population-level dynamics,"  A rigorous bridge between spiking-level and macroscopic quantities is an
on-going and well-developed story for asynchronously firing neurons, but focus
has shifted to include neural populations exhibiting varying synchronous
dynamics. Recent literature has used the Ott--Antonsen ansatz (2008) to great
effect, allowing a rigorous derivation of an order parameter for large
oscillator populations. The ansatz has been successfully applied using several
models including networks of Kuramoto oscillators, theta models, and
integrate-and-fire neurons, along with many types of network topologies. In the
present study, we take a converse approach: given the mean field dynamics of
slow synapses, predict the synchronization properties of finite neural
populations. The slow synapse assumption is amenable to averaging theory and
the method of multiple timescales. Our proposed theory applies to two
heterogeneous populations of N excitatory n-dimensional and N inhibitory
m-dimensional oscillators with homogeneous synaptic weights. We then
demonstrate our theory using two examples. In the first example we take a
network of excitatory and inhibitory theta neurons and consider the case with
and without heterogeneous inputs. In the second example we use Traub models
with calcium for the excitatory neurons and Wang-Buzs{á}ki models for the
inhibitory neurons. We accurately predict phase drift and phase locking in each
example even when the slow synapses exhibit non-trivial mean-field dynamics.
"
Convolution Forgetting Curve Model for Repeated Learning,"  Most of mathematic forgetting curve models fit well with the forgetting data
under the learning condition of one time rather than repeated. In the paper, a
convolution model of forgetting curve is proposed to simulate the memory
process during learning. In this model, the memory ability (i.e. the central
procedure in the working memory model) and learning material (i.e. the input in
the working memory model) is regarded as the system function and the input
function, respectively. The status of forgetting (i.e. the output in the
working memory model) is regarded as output function or the convolution result
of the memory ability and learning material. The model is applied to simulate
the forgetting curves in different situations. The results show that the model
is able to simulate the forgetting curves not only in one time learning
condition but also in multi-times condition. The model is further verified in
the experiments of Mandarin tone learning for Japanese learners. And the
predicted curve fits well on the test points.
"
Generalizing the first-difference correlated random walk for marine animal movement data,"  Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.
"
A model for random fire induced tree-grass coexistence in savannas,"  Tree-grass coexistence in savanna ecosystems depends strongly on
environmental disturbances out of which crucial is fire. Most modeling attempts
in the literature lack stochastic approach to fire occurrences which is
essential to reflect their unpredictability. Existing models that actually
include stochasticity of fire are usually analyzed only numerically. We
introduce new minimalistic model of tree-grass coexistence where fires occur
according to stochastic process. We use the tools of linear semigroup theory to
provide more careful mathematical analysis of the model. Essentially we show
that there exists a unique stationary distribution of tree and grass biomasses.
"
"The same strain of Piscine orthoreovirus (PRV-1) is involved with the development of different, but related, diseases in Atlantic and Pacific Salmon in British Columbia","  Piscine orthoreovirus Strain PRV-1 is the causative agent of heart and
skeletal muscle inflammation (HSMI) in Atlantic salmon (Salmo salar). Given its
high prevalence in net pen salmon, debate has arisen on whether PRV poses a
risk to migratory salmon, especially in British Columbia (BC) where
commercially important wild Pacific salmon are in decline. Various strains of
PRV have been associated with diseases in Pacific salmon, including
erythrocytic inclusion body syndrome (EIBS), HSMI-like disease, and
jaundice/anemia in Japan, Norway, Chile and Canada. We examine the
developmental pathway of HSMI and jaundice/anemia associated with PRV-1 in
farmed Atlantic and Chinook (Oncorhynchus tshawytscha) salmon in BC,
respectively. In situ hybridization localized PRV-1 within developing lesions
in both diseases. The two diseases showed dissimilar pathological pathways,
with inflammatory lesions in heart and skeletal muscle in Atlantic salmon, and
degenerative-necrotic lesions in kidney and liver in Chinook salmon, plausibly
explained by differences in PRV load tolerance in red blood cells. Viral genome
sequencing revealed no consistent differences in PRV-1 variants intimately
involved in the development of both diseases, suggesting that migratory Chinook
salmon may be at more than a minimal risk of disease from exposure to the high
levels of PRV occurring on salmon farms.
"
On the Analysis of Bacterial Cooperation with a Characterization of 2D Signal Propagation,"  The exchange of small molecular signals within microbial populations is
generally referred to as quorum sensing (QS). QS is ubiquitous in nature and
enables microorganisms to respond to fluctuations of living environments by
working together. In this work, a QS-based communication system within a
microbial population in a two-dimensional (2D) environment is analytically
modeled. Notably, the diffusion and degradation of signaling molecules within
the population is characterized. Microorganisms are randomly distributed on a
2D circle where each one releases molecules at random times. The number of
molecules observed at each randomly-distributed bacterium is analyzed. Using
this analysis and some approximation, the expected density of cooperating
bacteria is derived. The analytical results are validated via a particle-based
simulation method. The model can be used to predict and control behavioral
dynamics of microscopic populations that have imperfect signal propagation.
"
A Biomechanical Study on the Use of Curved Drilling Technique for Treatment of Osteonecrosis of Femoral Head,"  Osteonecrosis occurs due to the loss of blood supply to the bone, leading to
spontaneous death of the trabecular bone. Delayed treatment of the involved
patients results in collapse of the femoral head, which leads to a need for
total hip arthroplasty surgery. Core decompression, as the most popular
technique for treatment of the osteonecrosis, includes removal of the lesion
area by drilling a straight tunnel to the lesion, debriding the dead bone and
replacing it with bone substitutes. However, there are two drawbacks for this
treatment method. First, due to the rigidity of the instruments currently used
during core decompression, lesions cannot be completely removed and/or
excessive healthy bone may also be removed with the lesion. Second, the use of
bone substitutes, despite its biocompatibility and osteoconductivity, may not
provide sufficient mechanical strength and support for the bone. To address
these shortcomings, a novel robot-assisted curved core decompression (CCD)
technique is introduced to provide surgeons with direct access to the lesions
causing minimal damage to the healthy bone. In this study, with the aid of
finite element (FE) simulations, we investigate biomechanical performance of
core decompression using the curved drilling technique in the presence of
normal gait loading. In this regard, we compare the result of the CCD using
bone substitutes and flexible implants with other conventional core
decompression techniques. The study finding shows that the maximum principal
stress occurring at the superior domain of the neck is smaller in the CCD
techniques (i.e. 52.847 MPa) compared to the other core decompression methods.
"
A Brain-Inspired Trust Management Model to Assure Security in a Cloud based IoT Framework for Neuroscience Applications,"  Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
"
Preparation and Measurement in Quantum Memory Models,"  Quantum Cognition has delivered a number of models for semantic memory, but
to date these have tended to assume pure states and projective measurement.
Here we relax these assumptions. A quantum inspired model of human word
association experiments will be extended using a density matrix representation
of human memory and a POVM based upon non-ideal measurements. Our formulation
allows for a consideration of key terms like measurement and contextuality
within a rigorous modern approach. This approach both provides new conceptual
advances and suggests new experimental protocols.
"
Large-scale validation of an automatic EEG arousal detection algorithm using different heterogeneous databases,"  $\textbf{Objective}$: To assess the validity of an automatic EEG arousal
detection algorithm using large patient samples and different heterogeneous
databases
$\textbf{Methods}$: Automatic scorings were confronted with results from
human expert scorers on a total of 2768 full-night PSG recordings obtained from
two different databases. Of them, 472 recordings were obtained during clinical
routine at our sleep center and were subdivided into two subgroups of 220
(HMC-S) and 252 (HMC-M) recordings each, attending to the procedure followed by
the clinical expert during the visual review (semi-automatic or purely manual,
respectively). In addition, 2296 recordings from the public SHHS-2 database
were evaluated against the respective manual expert scorings.
$\textbf{Results}$: Event-by-event epoch-based validation resulted in an
overall Cohen kappa agreement K = 0.600 (HMC-S), 0.559 (HMC-M), and 0.573
(SHHS-2). Estimated inter-scorer variability on the datasets was, respectively,
K = 0.594, 0.561 and 0.543. Analyses of the corresponding Arousal Index scores
showed associated automatic-human repeatability indices ranging in 0.693-0.771
(HMC-S), 0.646-0.791 (HMC-M), and 0.759-0.791 (SHHS-2).
$\textbf{Conclusions}$: Large-scale validation of our automatic EEG arousal
detector on different databases has shown robust performance and good
generalization results comparable to the expected levels of human agreement.
Special emphasis has been put on allowing reproducibility of the results and
implementation of our method has been made accessible online as open source
code
"
The sequential loss of allelic diversity,"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
"
Online classification of imagined speech using functional near-infrared spectroscopy signals,"  Most brain-computer interfaces (BCIs) based on functional near-infrared
spectroscopy (fNIRS) require that users perform mental tasks such as motor
imagery, mental arithmetic, or music imagery to convey a message or to answer
simple yes or no questions. These cognitive tasks usually have no direct
association with the communicative intent, which makes them difficult for users
to perform. In this paper, a 3-class intuitive BCI is presented which enables
users to directly answer yes or no questions by covertly rehearsing the word
'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of
unconstrained rest which constitutes the third discernable task. Twelve
participants each completed one offline block and six online blocks over the
course of 2 sessions. The mean value of the change in oxygenated hemoglobin
concentration during a trial was calculated for each channel and used to train
a regularized linear discriminant analysis (RLDA) classifier. By the final
online block, 9 out of 12 participants were performing above chance (p<0.001),
with a 3-class accuracy of 83.8+9.4%. Even when considering all participants,
the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with
only 3 participants scoring below chance (p<0.001). For most participants,
channels in the left temporal and temporoparietal cortex provided the most
discriminative information. To our knowledge, this is the first report of an
online fNIRS 3-class imagined speech BCI. Our findings suggest that imagined
speech can be used as a reliable activation task for selected users for the
development of more intuitive BCIs for communication.
"
A Parsimonious Dynamical Model for Structural Learning in the Human Brain,"  The human brain is capable of diverse feats of intelligence. A particularly
salient example is the ability to deduce structure from time-varying auditory
and visual stimuli, enabling humans to master the rules of language and to
build rich expectations of their physical environment. The broad relevance of
this ability for human cognition motivates the need for a first-principles
model explicating putative mechanisms. Here we propose a general framework for
structural learning in the brain, composed of an evolving, high-dimensional
dynamical system driven by external stimuli or internal processes. We
operationalize the scenario in which humans learn the rules that generate a
sequence of stimuli, rather than the exemplar stimuli themselves. We model
external stimuli as seemingly disordered chaotic time series generated by
complex dynamical systems; the underlying structure being deduced is then that
of the corresponding chaotic attractor. This approach allows us to demonstrate
and theoretically explain the emergence of five distinct phenomena reminiscent
of cognitive functions: (i) learning the structure of a chaotic system purely
from time series, (ii) generating new streams of stimuli from a chaotic system,
(iii) switching stream generation among multiple learned chaotic systems,
either spontaneously or in response to external perturbations, (iv) inferring
missing data from sparse observations of the chaotic system, and (v)
deciphering superimposed input from different chaotic systems. Numerically, we
show that these phenomena emerge naturally from a recurrent neural network of
Erdos-Renyi topology in which the synaptic strengths adapt in a Hebbian-like
manner. Broadly, our work blends chaotic theory and artificial neural networks
to answer the long standing question of how neural systems can learn the
structure underlying temporal sequences of stimuli.
"
"Connecting the dots between mechanosensitive channel abundance, osmotic shock, and survival at single-cell resolution","  Rapid changes in extracellular osmolarity are one of many insults microbial
cells face on a daily basis. To protect against such shocks, Escherichia coli
and other microbes express several types of transmembrane channels which open
and close in response to changes in membrane tension. In E. coli, one of the
most abundant channels is the mechanosensitive channel of large conductance
(MscL). While this channel has been heavily characterized through structural
methods, electrophysiology, and theoretical modeling, our understanding of its
physiological role in preventing cell death by alleviating high membrane
tension remains tenuous. In this work, we examine the contribution of MscL
alone to cell survival after osmotic shock at single cell resolution using
quantitative fluorescence microscopy. We conduct these experiments in an E.
coli strain which is lacking all mechanosensitive channel genes save for MscL
whose expression is tuned across three orders of magnitude through
modifications of the Shine-Dalgarno sequence. While theoretical models suggest
that only a few MscL channels would be needed to alleviate even large changes
in osmotic pressure, we find that between 500 and 700 channels per cell are
needed to convey upwards of 80% survival. This number agrees with the average
MscL copy number measured in wild-type E. coli cells through proteomic studies
and quantitative Western blotting. Furthermore, we observe zero survival events
in cells with less than 100 channels per cell. This work opens new questions
concerning the contribution of other mechanosensitive channels to survival as
well as regulation of their activity.
"
Entropy Production Rate is Maximized in Non-Contractile Actomyosin,"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
"
When do we have the power to detect biological interactions in spatial point patterns?,"  Determining the relative importance of environmental factors, biotic
interactions and stochasticity in assembling and maintaining species-rich
communities remains a major challenge in ecology. In plant communities,
interactions between individuals of different species are expected to leave a
spatial signature in the form of positive or negative spatial correlations over
distances relating to the spatial scale of interaction. Most studies using
spatial point process tools have found relatively little evidence for
interactions between pairs of species. More interactions tend to be detected in
communities with fewer species. However, there is currently no understanding of
how the power to detect spatial interactions may change with sample size, or
the scale and intensity of interactions.
We use a simple 2-species model where the scale and intensity of interactions
are controlled to simulate point pattern data. In combination with an
approximation to the variance of the spatial summary statistics that we sample,
we investigate the power of current spatial point pattern methods to correctly
reject the null model of bivariate species independence.
We show that the power to detect interactions is positively related to the
abundances of the species tested, and the intensity and scale of interactions.
Increasing imbalance in abundances has a negative effect on the power to detect
interactions. At population sizes typically found in currently available
datasets for species-rich plant communities we find only a very low power to
detect interactions. Differences in power may explain the increased frequency
of interactions in communities with fewer species. Furthermore, the
community-wide frequency of detected interactions is very sensitive to a
minimum abundance criterion for including species in the analyses.
"
CLEAR: Coverage-based Limiting-cell Experiment Analysis for RNA-seq,"  Direct cDNA preamplification protocols developed for single-cell RNA-seq
(scRNA-seq) have enabled transcriptome profiling of rare cells without having
to pool multiple samples or to perform RNA extraction. We term this approach
limiting-cell RNA-seq (lcRNA-seq). Unlike scRNA-seq, which focuses on
'cell-atlasing', lcRNA-seq focuses on identifying differentially expressed
genes (DEGs) between experimental groups. This requires accounting for systems
noise which can obscure biological differences. We present CLEAR, a workflow
that identifies robust transcripts in lcRNA-seq data for between-group
comparisons. To develop CLEAR, we compared DEGs from RNA extracted from
FACS-derived CD5+ and CD5- cells from a single chronic lymphocytic leukemia
patient diluted to input RNA levels of 10-, 100- and 1,000pg. Data quality at
ultralow input levels are known to be noisy. When using CLEAR transcripts vs.
using all available transcripts, downstream analyses reveal more shared DEGs,
improved Principal Component Analysis separation of cell type, and increased
similarity between results across different input RNA amounts. CLEAR was
applied to two publicly available ultralow input RNA-seq data and an in-house
murine neural cell lcRNA-seq dataset. CLEAR provides a novel way to visualize
the public datasets while validates cell phenotype markers for astrocytes,
neural stem and progenitor cells.
"
Effects of atrial fibrillation on the arterial fluid dynamics: a modelling perspective,"  Atrial fibrillation (AF) is the most common form of arrhythmia with
accelerated and irregular heart rate (HR), leading to both heart failure and
stroke and being responsible for an increase in cardiovascular morbidity and
mortality. In spite of its importance, the direct effects of AF on the arterial
hemodynamic patterns are not completely known to date. Based on a multiscale
modelling approach, the proposed work investigates the effects of AF on the
local arterial fluid dynamics. AF and normal sinus rhythm (NSR) conditions are
simulated extracting 2000 $\mathrm{RR}$ heartbeats and comparing the most
relevant cardiac and vascular parameters at the same HR (75 bpm). Present
outcomes evidence that the arterial system is not able to completely absorb the
AF-induced variability, which can be even amplified towards the peripheral
circulation. AF is also able to locally alter the wave dynamics, by modifying
the interplay between forward and backward signals. The sole heart rhythm
variation (i.e., from NSR to AF) promotes an alteration of the regular dynamics
at the arterial level which, in terms of pressure and peripheral perfusion,
suggests a modification of the physiological phenomena ruled by periodicity
(e.g., regular organ perfusion)and a possible vascular dysfunction due to the
prolonged exposure to irregular and extreme values. The present study
represents a first modeling approach to characterize the variability of
arterial hemodynamics in presence of AF, which surely deserves further clinical
investigation.
"
Eco-evolutionary feedbacks - theoretical models and perspectives,"  1. Theoretical models pertaining to feedbacks between ecological and
evolutionary processes are prevalent in multiple biological fields. An
integrative overview is currently lacking, due to little crosstalk between the
fields and the use of different methodological approaches.
2. Here we review a wide range of models of eco-evolutionary feedbacks and
highlight their underlying assumptions. We discuss models where feedbacks occur
both within and between hierarchical levels of ecosystems, including
populations, communities, and abiotic environments, and consider feedbacks
across spatial scales.
3. Identifying the commonalities among feedback models, and the underlying
assumptions, helps us better understand the mechanistic basis of
eco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled
by coupling demographic and evolutionary formalisms. We provide an overview of
these approaches and suggest future integrative modelling avenues.
4. Our overview highlights that eco-evolutionary feedbacks have been
incorporated in theoretical work for nearly a century. Yet, this work does not
always include the notion of rapid evolution or concurrent ecological and
evolutionary time scales. We discuss the importance of density- and
frequency-dependent selection for feedbacks, as well as the importance of
dispersal as a central linking trait between ecology and evolution in a spatial
context.
"
Non-equilibrium statistical mechanics of continuous attractors,"  Continuous attractors have been used to understand recent neuroscience
experiments where persistent activity patterns encode internal representations
of external attributes like head direction or spatial location. However, the
conditions under which the emergent bump of neural activity in such networks
can be manipulated by space and time-dependent external sensory or motor
signals are not understood. Here, we find fundamental limits on how rapidly
internal representations encoded along continuous attractors can be updated by
an external signal. We apply these results to place cell networks to derive a
velocity-dependent non-equilibrium memory capacity in neural networks.
"
Simple property of heterogeneous aspiration dynamics: Beyond weak selection,"  How individuals adapt their behavior in cultural evolution remains elusive.
Theoretical studies have shown that the update rules chosen to model individual
decision making can dramatically modify the evolutionary outcome of the
population as a whole. This hints at the complexities of considering the
personality of individuals in a population, where each one uses its own rule.
Here, we investigate whether and how heterogeneity in the rules of behavior
update alters the evolutionary outcome. We assume that individuals update
behaviors by aspiration-based self-evaluation and they do so in their own ways.
Under weak selection, we analytically reveal a simple property that holds for
any two-strategy multi-player games in well-mixed populations and on regular
graphs: the evolutionary outcome in a population with heterogeneous update
rules is the weighted average of the outcomes in the corresponding homogeneous
populations, and the associated weights are the frequencies of each update rule
in the heterogeneous population. Beyond weak selection, we show that this
property holds for public goods games. Our finding implies that heterogeneous
aspiration dynamics is additive. This additivity greatly reduces the complexity
induced by the underlying individual heterogeneity. Our work thus provides an
efficient method to calculate evolutionary outcomes under heterogeneous update
rules.
"
Harnessing functional segregation across brain rhythms as a means to detect EEG oscillatory multiplexing during music listening,"  Music, being a multifaceted stimulus evolving at multiple timescales,
modulates brain function in a manifold way that encompasses not only the
distinct stages of auditory perception but also higher cognitive processes like
memory and appraisal. Network theory is apparently a promising approach to
describe the functional reorganization of brain oscillatory dynamics during
music listening. However, the music induced changes have so far been examined
within the functional boundaries of isolated brain rhythms. Using naturalistic
music, we detected the functional segregation patterns associated with
different cortical rhythms, as these were reflected in the surface EEG
measurements. The emerged structure was compared across frequency bands to
quantify the interplay among rhythms. It was also contrasted against the
structure from the rest and noise listening conditions to reveal the specific
components stemming from music listening. Our methodology includes an efficient
graph-partitioning algorithm, which is further utilized for mining prototypical
modular patterns, and a novel algorithmic procedure for identifying switching
nodes that consistently change module during music listening. Our results
suggest the multiplex character of the music-induced functional reorganization
and particularly indicate the dependence between the networks reconstructed
from the {\delta} and {\beta}H rhythms. This dependence is further justified
within the framework of nested neural oscillations and fits perfectly within
the context of recently introduced cortical entrainment to music. Considering
its computational efficiency, and in conjunction with the flexibility of in
situ electroencephalography, it may lead to novel assistive tools for real-life
applications.
"
Strong deformations of DNA: Effect on the persistence length,"  Extreme deformations of the DNA double helix attracted a lot of attention
during the past decades. Particularly, the determination of the persistence
length of DNA with extreme local disruptions, or kinks, has become a crucial
problem in the studies of many important biological processes. In this paper we
review an approach to calculate the persistence length of the double helix by
taking into account the formation of kinks of arbitrary configuration. The
reviewed approach improves the Kratky--Porod model to determine the type and
nature of kinks that occur in the double helix, by measuring a reduction of the
persistence length of the kinkable DNA.
"
"Hybrid Sterility Can Only be Primary When Acting as a Reproductive Barrier for Sympatric Speciation,","  Parental gametes unite to form a zygote that develops into an adult with
gonads that, in turn, produce gametes. Interruption of this germinal cycle by
prezygotic or postzygotic reproductive barriers can result in two independent
cycles, each with the potential to evolve into a new species. When the
speciation process is complete, members of each species are fully
reproductively isolated from those of the other. During speciation a primary
barrier may be supported and eventually superceded by a later appearing
secondary barrier. For those holding certain cases of prezygotic isolation to
be primary (e.g. elephant cannot copulate with mouse), the onus is to show that
they had not been preceded over evolutionary time by periods of postzygotic
hybrid inviability (genically determined) or sterility (genically or
chromosomally determined). Likewise, the onus is upon those holding cases of
hybrid inviability to be primary (e.g. Dobzhansky-Muller epistatic
incompatibilities), to show that they had not been preceded by periods, however
brief, of hybrid sterility. The latter, when acting as a sympatric barrier
causing reproductive isolation, can only be primary. In many cases, hybrid
sterility may result from incompatibilities between parental chromosomes that
attempt to pair during meiosis in the gonad of their offspring
(Winge-Crowther-Bateson incompatibilities). While WCB incompatibilities have
long been observed on a microscopic scale, there is growing evidence for a role
of dispersed finer DNA sequence differences.
"
Network flow of mobile agents enhances the evolution of cooperation,"  We study the effect of contingent movement on the persistence of cooperation
on complex networks with empty nodes. Each agent plays Prisoner's Dilemma game
with its neighbors and then it either updates the strategy depending on the
payoff difference with neighbors or it moves to another empty node if not
satisfied with its own payoff. If no neighboring node is empty, each agent
stays at the same site. By extensive evolutionary simulations, we show that the
medium density of agents enhances cooperation where the network flow of mobile
agents is also medium. Moreover, if the movements of agents are more frequent
than the strategy updating, cooperation is further promoted. In scale-free
networks, the optimal density for cooperation is lower than other networks
because agents get stuck at hubs. Our study suggests that keeping a smooth
network flow is significant for the persistence of cooperation in ever-changing
societies.
"
"Fluid-Structure Interaction for the Classroom: Interpolation, Hearts, and Swimming!","  While students may find spline interpolation easily digestible, based on
their familiarity with continuity of a function and its derivatives, some of
its inherent value may be missed when students only see it applied to standard
data interpolation exercises. In this paper, we offer alternatives where
students can qualitatively and quantitatively witness the resulting dynamical
differences when objects are driven through a fluid using different spline
interpolation methods. They say, seeing is believing; here we showcase the
differences between linear and cubic spline interpolation using examples from
fluid pumping and aquatic locomotion. Moreover, students can define their own
interpolation functions and visualize the dynamics unfold. To solve the
fluid-structure interaction system, the open source software IB2d is used. In
that vein, all simulation codes, analysis scripts, and movies are provided for
streamlined use.
"
Enhancing Blood Glucose Prediction with Meal Absorption and Physical Exercise Information,"  Objective: Numerous glucose prediction algorithm have been proposed to
empower type 1 diabetes (T1D) management. Most of these algorithms only account
for input such as glucose, insulin and carbohydrate, which limits their
performance. Here, we present a novel glucose prediction algorithm which, in
addition to standard inputs, accounts for meal absorption and physical exercise
information to enhance prediction accuracy. Methods: a compartmental model of
glucose-insulin dynamics combined with a deconvolution technique for state
estimation is employed for glucose prediction. In silico data corresponding
from the 10 adult subjects of UVa-Padova simulator, and clinical data from 10
adults with T1D were used. Finally, a comparison against a validated glucose
prediction algorithm based on a latent variable with exogenous input (LVX)
model is provided. Results: For a prediction horizon of 60 minutes, accounting
for meal absorption and physical exercise improved glucose forecasting
accuracy. In particular, root mean square error (mg/dL) went from 26.68 to
23.89, p<0.001 (in silico data); and from 37.02 to 35.96, p<0.001 (clinical
data - only meal information). Such improvement in accuracy was translated into
significant improvements on hypoglycaemia and hyperglycaemia prediction.
Finally, the performance of the proposed algorithm is statistically superior to
that of the LVX algorithm (26.68 vs. 32.80, p<0.001 (in silico data); 37.02 vs.
49.17, p<0.01 (clinical data). Conclusion: Taking into account meal absorption
and physical exercise information improves glucose prediction accuracy.
"
Age-at-harvest models as monitoring and harvest management tools for Wisconsin carnivores,"  Quantifying and estimating wildlife population sizes is a foundation of
wildlife management. However, many carnivore species are cryptic, leading to
innate difficulties in estimating their populations. We evaluated the potential
for using more rigorous statistical models to estimate the populations of black
bears (Ursus americanus) in Wisconisin, and their applicability to other
furbearers such as bobcats (Lynx rufus). To estimate black bear populations, we
developed an AAH state-space model in a Bayesian framework based on Norton
(2015) that can account for variation in harvest and population demographics
over time. Our state-space model created an accurate estimate of the black bear
population in Wisconsin based on age-at-harvest data and improves on previous
models by using little demographic data, no auxiliary data, and not being
sensitive to initial population size. The increased accuracy of the AAH
state-space models should allow for better management by being able to set
accurate quotas to ensure a sustainable harvest for the black bear population
in each zone. We also evaluated the demography and annual harvest data of
bobcats in Wisconsin to determine trends in harvest, method, and hunter
participation in Wisconsin. Each trait of harvested bobcats that we tested
(mass, male:female sex ratio, and age) changed over time, and because these
were interrelated, and we inferred that harvest selection for larger size
biased harvests in favor of a) larger, b) older, and c) male bobcats by hound
hunters. We also found an increase in the proportion of bobcats that were
harvested by hound hunting compared to trapping, and that hound hunters were
more likely to create taxidermy mounts than trappers. We also found that
decreasing available bobcat tags and increasing success have occurred over
time, and correlate with substantially increasing both hunter populations and
hunter interest.
"
CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse Embryonic Stem Cells,"  Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
"
Stochastic Model of SIR Epidemic Modelling,"  Threshold theorem is probably the most important development of mathematical
epidemic modelling. Unfortunately, some models may not behave according to the
threshold. In this paper, we will focus on the final outcome of SIR model with
demography. The behaviour of the model approached by deteministic and
stochastic models will be introduced, mainly using simulations. Furthermore, we
will also investigate the dynamic of susceptibles in population in absence of
infective. We have successfully showed that both deterministic and stochastic
models performed similar results when $R_0 \leq 1$. That is, the disease-free
stage in the epidemic. But when $R_0 > 1$, the deterministic and stochastic
approaches had different interpretations.
"
Effect of Blast Exposure on Gene-Gene Interactions,"  Repeated exposure to low-level blast may initiate a range of adverse health
problem such as traumatic brain injury (TBI). Although many studies
successfully identified genes associated with TBI, yet the cellular mechanisms
underpinning TBI are not fully elucidated. In this study, we investigated
underlying relationship among genes through constructing transcript Bayesian
networks using RNA-seq data. The data for pre- and post-blast transcripts,
which were collected on 33 individuals in Army training program, combined with
our system approach provide unique opportunity to investigate the effect of
blast-wave exposure on gene-gene interactions. Digging into the networks, we
identified four subnetworks related to immune system and inflammatory process
that are disrupted due to the exposure. Among genes with relatively high fold
change in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,
S100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the
gene-gene interactions. This study reveals how repeated exposures to traumatic
conditions increase the level of fold change of transcript expression and
hypothesizes new targets for further experimental studies.
"
Introduction to the Special Issue on Approaches to Control Biological and Biologically Inspired Networks,"  The emerging field at the intersection of quantitative biology, network
modeling, and control theory has enjoyed significant progress in recent years.
This Special Issue brings together a selection of papers on complementary
approaches to observe, identify, and control biological and biologically
inspired networks. These approaches advance the state of the art in the field
by addressing challenges common to many such networks, including high
dimensionality, strong nonlinearity, uncertainty, and limited opportunities for
observation and intervention. Because these challenges are not unique to
biological systems, it is expected that many of the results presented in these
contributions will also find applications in other domains, including physical,
social, and technological networks.
"
Computational modeling approaches in gonadotropin signaling,"  Follicle-stimulating hormone (FSH) and luteinizing hormone (LH) play
essential roles in animal reproduction. They exert their function through
binding to their cognate receptors, which belong to the large family of G
protein-coupled receptors (GPCRs). This recognition at the plasma membrane
triggers a plethora of cellular events, whose processing and integration
ultimately lead to an adapted biological response. Understanding the nature and
the kinetics of these events is essential for innovative approaches in drug
discovery. The study and manipulation of such complex systems requires the use
of computational modeling approaches combined with robust in vitro functional
assays for calibration and validation. Modeling brings a detailed understanding
of the system and can also be used to understand why existing drugs do not work
as well as expected, and how to design more efficient ones.
"
Markov chain aggregation and its application to rule-based modelling,"  Rule-based modelling allows to represent molecular interactions in a compact
and natural way. The underlying molecular dynamics, by the laws of stochastic
chemical kinetics, behaves as a continuous-time Markov chain. However, this
Markov chain enumerates all possible reaction mixtures, rendering the analysis
of the chain computationally demanding and often prohibitive in practice. We
here describe how it is possible to efficiently find a smaller, aggregate
chain, which preserves certain properties of the original one. Formal methods
and lumpability notions are used to define algorithms for automated and
efficient construction of such smaller chains (without ever constructing the
original ones). We here illustrate the method on an example and we discuss the
applicability of the method in the context of modelling large signalling
pathways.
"
The unreasonable effectiveness of small neural ensembles in high-dimensional brain,"  Despite the widely-spread consensus on the brain complexity, sprouts of the
single neuron revolution emerged in neuroscience in the 1970s. They brought
many unexpected discoveries, including grandmother or concept cells and sparse
coding of information in the brain.
In machine learning for a long time, the famous curse of dimensionality
seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of
dimensionality becomes gradually more and more popular. Ensembles of
non-interacting or weakly interacting simple units prove to be an effective
tool for solving essentially multidimensional problems. This approach is
especially useful for one-shot (non-iterative) correction of errors in large
legacy artificial intelligence systems.
These simplicity revolutions in the era of complexity have deep fundamental
reasons grounded in geometry of multidimensional data spaces. To explore and
understand these reasons we revisit the background ideas of statistical
physics. In the course of the 20th century they were developed into the
concentration of measure theory. New stochastic separation theorems reveal the
fine structure of the data clouds.
We review and analyse biological, physical, and mathematical problems at the
core of the fundamental question: how can high-dimensional brain organise
reliable and fast learning in high-dimensional world of data by simple tools?
Two critical applications are reviewed to exemplify the approach: one-shot
correction of errors in intellectual systems and emergence of static and
associative memories in ensembles of single neurons.
"
Mitochondrial network fragmentation modulates mutant mtDNA accumulation independently of absolute fission-fusion rates,"  Mitochondrial DNA (mtDNA) mutations cause severe congenital diseases but may
also be associated with healthy aging. MtDNA is stochastically replicated and
degraded, and exists within organelles which undergo dynamic fusion and
fission. The role of the resulting mitochondrial networks in determining the
time evolution of the cellular proportion of mutated mtDNA molecules
(heteroplasmy), and cell-to-cell variability in heteroplasmy (heteroplasmy
variance), remains incompletely understood. Heteroplasmy variance is
particularly important since it modulates the number of pathological cells in a
tissue. Here, we provide the first wide-reaching mathematical treatment which
bridges mitochondrial network and genetic states. We show that, for a range of
models, the rate of increase in heteroplasmy variance, and the rate of
\textit{de novo} mutation, is proportionately modulated by the fraction of
unfused mitochondria, independently of the absolute fission-fusion rate. In the
context of selective fusion, we show that intermediate fusion/fission ratios
are optimal for the clearance of mtDNA mutants. Our findings imply that
modulating network state, mitophagy rate and copy number to slow down
heteroplasmy dynamics when mean heteroplasmy is low, could have therapeutic
advantages for mitochondrial disease and healthy aging.
"
Anticipation: an effective evolutionary strategy for a sub-optimal population in a cyclic environment,"  We built a two-state model of an asexually reproducing organism in a periodic
environment endowed with the capability to anticipate an upcoming environmental
change and undergo pre-emptive switching. By virtue of these anticipatory
transitions, the organism oscillates between its two states that is a time
$\theta$ out of sync with the environmental oscillation. We show that an
anticipation-capable organism increases its long-term fitness over an organism
that oscillates in-sync with the environment, provided $\theta$ does not exceed
a threshold. We also show that the long-term fitness is maximized for an
optimal anticipation time that decreases approximately as $1/n$, $n$ being the
number of cell divisions in time $T$. Furthermore, we demonstrate that optimal
""anticipators"" outperforms ""bet-hedgers"" in the range of parameters considered.
For a sub-optimal ensemble of anticipators, anticipation performs better to
bet-hedging only when the variance in anticipation is small compared to the
mean and the rate of pre-emptive transition is high. Taken together, our work
suggests that anticipation increases overall fitness of an organism in a
periodic environment and it is a viable alternative to bet-hedging provided the
error in anticipation is small.
"
The self-referring DNA and protein: a remark on physical and geometrical aspects,"  All known life forms are based upon a hierarchy of interwoven feedback loops,
operating over a cascade of space, time and energy scales. Among the most basic
loops are those connecting DNA and proteins. For example, in genetic networks,
DNA genes are expressed as proteins, which may bind near the same genes and
thereby control their own expression. In this molecular type of self-reference,
information is mapped from the DNA sequence to the protein and back to DNA.
There is a variety of dynamic DNA-protein self-reference loops, and the purpose
of this remark is to discuss certain geometrical and physical aspects related
to the back and forth mapping between DNA and proteins. The discussion raises
basic questions regarding the nature of DNA and proteins as self-referring
matter, which are examined in a simple toy model.
"
Complexity of human response delay in intermittent control: The case of virtual stick balancing,"  Response delay is an inherent and essential part of human actions. In the
context of human balance control, the response delay is traditionally modeled
using the formalism of delay-differential equations, which adopts the
approximation of fixed delay. However, experimental studies revealing
substantial variability, adaptive anticipation, and non-stationary dynamics of
response delay provide evidence against this approximation. In this paper, we
call for development of principally new mathematical formalism describing human
response delay. To support this, we present the experimental data from a simple
virtual stick balancing task. Our results demonstrate that human response delay
is a widely distributed random variable with complex properties, which can
exhibit oscillatory and adaptive dynamics characterized by long-range
correlations. Given this, we argue that the fixed-delay approximation ignores
essential properties of human response, and conclude with possible directions
for future developments of new mathematical notions describing human control.
"
Exact Combinatorial Inference for Brain Images,"  The permutation test is known as the exact test procedure in statistics.
However, often it is not exact in practice and only an approximate method since
only a small fraction of every possible permutation is generated. Even for a
small sample size, it often requires to generate tens of thousands
permutations, which can be a serious computational bottleneck. In this paper,
we propose a novel combinatorial inference procedure that enumerates all
possible permutations combinatorially without any resampling. The proposed
method is validated against the standard permutation test in simulation studies
with the ground truth. The method is further applied in twin DTI study in
determining the genetic contribution of the minimum spanning tree of the
structural brain connectivity.
"
Compact Convolutional Neural Networks for Classification of Asynchronous Steady-state Visual Evoked Potentials,"  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.
"
Variable domain N-linked glycosylation and negative surface charge are key features of monoclonal ACPA: implications for B-cell selection,"  Autoreactive B cells have a central role in the pathogenesis of rheumatoid
arthritis (RA), and recent findings have proposed that anti-citrullinated
protein autoantibodies (ACPA) may be directly pathogenic. Herein, we
demonstrate the frequency of variable-region glycosylation in single-cell
cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked
glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA
patients and controls. Variable region N-linked motifs (N-X-S/T) were
strikingly prevalent within ACPA (100%) compared to somatically hypermutated
(SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from
seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA
still had significantly higher frequency of N-linked motifs compared to all
studied mAbs including highly-mutated HIV broadly-neutralizing and
malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated,
contributed to altered charge, but did not influence antigen binding. The
analysis revealed evidence of unusual B-cell selection pressure and
SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is
still unknown how these distinct features of anti-citrulline immunity may have
an impact on pathogenesis. However, it is evident that they offer selective
advantages for ACPA+ B cells, possibly also through non-antigen driven
mechanisms.
"
Scalar Reduction of a Neural Field Model with Spike Frequency Adaptation,"  We study a deterministic version of a one- and two-dimensional attractor
neural network model of hippocampal activity first studied by Itskov et al
2011. We analyze the dynamics of the system on the ring and torus domain with
an even periodized weight matrix, assum- ing weak and slow spike frequency
adaptation and a weak stationary input current. On these domains, we find
transitions from spatially localized stationary solutions (""bumps"") to
(periodically modulated) solutions (""sloshers""), as well as constant and
non-constant velocity traveling bumps depending on the relative strength of
external input current and adaptation. The weak and slow adaptation allows for
a reduction of the system from a distributed partial integro-differential
equation to a system of scalar Volterra integro-differential equations
describing the movement of the centroid of the bump solution. Using this
reduction, we show that on both domains, sloshing solutions arise through an
Andronov-Hopf bifurcation and derive a normal form for the Hopf bifurcation on
the ring. We also show existence and stability of constant velocity solutions
on both domains using Evans functions. In contrast to existing studies, we
assume a general weight matrix of Mexican-hat type in addition to a smooth
firing rate function.
"
Analysis of Sequence Polymorphism of LINEs and SINEs in Entamoeba histolytica,"  The goal of this dissertation is to study the sequence polymorphism in
retrotransposable elements of Entamoeba histolytica. The Quasispecies theory, a
concept of equilibrium (stationary), has been used to understand the behaviour
of these elements. Two datasets of retrotransposons of Entamoeba histolytica
have been used. We present results from both datasets of retrotransposons
(SINE1s) of E. histolytica. We have calculated the mutation rate of EhSINE1s
for both datasets and drawn a phylogenetic tree for newly determined EhSINE1s
(dataset II). We have also discussed the variation in lengths of EhSINE1s for
both datasets. Using the quasispecies model, we have shown how sequences of
SINE1s vary within the population. The outputs of the quasispecies model are
discussed in the presence and the absence of back mutation by taking different
values of fitness. From our study of Non-long terminal repeat retrotransposons
(LINEs and their non-autonomous partner's SINEs) of Entamoeba histolytica, we
can conclude that an active EhSINE can generate very similar copies of itself
by retrotransposition. Due to this reason, it increases mutations which give
the result of sequence polymorphism. We have concluded that the mutation rate
of SINE is very high. This high mutation rate provides an idea for the
existence of SINEs, which may affect the genetic analysis of EhSINE1
ancestries, and calculation of phylogenetic distances.
"
Correlating Cellular Features with Gene Expression using CCA,"  To understand the biology of cancer, joint analysis of multiple data
modalities, including imaging and genomics, is crucial. The involved nature of
gene-microenvironment interactions necessitates the use of algorithms which
treat both data types equally. We propose the use of canonical correlation
analysis (CCA) and a sparse variant as a preliminary discovery tool for
identifying connections across modalities, specifically between gene expression
and features describing cell and nucleus shape, texture, and stain intensity in
histopathological images. Applied to 615 breast cancer samples from The Cancer
Genome Atlas, CCA revealed significant correlation of several image features
with expression of PAM50 genes, known to be linked to outcome, while Sparse CCA
revealed associations with enrichment of pathways implicated in cancer without
leveraging prior biological understanding. These findings affirm the utility of
CCA for joint phenotype-genotype analysis of cancer.
"
Immigration-induced phase transition in a regulated multispecies birth-death process,"  Power-law-distributed species counts or clone counts arise in many biological
settings such as multispecies cell populations, population genetics, and
ecology. This empirical observation that the number of species $c_{k}$
represented by $k$ individuals scales as negative powers of $k$ is also
supported by a series of theoretical birth-death-immigration (BDI) models that
consistently predict many low-population species, a few intermediate-population
species, and very high-population species. However, we show how a simple global
population-dependent regulation in a neutral BDI model destroys the power law
distributions. Simulation of the regulated BDI model shows a high probability
of observing a high-population species that dominates the total population.
Further analysis reveals that the origin of this breakdown is associated with
the failure of a mean-field approximation for the expected species abundance
distribution. We find an accurate estimate for the expected distribution
$\langle c_k \rangle$ by mapping the problem to a lower-dimensional Moran
process, allowing us to also straightforwardly calculate the covariances
$\langle c_k c_\ell \rangle$. Finally, we exploit the concepts associated with
energy landscapes to explain the failure of the mean-field assumption by
identifying a phase transition in the quasi-steady-state species counts
triggered by a decreasing immigration rate.
"
Fine-scale population structure analysis in Armadillidium vulgare (Isopoda: Oniscidea) reveals strong female philopatry,"  In the last decades, dispersal studies have benefitted from the use of
molecular markers for detecting patterns differing between categories of
individuals, and have highlighted sex-biased dispersal in several species. To
explain this phenomenon, sex-related handicaps such as parental care have been
recently proposed as a hypothesis. Herein we tested this hypothesis in
Armadillidium vulgare, a terrestrial isopod in which females bear the totality
of the high parental care costs. We performed a fine-scale analysis of
sex-specific dispersal patterns, using males and females originating from five
sampling points located within 70 meters of each other. Based on microsatellite
markers and both F-statistics and spatial autocorrelation analyses, our results
revealed that while males did not present a significant structure at this
geographic scale, females were significantly more similar to each other when
they were collected in the same sampling point. These results support the
sex-handicap hypothesis, and we suggest that widening dispersal studies to
other isopods or crustaceans, displaying varying levels of parental care but
differing in their ecology or mating system, might shed light on the processes
underlying the evolution of sex-biased dispersal.
"
Evaluating Overfit and Underfit in Models of Network Community Structure,"  A common data mining task on networks is community detection, which seeks an
unsupervised decomposition of a network into structural groups based on
statistical regularities in the network's connectivity. Although many methods
exist, the No Free Lunch theorem for community detection implies that each
makes some kind of tradeoff, and no algorithm can be optimal on all inputs.
Thus, different algorithms will over or underfit on different inputs, finding
more, fewer, or just different communities than is optimal, and evaluation
methods that use a metadata partition as a ground truth will produce misleading
conclusions about general accuracy. Here, we present a broad evaluation of over
and underfitting in community detection, comparing the behavior of 16
state-of-the-art community detection algorithms on a novel and structurally
diverse corpus of 406 real-world networks. We find that (i) algorithms vary
widely both in the number of communities they find and in their corresponding
composition, given the same input, (ii) algorithms can be clustered into
distinct high-level groups based on similarities of their outputs on real-world
networks, and (iii) these differences induce wide variation in accuracy on link
prediction and link description tasks. We introduce a new diagnostic for
evaluating overfitting and underfitting in practice, and use it to roughly
divide community detection methods into general and specialized learning
algorithms. Across methods and inputs, Bayesian techniques based on the
stochastic block model and a minimum description length approach to
regularization represent the best general learning approach, but can be
outperformed under specific circumstances. These results introduce both a
theoretically principled approach to evaluate over and underfitting in models
of network community structure and a realistic benchmark by which new methods
may be evaluated and compared.
"
Evaluation of matrix factorisation approaches for muscle synergy extraction,"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
"
Approximate Analytical Solution of a Cancer Immunotherapy Model by the Application of Differential Transform and Adomian Decomposition Methods,"  Immunotherapy plays a major role in tumour treatment, in comparison with
other methods of dealing with cancer. The Kirschner-Panetta (KP) model of
cancer immunotherapy describes the interaction between tumour cells, effector
cells and interleukin-2 which are clinically utilized as medical treatment. The
model selects a rich concept of immune-tumour dynamics. In this paper,
approximate analytical solutions to KP model are represented by using the
differential transform and Adomian decomposition. The complicated nonlinearity
of the KP system causes the application of these two methods to require more
involved calculations. The approximate analytical solutions to the model are
compared with the results obtained by numerical fourth order Runge-Kutta
method.
"
An Introduction to Animal Movement Modeling with Hidden Markov Models using Stan for Bayesian Inference,"  Hidden Markov models (HMMs) are popular time series model in many fields
including ecology, economics and genetics. HMMs can be defined over discrete or
continuous time, though here we only cover the former. In the field of movement
ecology in particular, HMMs have become a popular tool for the analysis of
movement data because of their ability to connect observed movement data to an
underlying latent process, generally interpreted as the animal's unobserved
behavior. Further, we model the tendency to persist in a given behavior over
time. Notation presented here will generally follow the format of Zucchini et
al. (2016) and cover HMMs applied in an unsupervised case to animal movement
data, specifically positional data. We provide Stan code to analyze movement
data of the wild haggis as presented first in Michelot et al. (2016).
"
Zinc oxide induces the stringent response and major reorientations in the central metabolism of Bacillus subtilis,"  Microorganisms, such as bacteria, are one of the first targets of
nanoparticles in the environment. In this study, we tested the effect of two
nanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the
Gram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based
proteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no
detectable effect on the proteomic pattern, while ZnO NPs and ZnSO4
significantly modified B. subtilis metabolism. These results allowed us to
conclude that the effects of ZnO observed in this work were mainly attributable
to Zn dissolution in the culture media. Proteomic analysis highlighted twelve
modulated proteins related to central metabolism: MetE and MccB (cysteine
metabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,
YqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free
sulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed
central metabolism reorientation and showed that Zn stress induced oxidative
stress, probably as a consequence of thiol chelation stress by Zn ions. The
other patterns affected by ZnO and ZnSO4 were the stringent response and the
general stress response. Nine proteins involved in or controlled by the
stringent response showed a modified expression profile in the presence of ZnO
NPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An
increase in the ppGpp concentration confirmed the involvement of the stringent
response during a Zn stress. All these metabolic reorientations in response to
Zn stress were probably the result of complex regulatory mechanisms including
at least the stringent response via YwaC.
"
Fast Characterization of Segmental Duplications in Genome Assemblies,"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
"
Implementation of Control Strategies for Sterile Insect Techniques,"  In this paper, we propose a sex-structured entomological model that serves as
a basis for design of control strategies relying on releases of sterile male
mosquitoes (Aedes spp) and aiming at elimination of the wild vector population
in some target locality. We consider different types of releases (constant and
periodic impulsive), providing necessary conditions to reach elimination.
However, the main part of the paper is focused on the study of the periodic
impulsive control in different situations. When the size of wild mosquito
population cannot be assessed in real time, we propose the so-called open-loop
control strategy that relies on periodic impulsive releases of sterile males
with constant release size. Under this control mode, global convergence towards
the mosquito-free equilibrium is proved on the grounds of sufficient condition
that relates the size and frequency of releases. If periodic assessments
(either synchronized with releases or more sparse) of the wild population size
are available in real time, we propose the so-called closed-loop control
strategy, which is adjustable in accordance with reliable estimations of the
wild population sizes. Under this control mode, global convergence to the
mosquito-free equilibrium is proved on the grounds of another sufficient
condition that relates not only the size and frequency of periodic releases but
also the frequency of sparse measurements taken on wild populations. Finally,
we propose a mixed control strategy that combines open-loop and closed-loop
strategies. This control mode renders the best result, in terms of overall time
needed to reach elimination and the number of releases to be effectively
carried out during the whole release campaign, while requiring for a reasonable
amount of released sterile insects.
"
Mining within-trial oscillatory brain dynamics to address the variability of optimized spatial filters,"  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
"
PaccMann: Prediction of anticancer compound sensitivity with multi-modal attention-based neural networks,"  We present a novel approach for the prediction of anticancer compound
sensitivity by means of multi-modal attention-based neural networks (PaccMann).
In our approach, we integrate three key pillars of drug sensitivity, namely,
the molecular structure of compounds, transcriptomic profiles of cancer cells
as well as prior knowledge about interactions among proteins within cells. Our
models ingest a drug-cell pair consisting of SMILES encoding of a compound and
the gene expression profile of a cancer cell and predicts an IC50 sensitivity
value. Gene expression profiles are encoded using an attention-based encoding
mechanism that assigns high weights to the most informative genes. We present
and study three encoders for SMILES string of compounds: 1) bidirectional
recurrent 2) convolutional 3) attention-based encoders. We compare our devised
models against a baseline model that ingests engineered fingerprints to
represent the molecular structure. We demonstrate that using our
attention-based encoders, we can surpass the baseline model. The use of
attention-based encoders enhance interpretability and enable us to identify
genes, bonds and atoms that were used by the network to make a prediction.
"
Statistical mechanics of low-rank tensor decomposition,"  Often, large, high dimensional datasets collected across multiple modalities
can be organized as a higher order tensor. Low-rank tensor decomposition then
arises as a powerful and widely used tool to discover simple low dimensional
structures underlying such data. However, we currently lack a theoretical
understanding of the algorithmic behavior of low-rank tensor decompositions. We
derive Bayesian approximate message passing (AMP) algorithms for recovering
arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic
mean field theory to precisely characterize their performance. Our theory
reveals the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations. Moreover,
it reveals several qualitative surprises compared to the behavior of symmetric,
cubic tensor decomposition. Finally, we compare our AMP algorithm to the most
commonly used algorithm, alternating least squares (ALS), and demonstrate that
AMP significantly outperforms ALS in the presence of noise.
"
End-to-end distance and contour length distribution functions of DNA helices,"  We present a computational method to evaluate the end-to-end and the contour
length distribution functions of short DNA molecules described by a mesoscopic
Hamiltonian. The method generates a large statistical ensemble of possible
configurations for each dimer in the sequence, selects the global equilibrium
twist conformation for the molecule and determines the average base pair
distances along the molecule backbone. Integrating over the base pair radial
and angular fluctuations, we derive the room temperature distribution functions
as a function of the sequence length. The obtained values for the most probable
end-to-end distance and contour length distance, providing a measure of the
global molecule size, are used to examine the DNA flexibility at short length
scales. It is found that, also in molecules with less than $\sim 60$ base
pairs, coiled configurations maintain a large statistical weight and,
consistently, the persistence lengths may be much smaller than in kilo-base
DNA.
"
Online Estimation of Multiple Dynamic Graphs in Pattern Sequences,"  Many time-series data including text, movies, and biological signals can be
represented as sequences of correlated binary patterns. These patterns may be
described by weighted combinations of a few dominant structures that underpin
specific interactions among the binary elements. To extract the dominant
correlation structures and their contributions to generating data in a
time-dependent manner, we model the dynamics of binary patterns using the
state-space model of an Ising-type network that is composed of multiple
undirected graphs. We provide a sequential Bayes algorithm to estimate the
dynamics of weights on the graphs while gaining the graph structures online.
This model can uncover overlapping graphs underlying the data better than a
traditional orthogonal decomposition method, and outperforms an original
time-dependent full Ising model. We assess the performance of the method by
simulated data, and demonstrate that spontaneous activity of cultured
hippocampal neurons is represented by dynamics of multiple graphs.
"
Modelling thermo-electro-mechanical effects in orthotropic cardiac tissue,"  In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
"
Fooling the classifier: Ligand antagonism and adversarial examples,"  Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to ""boundary tilting""
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the ""feature-to-prototype"" transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
"
Matrix Completion and Performance Guarantees for Single Individual Haplotyping,"  Single individual haplotyping is an NP-hard problem that emerges when
attempting to reconstruct an organism's inherited genetic variations using data
typically generated by high-throughput DNA sequencing platforms. Genomes of
diploid organisms, including humans, are organized into homologous pairs of
chromosomes that differ from each other in a relatively small number of variant
positions. Haplotypes are ordered sequences of the nucleotides in the variant
positions of the chromosomes in a homologous pair; for diploids, haplotypes
associated with a pair of chromosomes may be conveniently represented by means
of complementary binary sequences. In this paper, we consider a binary matrix
factorization formulation of the single individual haplotyping problem and
efficiently solve it by means of alternating minimization. We analyze the
convergence properties of the alternating minimization algorithm and establish
theoretical bounds for the achievable haplotype reconstruction error. The
proposed technique is shown to outperform existing methods when applied to
synthetic as well as real-world Fosmid-based HapMap NA12878 datasets.
"
Kinetic Trans-assembly of DNA Nanostructures,"  The central dogma of molecular biology is the principal framework for
understanding how nucleic acid information is propagated and used by living
systems to create complex biomolecules. Here, by integrating the structural and
dynamic paradigms of DNA nanotechnology, we present a rationally designed
synthetic platform which functions in an analogous manner to create complex DNA
nanostructures. Starting from one type of DNA nanostructure, DNA strand
displacement circuits were designed to interact and pass along the information
encoded in the initial structure to mediate the self-assembly of a different
type of structure, the final output structure depending on the type of circuit
triggered. Using this concept of a DNA structure ""trans-assembling"" a different
DNA structure through non-local strand displacement circuitry, four different
schemes were implemented. Specifically, 1D ladder and 2D double-crossover (DX)
lattices were designed to kinetically trigger DNA circuits to activate
polymerization of either ring structures or another type of DX lattice under
enzyme-free, isothermal conditions. In each scheme, the desired multilayer
reaction pathway was activated, among multiple possible pathways, ultimately
leading to the downstream self-assembly of the correct output structure.
"
Robust parameter determination in epidemic models with analytical descriptions of uncertainties,"  Compartmental equations are primary tools in disease spreading studies. Their
predictions are accurate for large populations but disagree with empirical and
simulated data for finite populations, where uncertainties become a relevant
factor. Starting from the agent-based approach, we investigate the role of
uncertainties and autocorrelation functions in SIS epidemic model, including
their relationship with epidemiological variables. We find new differential
equations that take uncertainties into account. The findings provide improved
predictions to the SIS model and it can offer new insights for emerging
diseases.
"
Finite size effects for spiking neural networks with spatially dependent coupling,"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
"
WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,"  Learning sparse linear models with two-way interactions is desirable in many
application domains such as genomics. l1-regularised linear models are popular
to estimate sparse models, yet standard implementations fail to address
specifically the quadratic explosion of candidate two-way interactions in high
dimensions, and typically do not scale to genetic data with hundreds of
thousands of features. Here we present WHInter, a working set algorithm to
solve large l1-regularised problems with two-way interactions for binary design
matrices. The novelty of WHInter stems from a new bound to efficiently identify
working sets while avoiding to scan all features, and on fast computations
inspired from solutions to the maximum inner product search problem. We apply
WHInter to simulated and real genetic data and show that it is more scalable
and two orders of magnitude faster than the state of the art.
"
Fixed points of competitive threshold-linear networks,"  Threshold-linear networks (TLNs) are models of neural networks that consist
of simple, perceptron-like neurons and exhibit nonlinear dynamics that are
determined by the network's connectivity. The fixed points of a TLN, including
both stable and unstable equilibria, play a critical role in shaping its
emergent dynamics. In this work, we provide two novel characterizations for the
set of fixed points of a competitive TLN: the first is in terms of a simple
sign condition, while the second relies on the concept of domination. We apply
these results to a special family of TLNs, called combinatorial
threshold-linear networks (CTLNs), whose connectivity matrices are defined from
directed graphs. This leads us to prove a series of graph rules that enable one
to determine fixed points of a CTLN by analyzing the underlying graph.
Additionally, we study larger networks composed of smaller ""building block""
subnetworks, and prove several theorems relating the fixed points of the full
network to those of its components. Our results provide the foundation for a
kind of ""graphical calculus"" to infer features of the dynamics from a network's
connectivity.
"
Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching,"  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
"
PIMKL: Pathway Induced Multiple Kernel Learning,"  Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
"
Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning,"  We introduce Error Forward-Propagation, a biologically plausible mechanism to
propagate error feedback forward through the network. Architectural constraints
on connectivity are virtually eliminated for error feedback in the brain;
systematic backward connectivity is not used or needed to deliver error
feedback. Feedback as a means of assigning credit to neurons earlier in the
forward pathway for their contribution to the final output is thought to be
used in learning in the brain. How the brain solves the credit assignment
problem is unclear. In machine learning, error backpropagation is a highly
successful mechanism for credit assignment in deep multilayered networks.
Backpropagation requires symmetric reciprocal connectivity for every neuron.
From a biological perspective, there is no evidence of such an architectural
constraint, which makes backpropagation implausible for learning in the brain.
This architectural constraint is reduced with the use of random feedback
weights. Models using random feedback weights require backward connectivity
patterns for every neuron, but avoid symmetric weights and reciprocal
connections. In this paper, we practically remove this architectural
constraint, requiring only a backward loop connection for effective error
feedback. We propose reusing the forward connections to deliver the error
feedback by feeding the outputs into the input receiving layer. This mechanism,
Error Forward-Propagation, is a plausible basis for how error feedback occurs
deep in the brain independent of and yet in support of the functionality
underlying intricate network architectures. We show experimentally that
recurrent neural networks with two and three hidden layers can be trained using
Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving
$1.90\%$ and $11\%$ generalization errors respectively.
"
CANA: A python package for quantifying control and canalization in Boolean Networks,"  Logical models offer a simple but powerful means to understand the complex
dynamics of biochemical regulation, without the need to estimate kinetic
parameters. However, even simple automata components can lead to collective
dynamics that are computationally intractable when aggregated into networks. In
previous work we demonstrated that automata network models of biochemical
regulation are highly canalizing, whereby many variable states and their
groupings are redundant (Marques-Pita and Rocha, 2013). The precise charting
and measurement of such canalization simplifies these models, making even very
large networks amenable to analysis. Moreover, canalization plays an important
role in the control, robustness, modularity and criticality of Boolean network
dynamics, especially those used to model biochemical regulation (Gates and
Rocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new
publicly-available Python package that provides the necessary tools to extract,
measure, and visualize canalizing redundancy present in Boolean network models.
It extracts the pathways most effective in controlling dynamics in these
models, including their effective graph and dynamics canalizing map, as well as
other tools to uncover minimum sets of control variables.
"
The Frequent Network Neighborhood Mapping of the Human Hippocampus Shows Much More Frequent Neighbor Sets in Males Than in Females,"  In the study of the human connectome, the vertices and the edges of the
network of the human brain are analyzed: the vertices of the graphs are the
anatomically identified gray matter areas of the subjects; this set is exactly
the same for all the subjects. The edges of the graphs correspond to the axonal
fibers, connecting these areas. In the biological applications of graph theory,
it happens very rarely that scientists examine numerous large graphs on the
very same, labeled vertex set. Exactly this is the case in the study of the
connectomes. Because of the particularity of these sets of graphs, novel,
robust methods need to be developed for their analysis. Here we introduce the
new method of the Frequent Network Neighborhood Mapping for the connectome,
which serves as a robust identification of the neighborhoods of given vertices
of special interest in the graph. We apply the novel method for mapping the
neighborhoods of the human hippocampus and discover strong statistical
asymmetries between the connectomes of the sexes, computed from the Human
Connectome Project. We analyze 413 braingraphs, each with 463 nodes. We show
that the hippocampi of men have much more significantly frequent neighbor sets
than women; therefore, in a sense, the connections of the hippocampi are more
regularly distributed in men and more varied in women. Our results are in
contrast to the volumetric studies of the human hippocampus, where it was shown
that the relative volume of the hippocampus is the same in men and women.
"
"A mechanistic model of connector hubs, modularity, and cognition","  The human brain network is modular--comprised of communities of tightly
interconnected nodes. This network contains local hubs, which have many
connections within their own communities, and connector hubs, which have
connections diversely distributed across communities. A mechanistic
understanding of these hubs and how they support cognition has not been
demonstrated. Here, we leveraged individual differences in hub connectivity and
cognition. We show that a model of hub connectivity accurately predicts the
cognitive performance of 476 individuals in four distinct tasks. Moreover,
there is a general optimal network structure for cognitive
performance--individuals with diversely connected hubs and consequent modular
brain networks exhibit increased cognitive performance, regardless of the task.
Critically, we find evidence consistent with a mechanistic model in which
connector hubs tune the connectivity of their neighbors to be more modular
while allowing for task appropriate information integration across communities,
which increases global modularity and cognitive performance.
"
Doing good vs. avoiding bad in prosocial choice: A refined test and extension of the morality preference hypothesis,"  Prosociality is fundamental to human social life, and, accordingly, much
research has attempted to explain human prosocial behavior. Capraro and Rand
(Judgment and Decision Making, 13, 99-111, 2018) recently provided experimental
evidence that prosociality in anonymous, one-shot interactions (such as
Prisoner's Dilemma and Dictator Game experiments) is not driven by
outcome-based social preferences - as classically assumed - but by a
generalized morality preference for ""doing the right thing"". Here we argue that
the key experiments reported in Capraro and Rand (2018) comprise prominent
methodological confounds and open questions that bear on influential
psychological theory. Specifically, their design confounds: (i) preferences for
efficiency with self-interest; and (ii) preferences for action with preferences
for morality. Furthermore, their design fails to dissociate the preference to
do ""good"" from the preference to avoid doing ""bad"". We thus designed and
conducted a preregistered, refined and extended test of the morality preference
hypothesis (N=801). Consistent with this hypothesis, our findings indicate that
prosociality in the anonymous, one-shot Dictator Game is driven by preferences
for doing the morally right thing. Inconsistent with influential psychological
theory, however, our results suggest the preference to do ""good"" was as potent
as the preference to avoid doing ""bad"" in this case.
"
Finding influential nodes for integration in brain networks using optimal percolation theory,"  Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
"
Frictional Effects on RNA Folding: Speed Limit and Kramers Turnover,"  We investigated frictional effects on the folding rates of a human telomerase
hairpin (hTR HP) and H-type pseudoknot from the Beet Western Yellow Virus (BWYV
PK) using simulations of the Three Interaction Site (TIS) model for RNA. The
heat capacity from TIS model simulations, calculated using temperature replica
exchange simulations, reproduces nearly quantitatively the available
experimental data for the hTR HP. The corresponding results for BWYV PK serve
as predictions. We calculated the folding rates ($k_\mathrm{F}$) from more than
100 folding trajectories for each value of the solvent viscosity ($\eta$) at a
fixed salt concentration of 200 mM. By using the theoretical estimate
($\propto$$\sqrt{N}$ where $N$ is the number of nucleotides) for folding free
energy barrier, $k_\mathrm{F}$ data for both the RNAs are quantitatively fit
using one-dimensional Kramers' theory with two parameters specifying the
curvatures in the unfolded basin and the barrier top. In the high-friction
regime ($\eta\gtrsim10^{-5}\,\textrm{Pa\ensuremath{\cdot}s}$), for both HP and
PK, $k_\mathrm{F}$s decrease as $1/\eta$ whereas in the low friction regime,
$k_\mathrm{F}$ values increase as $\eta$ increases, leading to a maximum
folding rate at a moderate viscosity
($\sim10^{-6}\,\textrm{Pa\ensuremath{\cdot}s}$), which is the Kramers turnover.
From the fits, we find that the speed limit to RNA folding at water viscosity
is between 1 and 4 $\mathrm{\mu s}$, which is in accord with our previous
theoretical prediction as well as results from several single molecule
experiments. Both the RNA constructs fold by parallel pathways. Surprisingly,
we find that the flux through the pathways could be altered by changing solvent
viscosity, a prediction that is more easily testable in RNA than in proteins.
"
From jamming to collective cell migration through a boundary induced transition,"  Cell monolayers provide an interesting example of active matter, exhibiting a
phase transition from a flowing to jammed state as they age. Here we report
experiments and numerical simulations illustrating how a jammed cellular layer
rapidly reverts to a flowing state after a wound. Quantitative comparison
between experiments and simulations shows that cells change their
self-propulsion and alignement strength so that the system crosses a phase
transition line, which we characterize by finite-size scaling in an active
particle model. This wound-induced unjamming transition is found to occur
generically in epithelial, endothelial and cancer cells.
"
A Critical-like Collective State Leads to Long-range Cell Communication in Dictyostelium discoideum Aggregation,"  The transition from single-cell to multicellular behavior is important in
early development but rarely studied. The starvation-induced aggregation of the
social amoeba Dictyostelium discoideum into a multicellular slug is known to
result from single-cell chemotaxis towards emitted pulses of cyclic adenosine
monophosphate (cAMP). However, how exactly do transient short-range chemical
gradients lead to coherent collective movement at a macroscopic scale? Here, we
use a multiscale model verified by quantitative microscopy to describe
wide-ranging behaviors from chemotaxis and excitability of individual cells to
aggregation of thousands of cells. To better understand the mechanism of
long-range cell-cell communication and hence aggregation, we analyze cell-cell
correlations, showing evidence for self-organization at the onset of
aggregation (as opposed to following a leader cell). Surprisingly, cell
collectives, despite their finite size, show features of criticality known from
phase transitions in physical systems. Application of external cAMP
perturbations in our simulations near the sensitive critical point allows
steering cells into early aggregation and towards certain locations but not
once an aggregation center has been chosen.
"
A framework for cost-constrained genome rearrangement under Double Cut and Join,"  The study of genome rearrangement has many flavours, but they all are somehow
tied to edit distances on variations of a multi-graph called the breakpoint
graph. We study a weighted 2-break distance on Eulerian 2-edge-colored
multi-graphs, which generalizes weighted versions of several Double Cut and
Join problems, including those on genomes with unequal gene content. We affirm
the connection between cycle decompositions and edit scenarios first discovered
with the Sorting By Reversals problem. Using this we show that the problem of
finding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored
multi-graph - with a general cost function for 2-breaks - can be solved by
decomposing the problem into independent instances on simple alternating
cycles. For breakpoint graphs, and a more constrained cost function, based on
coloring the vertices, we give a polynomial-time algorithm for finding a
parsimonious 2-break scenario of minimum cost, while showing that finding a
non-parsimonious 2-break scenario of minimum cost is NP-Hard.
"
Bivariate Causal Discovery and its Applications to Gene Expression and Imaging Data Analysis,"  The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
"
Improving Protein Gamma-Turn Prediction Using Inception Capsule Networks,"  Protein gamma-turn prediction is useful in protein function studies and
experimental design. Several methods for gamma-turn prediction have been
developed, but the results were unsatisfactory with Matthew correlation
coefficients (MCC) around 0.2-0.4. One reason for the low prediction accuracy
is the limited capacity of the methods; in particular, the traditional
machine-learning methods like SVM may not extract high-level features well to
distinguish between turn or non-turn. Hence, it is worthwhile exploring new
machine-learning methods for the prediction. A cutting-edge deep neural
network, named Capsule Network (CapsuleNet), provides a new opportunity for
gamma-turn prediction. Even when the number of input samples is relatively
small, the capsules from CapsuleNet are very effective to extract high-level
features for classification tasks. Here, we propose a deep inception capsule
network for gamma-turn prediction. Its performance on the gamma-turn benchmark
GT320 achieved an MCC of 0.45, which significantly outperformed the previous
best method with an MCC of 0.38. This is the first gamma-turn prediction method
utilizing deep neural networks. Also, to our knowledge, it is the first
published bioinformatics application utilizing capsule network, which will
provides a useful example for the community.
"
Micro-sized cold atmospheric plasma source for brain and breast cancer treatment,"  Micro-sized cold atmospheric plasma (uCAP) has been developed to expand the
applications of CAP in cancer therapy. In this paper, uCAP devices with
different nozzle lengths were applied to investigate effects on both brain
(glioblastoma U87) and breast (MDA-MB-231) cancer cells. Various diagnostic
techniques were employed to evaluate the parameters of uCAP devices with
different lengths such as potential distribution, electron density, and optical
emission spectroscopy. The generation of short- and long-lived species (such as
hydroxyl radical (.OH), superoxide (O2-), hydrogen peroxide (H2O2), nitrite
(NO2-), et al) were studied. These data revealed that uCAP treatment with a 20
mm length tube has a stronger effect than that of the 60 mm tube due to the
synergetic effects of reactive species and free radicals. Reactive species
generated by uCAP enhanced tumor cell death in a dose-dependent fashion and was
not specific with regards to tumor cell type.
"
PT-Spike: A Precise-Time-Dependent Single Spike Neuromorphic Architecture with Efficient Supervised Learning,"  One of the most exciting advancements in AI over the last decade is the wide
adoption of ANNs, such as DNN and CNN, in many real-world applications.
However, the underlying massive amounts of computation and storage requirement
greatly challenge their applicability in resource-limited platforms like the
drone, mobile phone, and IoT devices etc. The third generation of neural
network model--Spiking Neural Network (SNN), inspired by the working mechanism
and efficiency of human brain, has emerged as a promising solution for
achieving more impressive computing and power efficiency within light-weighted
devices (e.g. single chip). However, the relevant research activities have been
narrowly carried out on conventional rate-based spiking system designs for
fulfilling the practical cognitive tasks, underestimating SNN's energy
efficiency, throughput, and system flexibility. Although the time-based SNN can
be more attractive conceptually, its potentials are not unleashed in realistic
applications due to lack of efficient coding and practical learning schemes. In
this work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,
namely ""PT-Spike"", is developed to bridge this gap. Three constituent
hardware-favorable techniques: precise single-spike temporal encoding,
efficient supervised temporal learning, and fast asymmetric decoding are
proposed accordingly to boost the energy efficiency and data processing
capability of the time-based SNN at a more compact neural network model size
when executing real cognitive tasks. Simulation results show that ""PT-Spike""
demonstrates significant improvements in network size, processing efficiency
and power consumption with marginal classification accuracy degradation when
compared with the rate-based SNN and ANN under the similar network
configuration.
"
"Modelling diverse sources of Clostridium difficile in the community: importance of animals, infants and asymptomatic carriers","  Clostridium difficile infections (CDIs) affect patients in hospitals and in
the community, but the relative importance of transmission in each setting is
unknown. We developed a mathematical model of C. difficile transmission in a
hospital and surrounding community that included infants, adults, and
transmission from animal reservoirs. We assessed the role of these transmission
routes in maintaining disease and evaluated the recommended classification
system for hospital and community-acquired CDIs. The reproduction number in the
hospital was <1 (range: 0.16-0.46) for all scenarios. Outside the hospital, the
reproduction number was >1 for nearly all scenarios without transmission from
animal reservoirs (range: 1.0-1.34). However, the reproduction number for the
human population was <1 if a minority (>3.5-26.0%) of human exposures
originated from animal reservoirs. Symptomatic adults accounted for <10%
transmission in the community. Under conservative assumptions, infants
accounted for 17% of community transmission. An estimated 33-40% of
community-acquired cases were reported but 28-39% of these reported cases were
misclassified as hospital-acquired by recommended definitions. Transmission
could be plausibly sustained by asymptomatically colonized adults and infants
in the community or exposure to animal reservoirs, but not hospital
transmission alone. Underreporting of community-onset cases and systematic
misclassification underplays the role of community transmission.
"
Non-equilibrium time dynamics of genetic evolution,"  Biological systems are typically highly open, non-equilibrium systems that
are very challenging to understand from a statistical mechanics perspective.
While statistical treatments of evolutionary biological systems have a long and
rich history, examination of the time-dependent non-equilibrium dynamics has
been less studied. In this paper we first derive a generalized master equation
in the genotype space for diploid organisms incorporating the processes of
selection, mutation, recombination, and reproduction. The master equation is
defined in terms of continuous time and can handle an arbitrary number of gene
loci and alleles, and can be defined in terms of an absolute population or
probabilities. We examine and analytically solve several prototypical cases
which illustrate the interplay of the various processes and discuss the
timescales of their evolution. The entropy production during the evolution
towards steady state is calculated and we find that it agrees with predictions
from non-equilibrium statistical mechanics where it is large when the
population distribution evolves towards a more viable genotype. The stability
of the non-equilibrium steady state is confirmed using the Glansdorff-Prigogine
criterion.
"
Burst Synchronization in A Scale-Free Neuronal Network with Inhibitory Spike-Timing-Dependent Plasticity,"  We are concerned about burst synchronization (BS), related to neural
information processes in health and disease, in the Barabási-Albert
scale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose
neurons. This inhibitory neuronal population has adaptive dynamic synaptic
strengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).
In previous works without considering iSTDP, BS was found to appear in a range
of noise intensities for fixed synaptic inhibition strengths. In contrast, in
our present work, we take into consideration iSTDP and investigate its effect
on BS by varying the noise intensity. Our new main result is to find occurrence
of a Matthew effect in inhibitory synaptic plasticity: good BS gets better via
LTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory
synaptic plasticity is in contrast to that in excitatory synaptic plasticity
where good (bad) synchronization gets better (worse) via LTP (LTD). We note
that, due to inhibition, the roles of LTD and LTP in inhibitory synaptic
plasticity are reversed in comparison with those in excitatory synaptic
plasticity. Moreover, emergences of LTD and LTP of synaptic inhibition
strengths are intensively investigated via a microscopic method based on the
distributions of time delays between the pre- and the post-synaptic burst onset
times. Finally, in the presence of iSTDP we investigate the effects of network
architecture on BS by varying the symmetric attachment degree $l^*$ and the
asymmetry parameter $\Delta l$ in the SFN.
"
Neural correlates of episodic memory in the Memento cohort,"  IntroductionThe free and cued selective reminding test is used to identify
memory deficits in mild cognitive impairment and demented patients. It allows
assessing three processes: encoding, storage, and recollection of verbal
episodic memory.MethodsWe investigated the neural correlates of these three
memory processes in a large cohort study. The Memento cohort enrolled 2323
outpatients presenting either with subjective cognitive decline or mild
cognitive impairment who underwent cognitive, structural MRI and, for a subset,
fluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding
was associated with a network including parietal and temporal cortices; storage
was mainly associated with entorhinal and parahippocampal regions, bilaterally;
retrieval was associated with a widespread network encompassing frontal
regions.DiscussionThe neural correlates of episodic memory processes can be
assessed in large and standardized cohorts of patients at risk for Alzheimer's
disease. Their relation to pathophysiological markers of Alzheimer's disease
remains to be studied.
"
Anticipating Persistent Infection,"  We explore the emergence of persistent infection in a closed region where the
disease progression of the individuals is given by the SIRS model, with an
individual becoming infected on contact with another infected individual within
a given range. We focus on the role of synchronization in the persistence of
contagion. Our key result is that higher degree of synchronization, both
globally in the population and locally in the neighborhoods, hinders
persistence of infection. Importantly, we find that early short-time asynchrony
appears to be a consistent precursor to future persistence of infection, and
can potentially provide valuable early warnings for sustained contagion in a
population patch. Thus transient synchronization can help anticipate the
long-term persistence of infection. Further we demonstrate that when the range
of influence of an infected individual is wider, one obtains lower persistent
infection. This counter-intuitive observation can also be understood through
the relation of synchronization to infection burn-out.
"
The exit time finite state projection scheme: bounding exit distributions and occupation measures of continuous-time Markov chains,"  We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
"
Who is the infector? Epidemic models with symptomatic and asymptomatic cases,"  What role do asymptomatically infected individuals play in the transmission
dynamics? There are many diseases, such as norovirus and influenza, where some
infected hosts show symptoms of the disease while others are asymptomatically
infected, i.e. do not show any symptoms. The current paper considers a class of
epidemic models following an SEIR (Susceptible $\to$ Exposed $\to$ Infectious
$\to$ Recovered) structure that allows for both symptomatic and asymptomatic
cases. The following question is addressed: what fraction $\rho$ of those
individuals getting infected are infected by symptomatic (asymptomatic) cases?
This is a more complicated question than the related question for the beginning
of the epidemic: what fraction of the expected number of secondary cases of a
typical newly infected individual, i.e. what fraction of the basic reproduction
number $R_0$, is caused by symptomatic individuals? The latter fraction only
depends on the type-specific reproduction numbers, while the former fraction
$\rho$ also depends on timing and hence on the probabilistic distributions of
latent and infectious periods of the two types (not only their means). Bounds
on $\rho$ are derived for the situation where these distributions (and even
their means) are unknown. Special attention is given to the class of Markov
models and the class of continuous-time Reed-Frost models as two classes of
distribution functions. We show how these two classes of models can exhibit
very different behaviour.
"
Linking de novo assembly results with long DNA reads by dnaasm-link application,"  Currently, third-generation sequencing techniques, which allow to obtain much
longer DNA reads compared to the next-generation sequencing technologies, are
becoming more and more popular. There are many possibilities to combine data
from next-generation and third-generation sequencing.
Herein, we present a new application called dnaasm-link for linking contigs,
a result of \textit{de novo} assembly of second-generation sequencing data,
with long DNA reads. Our tool includes an integrated module to fill gaps with a
suitable fragment of appropriate long DNA read, which improves the consistency
of the resulting DNA sequences. This feature is very important, in particular
for complex DNA regions, as presented in the paper. Finally, our implementation
outperforms other state-of-the-art tools in terms of speed and memory
requirements, which may enable the usage of the presented application for
organisms with a large genome, which is not possible in~existing applications.
The presented application has many advantages as (i) significant memory
optimization and reduction of computation time (ii) filling the gaps through
the appropriate fragment of a specified long DNA read (iii) reducing number of
spanned and unspanned gaps in the existing genome drafts.
The application is freely available to all users under GNU Library or Lesser
General Public License version 3.0 (LGPLv3). The demo application, docker image
and source code are available at this http URL.
"
"Chaotic dynamics of movements stochastic instability and the hypothesis of N.A. Bernstein about ""repetition without repetition""","  The registration of tremor was performed in two groups of subjects (15 people
in each group) with different physical fitness at rest and at a static loads of
3N. Each subject has been tested 15 series (number of series N=15) in both
states (with and without physical loads) and each series contained 15 samples
(n=15) of tremorogramm measurements (500 elements in each sample, registered
coordinates x1(t) of the finger position relative to eddy current sensor) of
the finger. Using non-parametric Wilcoxon test of each series of experiment a
pairwise comparison was made forming 15 tables in which the results of
calculation of pairwise comparison was presented as a matrix (15x15) for
tremorogramms are presented. The average number of hits random pairs of samples
(<k>) and standard deviation {\sigma} were calculated for all 15 matrices
without load and under the impact of physical load (3N), which showed an
increase almost in twice in the number k of pairs of matching samples of
tremorogramms at conditions of a static load. For all these samples it was
calculated special quasi-attractor (this square was presented the distinguishes
between physical load and without it. All samples present the stochastic
unstable state.
"
Impact of Intervals on the Emotional Effect in Western Music,"  Every art form ultimately aims to invoke an emotional response over the
audience, and music is no different. While the precise perception of music is a
highly subjective topic, there is an agreement in the ""feeling"" of a piece of
music in broad terms. Based on this observation, in this study, we aimed to
determine the emotional feeling associated with short passages of music;
specifically by analyzing the melodic aspects. We have used the dataset put
together by Eerola et. al. which is comprised of labeled short passages of film
music. Our initial survey of the dataset indicated that other than ""happy"" and
""sad"" labels do not possess a melodic structure. We transcribed the main melody
of the happy and sad tracks and used the intervals between the notes to
classify them. Our experiments have shown that treating a melody as a
bag-of-intervals do not possess any predictive power whatsoever, whereas
counting intervals with respect to the key of the melody yielded a classifier
with 85% accuracy.
"
Data-driven Probabilistic Atlases Capture Whole-brain Individual Variation,"  Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
""Frankenstein's creature paradigm"" which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
"
Single-cell diffraction tomography with optofluidic rotation about a tilted axis,"  Optical diffraction tomography (ODT) is a tomographic technique that can be
used to measure the three-dimensional (3D) refractive index distribution within
living cells without the requirement of any marker. In principle, ODT can be
regarded as a generalization of optical projection tomography which is
equivalent to computerized tomography (CT). Both optical tomographic techniques
require projection-phase images of cells measured at multiple angles. However,
the reconstruction of the 3D refractive index distribution post-measurement
differs for the two techniques. It is known that ODT yields better results than
projection tomography, because it takes into account diffraction of the imaging
light due to the refractive index structure of the sample. Here, we apply ODT
to biological cells in a microfluidic chip which combines optical trapping and
microfluidic flow to achieve an optofluidic single-cell rotation. In
particular, we address the problem that arises when the trapped cell is not
rotating about an axis perpendicular to the imaging plane, but instead about an
arbitrarily tilted axis. In this paper we show that the 3D reconstruction can
be improved by taking into account such a tilted rotational axis in the
reconstruction process.
"
Addressing Class Imbalance in Classification Problems of Noisy Signals by using Fourier Transform Surrogates,"  Randomizing the Fourier-transform (FT) phases of temporal-spatial data
generates surrogates that approximate examples from the data-generating
distribution. We propose such FT surrogates as a novel tool to augment and
analyze training of neural networks and explore the approach in the example of
sleep-stage classification. By computing FT surrogates of raw EEG, EOG, and EMG
signals of under-represented sleep stages, we balanced the CAPSLPDB sleep
database. We then trained and tested a convolutional neural network for sleep
stage classification, and found that our surrogate-based augmentation improved
the mean F1-score by 7%. As another application of FT surrogates, we formulated
an approach to compute saliency maps for individual sleep epochs. The
visualization is based on the response of inferred class probabilities under
replacement of short data segments by partial surrogates. To quantify how well
the distributions of the surrogates and the original data match, we evaluated a
trained classifier on surrogates of correctly classified examples, and
summarized these conditional predictions in a confusion matrix. We show how
such conditional confusion matrices can qualitatively explain the performance
of surrogates in class balancing. The FT-surrogate augmentation approach may
improve classification on noisy signals if carefully adapted to the data
distribution under analysis.
"
Fixed-Parameter Tractable Sampling for RNA Design with Multiple Target Structures,"  The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
"
Deep Robust Framework for Protein Function Prediction using Variable-Length Protein Sequences,"  Amino acid sequence portrays most intrinsic form of a protein and expresses
primary structure of protein. The order of amino acids in a sequence enables a
protein to acquire a particular stable conformation that is responsible for the
functions of the protein. This relationship between a sequence and its function
motivates the need to analyse the sequences for predicting protein functions.
Early generation computational methods using BLAST, FASTA, etc. perform
function transfer based on sequence similarity with existing databases and are
computationally slow. Although machine learning based approaches are fast, they
fail to perform well for long protein sequences (i.e., protein sequences with
more than 300 amino acid residues). In this paper, we introduce a novel method
for construction of two separate feature sets for protein sequences based on
analysis of 1) single fixed-sized segments and 2) multi-sized segments, using
bi-directional long short-term memory network. Further, model based on proposed
feature set is combined with the state of the art Multi-lable Linear
Discriminant Analysis (MLDA) features based model to improve the accuracy.
Extensive evaluations using separate datasets for biological processes and
molecular functions demonstrate promising results for both single-sized and
multi-sized segments based feature sets. While former showed an improvement of
+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%
respectively for two datasets over the state of the art MLDA based classifier.
After combining two models, there is a significant improvement of +7.41% and
+9.21% respectively for two datasets compared to MLDA based classifier.
Specifically, the proposed approach performed well for the long protein
sequences and superior overall performance.
"
A Framework for Implementing Machine Learning on Omics Data,"  The potential benefits of applying machine learning methods to -omics data
are becoming increasingly apparent, especially in clinical settings. However,
the unique characteristics of these data are not always well suited to machine
learning techniques. These data are often generated across different
technologies in different labs, and frequently with high dimensionality. In
this paper we present a framework for combining -omics data sets, and for
handling high dimensional data, making -omics research more accessible to
machine learning applications. We demonstrate the success of this framework
through integration and analysis of multi-analyte data for a set of 3,533
breast cancers. We then use this data-set to predict breast cancer patient
survival for individuals at risk of an impending event, with higher accuracy
and lower variance than methods trained on individual data-sets. We hope that
our pipelines for data-set generation and transformation will open up -omics
data to machine learning researchers. We have made these freely available for
noncommercial use at www.ccg.ai.
"
"Non-normality, reactivity, and intrinsic stochasticity in neural dynamics: a non-equilibrium potential approach","  Intrinsic stochasticity can induce highly non-trivial effects on dynamical
systems, including stochastic and coherence resonance, noise induced
bistability, noise-induced oscillations, to name but a few. In this paper we
revisit a mechanism first investigated in the context of neuroscience by which
relatively small demographic (intrinsic) fluctuations can lead to the emergence
of avalanching behavior in systems that are deterministically characterized by
a single stable fixed point (up state). The anomalously large response of such
systems to stochasticity stems (or is strongly associated with) the existence
of a ""non-normal"" stability matrix at the deterministic fixed point, which may
induce the system to be ""reactive"". Here, we further investigate this mechanism
by exploring the interplay between non-normality and intrinsic (demographic)
stochasticity, by employing a number of analytical and computational
approaches. We establish, in particular, that the resulting dynamics in this
type of systems cannot be simply derived from a scalar potential but,
additionally, one needs to consider a curl flux which describes the essential
non-equilibrium nature of this type of noisy non-normal systems. Moreover, we
shed further light on the origin of the phenomenon, introduce the novel concept
of ""non-linear reactivity"", and rationalize of the observed the value of the
emerging avalanche exponents.
"
Superregular grammars do not provide additional explanatory power but allow for a compact analysis of animal song,"  A pervasive belief with regard to the differences between human language and
animal vocal sequences (song) is that they belong to different classes of
computational complexity, with animal song belonging to regular languages,
whereas human language is superregular. This argument, however, lacks empirical
evidence since superregular analyses of animal song are understudied. The goal
of this paper is to perform a superregular analysis of animal song, using data
from gibbons as a case study, and demonstrate that a superregular analysis can
be effectively used with non-human data. A key finding is that a superregular
analysis does not increase explanatory power but rather provides for compact
analysis. For instance, fewer grammatical rules are necessary once
superregularity is allowed. This pattern is analogous to a previous
computational analysis of human language, and accordingly, the null hypothesis,
that human language and animal song are governed by the same type of
grammatical systems, cannot be rejected.
"
Revealing patterns in HIV viral load data and classifying patients via a novel machine learning cluster summarization method,"  HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
"
Generalized Similarity U: A Non-parametric Test of Association Based on Similarity,"  Second generation sequencing technologies are being increasingly used for
genetic association studies, where the main research interest is to identify
sets of genetic variants that contribute to various phenotype. The phenotype
can be univariate disease status, multivariate responses and even
high-dimensional outcomes. Considering the genotype and phenotype as two
complex objects, this also poses a general statistical problem of testing
association between complex objects. We here proposed a similarity-based test,
generalized similarity U (GSU), that can test the association between complex
objects. We first studied the theoretical properties of the test in a general
setting and then focused on the application of the test to sequencing
association studies. Based on theoretical analysis, we proposed to use
Laplacian kernel based similarity for GSU to boost power and enhance
robustness. Through simulation, we found that GSU did have advantages over
existing methods in terms of power and robustness. We further performed a whole
genome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative
(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with
imaging phenotype. We developed a C++ package for analysis of whole genome
sequencing data using GSU. The source codes can be downloaded at
this https URL.
"
Contrasting information theoretic decompositions of modulatory and arithmetic interactions in neural information processing systems,"  Biological and artificial neural systems are composed of many local
processors, and their capabilities depend upon the transfer function that
relates each local processor's outputs to its inputs. This paper uses a recent
advance in the foundations of information theory to study the properties of
local processors that use contextual input to amplify or attenuate transmission
of information about their driving inputs. This advance enables the information
transmitted by processors with two distinct inputs to be decomposed into those
components unique to each input, that shared between the two inputs, and that
which depends on both though it is in neither, i.e. synergy. The decompositions
that we report here show that contextual modulation has information processing
properties that contrast with those of all four simple arithmetic operators,
that it can take various forms, and that the form used in our previous studies
of artificial neural nets composed of local processors with both driving and
contextual inputs is particularly well-suited to provide the distinctive
capabilities of contextual modulation under a wide range of conditions. We
argue that the decompositions reported here could be compared with those
obtained from empirical neurobiological and psychophysical data under
conditions thought to reflect contextual modulation. That would then shed new
light on the underlying processes involved. Finally, we suggest that such
decompositions could aid the design of context-sensitive machine learning
algorithms.
"
DNA methylation markers to assess biological age,"  Among the different biomarkers of aging based on omics and clinical data, DNA
methylation clocks stand apart providing unmatched accuracy in assessing the
biological age of both humans and animal models of aging. Here, we discuss
robustness of DNA methylation clocks and bounds on their out-of-sample
performance and review computational strategies for development of the clocks.
"
Most Probable Evolution Trajectories in a Genetic Regulatory System Excited by Stable Lévy Noise,"  We study the most probable trajectories of the concentration evolution for
the transcription factor activator in a genetic regulation system, with
non-Gaussian stable Lévy noise in the synthesis reaction rate taking into
account. We calculate the most probable trajectory by spatially maximizing the
probability density of the system path, i.e., the solution of the associated
nonlocal Fokker-Planck equation. We especially examine those most probable
trajectories from low concentration state to high concentration state (i.e.,
the likely transcription regime) for certain parameters, in order to gain
insights into the transcription processes and the tipping time for the
transcription likely to occur. This enables us: (i) to visualize the progress
of concentration evolution (i.e., observe whether the system enters the
transcription regime within a given time period); (ii) to predict or avoid
certain transcriptions via selecting specific noise parameters in particular
regions in the parameter space. Moreover, we have found some peculiar or
counter-intuitive phenomena in this gene model system, including (a) a smaller
noise intensity may trigger the transcription process, while a larger noise
intensity can not, under the same asymmetric Lévy noise. This phenomenon does
not occur in the case of symmetric Lévy noise; (b) the symmetric Lévy
motion always induces transition to high concentration, but certain asymmetric
Lévy motions do not trigger the switch to transcription. These findings
provide insights for further experimental research, in order to achieve or to
avoid specific gene transcriptions, with possible relevance for medical
advances.
"
A Deep Learning Approach with an Attention Mechanism for Automatic Sleep Stage Classification,"  Automatic sleep staging is a challenging problem and state-of-the-art
algorithms have not yet reached satisfactory performance to be used instead of
manual scoring by a sleep technician. Much research has been done to find good
feature representations that extract the useful information to correctly
classify each epoch into the correct sleep stage. While many useful features
have been discovered, the amount of features have grown to an extent that a
feature reduction step is necessary in order to avoid the curse of
dimensionality. One reason for the need of such a large feature set is that
many features are good for discriminating only one of the sleep stages and are
less informative during other stages. This paper explores how a second feature
representation over a large set of pre-defined features can be learned using an
auto-encoder with a selective attention for the current sleep stage in the
training batch. This selective attention allows the model to learn feature
representations that focuses on the more relevant inputs without having to
perform any dimensionality reduction of the input data. The performance of the
proposed algorithm is evaluated on a large data set of polysomnography (PSG)
night recordings of patients with sleep-disordered breathing. The performance
of the auto-encoder with selective attention is compared with a regular
auto-encoder and previous works using a deep belief network (DBN).
"
Multilayer Network Modeling of Integrated Biological Systems,"  Biological systems, from a cell to the human brain, are inherently complex. A
powerful representation of such systems, described by an intricate web of
relationships across multiple scales, is provided by complex networks.
Recently, several studies are highlighting how simple networks -- obtained by
aggregating or neglecting temporal or categorical description of biological
data -- are not able to account for the richness of information characterizing
biological systems. More complex models, namely multilayer networks, are needed
to account for interdependencies, often varying across time, of biological
interacting units within a cell, a tissue or parts of an organism.
"
Discrete flow posteriors for variational inference in discrete dynamical systems,"  Each training step for a variational autoencoder (VAE) requires us to sample
from the approximate posterior, so we usually choose simple (e.g. factorised)
approximate posteriors in which sampling is an efficient computation that fully
exploits GPU parallelism. However, such simple approximate posteriors are often
insufficient, as they eliminate statistical dependencies in the posterior.
While it is possible to use normalizing flow approximate posteriors for
continuous latents, some problems have discrete latents and strong statistical
dependencies. The most natural approach to model these dependencies is an
autoregressive distribution, but sampling from such distributions is inherently
sequential and thus slow. We develop a fast, parallel sampling procedure for
autoregressive distributions based on fixed-point iterations which enables
efficient and accurate variational inference in discrete state-space latent
variable dynamical systems. To optimize the variational bound, we considered
two ways to evaluate probabilities: inserting the relaxed samples directly into
the pmf for the discrete distribution, or converting to continuous logistic
latent variables and interpreting the K-step fixed-point iterations as a
normalizing flow. We found that converting to continuous latent variables gave
considerable additional scope for mismatch between the true and approximate
posteriors, which resulted in biased inferences, we thus used the former
approach. Using our fast sampling procedure, we were able to realize the
benefits of correlated posteriors, including accurate uncertainty estimates for
one cell, and accurate connectivity estimates for multiple cells, in an order
of magnitude less time.
"
On the Three Properties of Stationary Populations and knotting with Non-Stationary Populations,"  We propose three properties that are related to the stationary population
identity (SPI) of population biology by connecting it with stationary
populations and non-stationary populations which are approaching stationarity.
These properties provide deeper insights into cohort formation in real-world
populations and the length of the duration for which stationary and
non-stationary conditions hold. The new concepts are based on the time gap
between the occurrence of stationary and non-stationary populations within the
SPI framework that we refer to as Oscillatory SPI and the Amplitude of SPI.
"
Environmental feedback drives cooperation in spatial social dilemmas,"  Exploiting others is beneficial individually but it could also be detrimental
globally. The reverse is also true: a higher cooperation level may change the
environment in a way that is beneficial for all competitors. To explore the
possible consequence of this feedback we consider a coevolutionary model where
the local cooperation level determines the payoff values of the applied
prisoner's dilemma game. We observe that the coevolutionary rule provides a
significantly higher cooperation level comparing to the traditional setup
independently of the topology of the applied interaction graph. Interestingly,
this cooperation supporting mechanism offers lonely defectors a high surviving
chance for a long period hence the relaxation to the final cooperating state
happens logarithmically slow. As a consequence, the extension of the
traditional evolutionary game by considering interactions with the environment
provides a good opportunity for cooperators, but their reward may arrive with
some delay.
"
Sex-biased dispersal: a review of the theory,"  Dispersal is ubiquitous throughout the tree of life: factors selecting for
dispersal include kin competition, inbreeding avoidance and spatiotemporal
variation in resources or habitat suitability. These factors differ in whether
they promote male and female dispersal equally strongly, and often selection on
dispersal of one sex depends on how much the other disperses. For example, for
inbreeding avoidance it can be sufficient that one sex disperses away from the
natal site. Attempts to understand sex-specific dispersal evolution have
created a rich body of theoretical literature, which we review here. We
highlight an interesting gap between empirical and theoretical literature. The
former associates different patterns of sex-biased dispersal with mating
systems, such as female-biased dispersal in monogamous birds and male-biased
dispersal in polygynous mammals. The predominant explanation is traceable back
to Greenwood's (1980) ideas of how successful philopatric or dispersing
individuals are at gaining mates or resources required to attract them. Theory,
however, has developed surprisingly independently of these ideas: predominant
ideas in theoretical work track how immigration and emigration change
relatedness patterns and alleviate competition for limiting resources,
typically considered sexually distinct, with breeding sites and fertilisable
females limiting reproductive success for females and males, respectively. We
show that the link between mating system and sex-biased dispersal is far from
resolved: there are studies showing that mating systems matter, but the
oft-stated association between polygyny and male-biased dispersal is not a
straightforward theoretical expectation... (full abstract in the PDF)
"
A geometrical analysis of global stability in trained feedback networks,"  Recurrent neural networks have been extensively studied in the context of
neuroscience and machine learning due to their ability to implement complex
computations. While substantial progress in designing effective learning
algorithms has been achieved in the last years, a full understanding of trained
recurrent networks is still lacking. Specifically, the mechanisms that allow
computations to emerge from the underlying recurrent dynamics are largely
unknown. Here we focus on a simple, yet underexplored computational setup: a
feedback architecture trained to associate a stationary output to a stationary
input. As a starting point, we derive an approximate analytical description of
global dynamics in trained networks which assumes uncorrelated connectivity
weights in the feedback and in the random bulk. The resulting mean-field theory
suggests that the task admits several classes of solutions, which imply
different stability properties. Different classes are characterized in terms of
the geometrical arrangement of the readout with respect to the input vectors,
defined in the high-dimensional space spanned by the network population. We
find that such approximate theoretical approach can be used to understand how
standard training techniques implement the input-output task in finite-size
feedback networks. In particular, our simplified description captures the local
and the global stability properties of the target solution, and thus predicts
training performance.
"
SMAGEXP: a galaxy tool suite for transcriptomics data meta-analysis,"  Bakground: With the proliferation of available microarray and high throughput
sequencing experiments in the public domain, the use of meta-analysis methods
increases. In these experiments, where the sample size is often limited,
meta-analysis offers the possibility to considerably enhance the statistical
power and give more accurate results. For those purposes, it combines either
effect sizes or results of single studies in a appropriate manner. R packages
metaMA and metaRNASeq perform meta-analysis on microarray and NGS data,
respectively. They are not interchangeable as they rely on statistical modeling
specific to each technology.
Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates
metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to
carry out meta-analysis of gene expression data, while taking care of their
specificities. We have developed this tool suite to analyse microarray data
from Gene Expression Omnibus (GEO) database or custom data from affymetrix
microarrays. These data are then combined to carry out meta-analysis using
metaMA package. SMAGEXP also offers to combine raw read counts from Next
Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In
both cases, key values, independent from the technology type, are reported to
judge the quality of the meta-analysis. These tools are available on the Galaxy
main tool shed. Source code, help and installation instructions are available
on github.
Conclusion: The use of Galaxy offers an easy-to-use gene expression
meta-analysis tool suite based on the metaMA and metaRNASeq packages.
"
The agreement distance of rooted phylogenetic networks,"  The minimal number of rooted subtree prune and regraft (rSPR) operations
needed to transform one phylogenetic tree into another one induces a metric on
phylogenetic trees - the rSPR-distance. The rSPR-distance between two
phylogenetic trees $T$ and $T'$ can be characterised by a maximum agreement
forest; a forest with a minimal number of components that covers both $T$ and
$T'$. The rSPR operation has recently been generalised to phylogenetic networks
with, among others, the subnetwork prune and regraft (SNPR) operation. Here, we
introduce maximum agreement graphs as an explicit representations of
differences of two phylogenetic networks, thus generalising maximum agreement
forests. We show that maximum agreement graphs induce a metric on phylogenetic
networks - the agreement distance. While this metric does not characterise the
distances induced by SNPR and other generalisations of rSPR, we prove that it
still bounds these distances with constant factors.
"
Introducing AIC model averaging in ecological niche modeling: a single-algorithm multi-model strategy to account for uncertainty in suitability predictions,"  Aim: The Akaike information Criterion (AIC) is widely used science to make
predictions about complex phenomena based on an entire set of models weighted
by Akaike weights. This approach (AIC model averaging; hereafter AvgAICc) is
often preferable than alternatives based on the selection of a single model.
Surprisingly, AvgAICc has not yet been introduced in ecological niche modeling
(ENM). We aimed to introduce AvgAICc in the context of ENM to serve both as an
optimality criterion in analyses that tune-up model parameters and as a
multi-model prediction strategy.
Innovation: Results from the AvgAICc approach differed from those of
alternative approaches with respect to model complexity, contribution of
environmental variables, and predicted amount and geographic location of
suitable conditions for the focal species. Two theoretical properties of the
AvgAICc approach might justify that future studies will prefer its use over
alternative approaches: (1) it is not limited to make predictions based on a
single model, but it also uses secondary models that might have important
predictive power absent in a given single model favored by alternative
optimality criteria; (2) it balances goodness of fit and model accuracy, this
being of critical importance in applications of ENM that require model
transference.
Main conclusions: Our introduction of the AvgAICc approach in ENM; its
theoretical properties, which are expected to confer advantages over
alternatives approaches; and the differences we found when comparing the
AvgAICc approach with alternative ones; should eventually lead to a wider use
of the AvgAICc approach. Our work should also promote further methodological
research comparing properties of the AvgAICc approach with respect to those of
alternative procedures.
"
Modeling polypharmacy side effects with graph convolutional networks,"  The use of drug combinations, termed polypharmacy, is common to treat
patients with complex diseases and co-existing conditions. However, a major
consequence of polypharmacy is a much higher risk of adverse side effects for
the patient. Polypharmacy side effects emerge because of drug-drug
interactions, in which activity of one drug may change if taken with another
drug. The knowledge of drug interactions is limited because these complex
relationships are rare, and are usually not observed in relatively small
clinical testing. Discovering polypharmacy side effects thus remains an
important challenge with significant implications for patient mortality. Here,
we present Decagon, an approach for modeling polypharmacy side effects. The
approach constructs a multimodal graph of protein-protein interactions,
drug-protein target interactions, and the polypharmacy side effects, which are
represented as drug-drug interactions, where each side effect is an edge of a
different type. Decagon is developed specifically to handle such multimodal
graphs with a large number of edge types. Our approach develops a new graph
convolutional neural network for multirelational link prediction in multimodal
networks. Decagon predicts the exact side effect, if any, through which a given
drug combination manifests clinically. Decagon accurately predicts polypharmacy
side effects, outperforming baselines by up to 69%. We find that it
automatically learns representations of side effects indicative of
co-occurrence of polypharmacy in patients. Furthermore, Decagon models
particularly well side effects with a strong molecular basis, while on
predominantly non-molecular side effects, it achieves good performance because
of effective sharing of model parameters across edge types. Decagon creates
opportunities to use large pharmacogenomic and patient data to flag and
prioritize side effects for follow-up analysis.
"
Machine learning of neuroimaging to diagnose cognitive impairment and dementia: a systematic review and comparative analysis,"  INTRODUCTION: Advanced machine learning methods might help to identify
dementia risk from neuroimaging, but their accuracy to date is unclear.
METHODS: We systematically reviewed the literature, 2006 to late 2016, for
machine learning studies differentiating healthy ageing through to dementia of
various types, assessing study quality, and comparing accuracy at different
disease boundaries.
RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs
healthy controls, used ADNI data, support vector machines and only T1-weighted
sequences. Accuracy was highest for differentiating AD from healthy controls,
and poor for differentiating healthy controls vs MCI vs AD, or MCI converters
vs non-converters. Accuracy increased using combined data types, but not by
data source, sample size or machine learning method.
DISCUSSION: Machine learning does not differentiate clinically-relevant
disease categories yet. More diverse datasets, combinations of different types
of data, and close clinical integration of machine learning would help to
advance the field.
"
GIFT: Guided and Interpretable Factorization for Tensors - An Application to Large-Scale Multi-platform Cancer Analysis,"  Given multi-platform genome data with prior knowledge of functional gene
sets, how can we extract interpretable latent relationships between patients
and genes? More specifically, how can we devise a tensor factorization method
which produces an interpretable gene factor matrix based on gene set
information while maintaining the decomposition quality and speed? We propose
GIFT, a Guided and Interpretable Factorization for Tensors. GIFT provides
interpretable factor matrices by encoding prior knowledge as a regularization
term in its objective function. Experiment results demonstrate that GIFT
produces interpretable factorizations with high scalability and accuracy, while
other methods lack interpretability. We apply GIFT to the PanCan12 dataset, and
GIFT reveals significant relations between cancers, gene sets, and genes, such
as influential gene sets for specific cancer (e.g., interferon-gamma response
gene set for ovarian cancer) or relations between cancers and genes (e.g., BRCA
cancer - APOA1 gene and OV, UCEC cancers - BST2 gene).
"
"Graphene oxide nanosheets disrupt lipid composition, Ca2+ homeostasis and synaptic transmission in primary cortical neurons","  Graphene has the potential to make a very significant impact on society, with
important applications in the biomedical field. The possibility to engineer
graphene-based medical devices at the neuronal interface is of particular
interest, making it imperative to determine the biocompatibility of graphene
materials with neuronal cells. Here we conducted a comprehensive analysis of
the effects of chronic and acute exposure of rat primary cortical neurons to
few-layers pristine graphene (GR) and monolayer graphene oxide (GO) flakes. By
combining a range of cell biology, microscopy, electrophysiology and omics
approaches we characterized the graphene neuron interaction from the first
steps of membrane contact and internalization to the long-term effects on cell
viability, synaptic transmission and cell metabolism. GR/GO flakes are found in
contact with the neuronal membrane, free in the cytoplasm and internalized
through the endolysosomal pathway, with no significant impact on neuron
viability. However, GO exposure selectively caused the inhibition of excitatory
transmission, paralleled by a reduction in the number of excitatory synaptic
contacts, and a concomitant enhancement of the inhibitory activity. This was
accompanied by induction of autophagy, altered Ca2+ dynamics and by a
downregulation of some of the main players in the regulation of Ca2+
homeostasis in both excitatory and inhibitory neurons. Our results show that,
although graphene exposure does not impact on neuron viability, it does
nevertheless have important effects on neuronal transmission and network
functionality, thus warranting caution when planning to employ this material
for neuro-biological applications.
"
A Simple Reservoir Model of Working Memory with Real Values,"  The prefrontal cortex is known to be involved in many high-level cognitive
functions, in particular, working memory. Here, we study to what extent a group
of randomly connected units (namely an Echo State Network, ESN) can store and
maintain (as output) an arbitrary real value from a streamed input, i.e. can
act as a sustained working memory unit. Furthermore, we explore to what extent
such an architecture can take advantage of the stored value in order to produce
non-linear computations. Comparison between different architectures (with and
without feedback, with and without a working memory unit) shows that an
explicit memory improves the performances.
"
Distinct dynamical behavior in random and all-to-all neuronal networks,"  Neuronal network dynamics depends on network structure. It is often assumed
that neurons are connected at random when their actual connectivity structure
is unknown. Such models are then often approximated by replacing the random
network by an all-to-all network, where every neuron is connected to all other
neurons. This mean-field approximation is a common approach in statistical
physics. In this paper we show that such approximation can be invalid. We solve
analytically a neuronal network model with binary-state neurons in both random
and all-to-all networks. We find strikingly different phase diagrams
corresponding to each network structure. Neuronal network dynamics is not only
different within certain parameter ranges, but it also undergoes different
bifurcations. Our results therefore suggest cautiousness when using mean-field
models based on all-to-all network topologies to represent random networks.
"
Interference effects of deleterious and beneficial mutations in large asexual populations,"  Linked beneficial and deleterious mutations are known to decrease the
fixation probability of a favorable mutation in large asexual populations.
While the hindering effect of strongly deleterious mutations on adaptive
evolution has been well studied, how weak deleterious mutations, either in
isolation or with superior beneficial mutations, influence the fixation of a
beneficial mutation has not been fully explored. Here, using a multitype
branching process, we obtain an accurate analytical expression for the fixation
probability when deleterious effects are weak, and exploit this result along
with the clonal interference theory to investigate the joint effect of linked
beneficial and deleterious mutations on the rate of adaptation. We find that
when the mutation rate is increased beyond the beneficial fitness effect, the
fixation probability of the beneficial mutant decreases from Haldane's
classical result towards zero. This has the consequence that above a critical
mutation rate that may depend on the population size, the adaptation rate
decreases exponentially with the mutation rate and is independent of the
population size. In addition, we find that for a range of mutation rates, both
beneficial and deleterious mutations interfere and impede the adaptation
process in large populations. We also study the evolution of mutation rates in
adapting asexual populations, and conclude that linked beneficial mutations
have a stronger influence on mutator fixation than the deleterious mutations.
"
RAFP-Pred: Robust Prediction of Antifreeze Proteins using Localized Analysis of n-Peptide Compositions,"  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)
to counter the otherwise lethal intracellular formation of ice. Structures and
sequences of various AFPs exhibit a high degree of heterogeneity, consequently
the prediction of the AFPs is considered to be a challenging task. In this
research, we propose to handle this arduous manifold learning task using the
notion of localized processing. In particular an AFP sequence is segmented into
two sub-segments each of which is analyzed for amino acid and di-peptide
compositions. We propose to use only the most significant features using the
concept of information gain (IG) followed by a random forest classification
approach. The proposed RAFP-Pred achieved an excellent performance on a number
of standard datasets. We report a high Youden's index
(sensitivity+specificity-1) value of 0.75 on the standard independent test data
set outperforming the AFP-PseAAC, AFP\_PSSM, AFP-Pred and iAFP by a margin of
0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB
dataset is found to be 83.19\% which is substantially superior to the 57.18\%
reported for the iAFP method.
"
Prioritizing network communities,"  Uncovering modular structure in networks is fundamental for systems in
biology, physics, and engineering. Community detection identifies candidate
modules as hypotheses, which then need to be validated through experiments,
such as mutagenesis in a biological laboratory. Only a few communities can
typically be validated, and it is thus important to prioritize which
communities to select for downstream experimentation. Here we develop CRank, a
mathematically principled approach for prioritizing network communities. CRank
efficiently evaluates robustness and magnitude of structural features of each
community and then combines these features into the community prioritization.
CRank can be used with any community detection method. It needs only
information provided by the network structure and does not require any
additional metadata or labels. However, when available, CRank can incorporate
domain-specific information to further boost performance. Experiments on many
large networks show that CRank effectively prioritizes communities, yielding a
nearly 50-fold improvement in community prioritization.
"
Contrastive Hebbian Learning with Random Feedback Weights,"  Neural networks are commonly trained to make predictions through learning
algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by
gradient backpropagation, is based on Hebb's rule and the contrastive
divergence algorithm. It operates in two phases, the forward (or free) phase,
where the data are fed to the network, and a backward (or clamped) phase, where
the target signals are clamped to the output layer of the network and the
feedback signals are transformed through the transpose synaptic weight
matrices. This implies symmetries at the synaptic level, for which there is no
evidence in the brain. In this work, we propose a new variant of the algorithm,
called random contrastive Hebbian learning, which does not rely on any synaptic
weights symmetries. Instead, it uses random matrices to transform the feedback
signals during the clamped phase, and the neural dynamics are described by
first order non-linear differential equations. The algorithm is experimentally
verified by solving a Boolean logic task, classification tasks (handwritten
digits and letters), and an autoencoding task. This article also shows how the
parameters affect learning, especially the random matrices. We use the
pseudospectra analysis to investigate further how random matrices impact the
learning process. Finally, we discuss the biological plausibility of the
proposed algorithm, and how it can give rise to better computational models for
learning.
"
Mechanisms for bacterial gliding motility on soft substrates,"  The motility mechanism of certain rod-shaped bacteria has long been a
mystery, since no external appendages are involved in their motion which is
known as gliding. However, the physical principles behind gliding motility
still remain poorly understood. Using myxobacteria as a canonical example of
such organisms, we identify here the physical principles behind gliding
motility, and develop a theoretical model that predicts a two-regime behavior
of the gliding speed as a function of the substrate stiffness. Our theory
describes the elastic, viscous, and capillary interactions between the
bacterial membrane carrying a traveling wave, the secreted slime acting as a
lubricating film, and the substrate which we model as a soft solid. Defining
the myxobacterial gliding as the horizontal motion on the substrate under zero
net force, we find the two-regime behavior is due to two different mechanisms
of motility thrust. On stiff substrates, the thrust arises from the bacterial
shape deformations creating a flow of slime that exerts a pressure along the
bacterial length. This pressure in conjunction with the bacterial shape
provides the necessary thrust for propulsion. However, we show that such a
mechanism cannot lead to gliding on very soft substrates. Instead, we show that
capillary effects lead to the formation of a ridge at the slime-substrate-air
interface, which creates a thrust in the form of a localized pressure gradient
at the tip of the bacteria. To test our theory, we perform experiments with
isolated cells on agar substrates of varying stiffness and find the measured
gliding speeds to be in good agreement with the predictions from our
elasto-capillary-hydrodynamic model. The physical mechanisms reported here
serve as an important step towards an accurate theory of friction and
substrate-mediated interaction between bacteria in a swarm of cells
proliferating in soft media.
"
Dynein catch bond as a mediator of codependent bidirectional cellular transport,"  Intracellular bidirectional transport of cargo on Microtubule filaments is
achieved by the collective action of oppositely directed dynein and kinesin
motors. Experimental investigations probing the nature of bidirectional
transport have found that in certain cases, inhibiting the activity of one type
of motor results in an overall decline in the motility of the cellular cargo in
both directions. This somewhat counter-intuitive observation, referred to as
paradox of codependence is inconsistent with the existing paradigm of a
mechanistic tug-of-war between oppositely directed motors. Existing theoretical
models do not take into account a key difference in the functionality of
kinesin and dynein. Unlike kinesin, dynein motors exhibit catchbonding, wherein
the unbinding rates of these motors from the filaments are seen to decrease
with increasing force on them. Incorporating this catchbonding behavior of
dynein in a theoretical model and using experimentally relevant measures
characterizing cargo transport, we show that the functional divergence of the
two motors species manifests itself as an internal regulatory mechanism for
bidirectional transport and resolves the paradox of codependence. Our model
reproduces the key experimental features in appropriate parameter regimes and
provides an unifying framework for bidirectional cargo transport.
"
Bayesian Detection of Abnormal ADS in Mutant Caenorhabditis elegans Embryos,"  Cell division timing is critical for cell fate specification and
morphogenesis during embryogenesis. How division timings are regulated among
cells during development is poorly understood. Here we focus on the comparison
of asynchrony of division between sister cells (ADS) between wild-type and
mutant individuals of Caenorhabditis elegans. Since the replicate number of
mutant individuals of each mutated gene, usually one, is far smaller than that
of wild-type, direct comparison of two distributions of ADS between wild-type
and mutant type, such as Kolmogorov- Smirnov test, is not feasible. On the
other hand, we find that sometimes ADS is correlated with the life span of
corresponding mother cell in wild-type. Hence, we apply a semiparametric
Bayesian quantile regression method to estimate the 95% confidence interval
curve of ADS with respect to life span of mother cell of wild-type individuals.
Then, mutant-type ADSs outside the corresponding confidence interval are
selected out as abnormal one with a significance level of 0.05. Simulation
study demonstrates the accuracy of our method and Gene Enrichment Analysis
validates the results of real data sets.
"
Inferring health conditions from fMRI-graph data,"  Automated classification methods for disease diagnosis are currently in the
limelight, especially for imaging data. Classification does not fully meet a
clinician's needs, however: in order to combine the results of multiple tests
and decide on a course of treatment, a clinician needs the likelihood of a
given health condition rather than binary classification yielded by such
methods. We illustrate how likelihoods can be derived step by step from first
principles and approximations, and how they can be assessed and selected,
illustrating our approach using fMRI data from a publicly available data set
containing schizophrenic and healthy control subjects. We start from the basic
assumption of partial exchangeability, and then the notion of sufficient
statistics and the ""method of translation"" (Edgeworth, 1898) combined with
conjugate priors. This method can be used to construct a likelihood that can be
used to compare different data-reduction algorithms. Despite the
simplifications and possibly unrealistic assumptions used to illustrate the
method, we obtain classification results comparable to previous, more realistic
studies about schizophrenia, whilst yielding likelihoods that can naturally be
combined with the results of other diagnostic tests.
"
Mean-field modeling of the basal ganglia-thalamocortical system. II. Dynamics of parkinsonian oscillations,"  Neuronal correlates of Parkinson's disease (PD) include a slowing of the
electroencephalogram (EEG) and enhanced synchrony at 3-7 and 7-30 Hz in the
basal ganglia, thalamus, and cortex. This study describes the dynamics of a
physiologically based mean-field model of the basal ganglia-thalamocortical
system, and shows how it accounts for key electrophysiological correlates of
PD. Its connectivity comprises partially segregated direct and indirect
pathways through the striatum, a hyperdirect pathway involving a
corticosubthalamic projection, thalamostriatal feedback, and local inhibition
in striatum and external pallidum (GPe). In a companion paper, realistic
steady-state firing rates were obtained for the healthy state, and after
dopamine loss modeled by weaker direct and stronger indirect pathways, reduced
intrapallidal inhibition, lower firing thresholds of the GPe and subthalamic
nucleus (STN), a stronger striato-GPe projection, and weaker cortical
interactions. Here we show that oscillations around 5 and 20 Hz can arise with
a strong indirect pathway, which also increases synchrony throughout the basal
ganglia. Further, increased theta power with nigrostriatal degeneration
correlates with reduced alpha power and peak frequency, matching experiments.
Unlike the hyperdirect pathway, the indirect pathway sustains oscillations with
realistic phase relationships. Changes in basal ganglia responses to transient
stimuli accord with experimental data. Reduced cortical gains due to both
nigrostriatal and mesocortical dopamine loss lead to slower cortical activity
changes and may be related to bradykinesia. Finally, increased EEG power found
in some studies may be partly explained by a lower effective GPe firing
threshold, reduced GPe-GPe inhibition, and/or weaker intracortical connections
in PD. Strict separation of the direct and indirect pathways is not necessary
for these results.
"
A novel methodology on distributed representations of proteins using their interacting ligands,"  The effective representation of proteins is a crucial task that directly
affects the performance of many bioinformatics problems. Related proteins
usually bind to similar ligands. Chemical characteristics of ligands are known
to capture the functional and mechanistic properties of proteins suggesting
that a ligand based approach can be utilized in protein representation. In this
study, we propose SMILESVec, a SMILES-based method to represent ligands and a
novel method to compute similarity of proteins by describing them based on
their ligands. The proteins are defined utilizing the word-embeddings of the
SMILES strings of their ligands. The performance of the proposed protein
description method is evaluated in protein clustering task using TransClust and
MCL algorithms. Two other protein representation methods that utilize protein
sequence, BLAST and ProtVec, and two compound fingerprint based protein
representation methods are compared. We showed that ligand-based protein
representation, which uses only SMILES strings of the ligands that proteins
bind to, performs as well as protein-sequence based representation methods in
protein clustering. The results suggest that ligand-based protein description
can be an alternative to the traditional sequence or structure based
representation of proteins and this novel approach can be applied to different
bioinformatics problems such as prediction of new protein-ligand interactions
and protein function annotation.
"
Cellular function given parametric variation: excitability in the Hodgkin-Huxley model,"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
"
Analysis of Extremely Obese Individuals Using Deep Learning Stacked Autoencoders and Genome-Wide Genetic Data,"  The aetiology of polygenic obesity is multifactorial, which indicates that
life-style and environmental factors may influence multiples genes to aggravate
this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have
been associated with BMI. However, identified loci only explain a small
proportion of the variation ob-served for this phenotype. The linear nature of
genome wide association studies (GWAS) used to identify associations between
genetic variants and the phenotype have had limited success in explaining the
heritability variation of BMI and shown low predictive capacity in
classification studies. GWAS ignores the epistatic interactions that less
significant variants have on the phenotypic outcome. In this paper we utilise a
novel deep learning-based methodology to reduce the high dimensional space in
GWAS and find epistatic interactions between SNPs for classification purposes.
SNPs were filtered based on the effects associations have with BMI. Since
Bonferroni adjustment for multiple testing is highly conservative, an important
proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,
only SNPs with p-values < 1x10-2 were considered for subsequent epistasis
analysis using stacked auto encoders (SAE). This allows the nonlinearity
present in SNP-SNP interactions to be discovered through progressively smaller
hidden layer units and to initialise a multi-layer feedforward artificial
neural network (ANN) classifier. The classifier is fine-tuned to classify
extremely obese and non-obese individuals. The best results were obtained with
2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,
Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it
was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,
Logloss=0.476864, AUC=0.85178 and MSE=0.156315).
"
An overview of the marine food web in Icelandic waters using Ecopath with Ecosim,"  Fishing activities have broad impacts that affect, although not exclusively,
the targeted stocks. These impacts affect predators and prey of the harvested
species, as well as the whole ecosystem it inhabits. Ecosystem models can be
used to study the interactions that occur within a system, including those
between different organisms and those between fisheries and targeted species.
Trophic web models like Ecopath with Ecosim (EwE) can handle fishing fleets as
a top predator, with top-down impact on harvested organisms. The aim of this
study was to better understand the Icelandic marine ecosystem and the
interactions within. This was done by constructing an EwE model of Icelandic
waters. The model was run from 1984 to 2013 and was fitted to time series of
biomass estimates, landings data and mean annual temperature. The final model
was chosen by selecting the model with the lowest Akaike information criterion.
A skill assessment was performed using the Pearson's correlation coefficient,
the coefficient of determination, the modelling efficiency and the reliability
index to evaluate the model performance. The model performed satisfactorily
when simulating previously estimated biomass and known landings. Most of the
groups with time series were estimated to have top-down control over their
prey. These are harvested species with direct and/or indirect links to lower
trophic levels and future fishing policies should take this into account. This
model could be used as a tool to investigate how such policies could impact the
marine ecosystem in Icelandic waters.
"
Increased adaptability to rapid environmental change can more than make up for the two-fold cost of males,"  The famous ""two-fold cost of sex"" is really the cost of anisogamy -- why
should females mate with males who do not contribute resources to offspring,
rather than isogamous partners who contribute equally? In typical anisogamous
populations, a single very fit male can have an enormous number of offspring,
far larger than is possible for any female or isogamous individual. If the
sexual selection on males aligns with the natural selection on females,
anisogamy thus allows much more rapid adaptation via super-successful males. We
show via simulations that this effect can be sufficient to overcome the
two-fold cost and maintain anisogamy against isogamy in populations adapting to
environmental change. The key quantity is the variance in male fitness -- if
this exceeds what is possible in an isogamous population, anisogamous
populations can win out in direct competition by adapting faster.
"
Mechanism Deduction from Noisy Chemical Reaction Networks,"  We introduce KiNetX, a fully automated meta-algorithm for the kinetic
analysis of complex chemical reaction networks derived from semi-accurate but
efficient electronic structure calculations. It is designed to (i) accelerate
the automated exploration of such networks, and (ii) cope with model-inherent
errors in electronic structure calculations on elementary reaction steps. We
developed and implemented KiNetX to possess three features. First, KiNetX
evaluates the kinetic relevance of every species in a (yet incomplete) reaction
network to confine the search for new elementary reaction steps only to those
species that are considered possibly relevant. Second, KiNetX identifies and
eliminates all kinetically irrelevant species and elementary reactions to
reduce a complex network graph to a comprehensible mechanism. Third, KiNetX
estimates the sensitivity of species concentrations toward changes in
individual rate constants (derived from relative free energies), which allows
us to systematically select the most efficient electronic structure model for
each elementary reaction given a predefined accuracy. The novelty of KiNetX
consists in the rigorous propagation of correlated free-energy uncertainty
through all steps of our kinetic analyis. To examine the performance of KiNetX,
we developed AutoNetGen. It semirandomly generates chemistry-mimicking reaction
networks by encoding chemical logic into their underlying graph structure.
AutoNetGen allows us to consider a vast number of distinct chemistry-like
scenarios and, hence, to discuss assess the importance of rigorous uncertainty
propagation in a statistical context. Our results reveal that KiNetX reliably
supports the deduction of product ratios, dominant reaction pathways, and
possibly other network properties from semi-accurate electronic structure data.
"
Robots as Powerful Allies for the Study of Embodied Cognition from the Bottom Up,"  A large body of compelling evidence has been accumulated demonstrating that
embodiment - the agent's physical setup, including its shape, materials,
sensors and actuators - is constitutive for any form of cognition and as a
consequence, models of cognition need to be embodied. In contrast to methods
from empirical sciences to study cognition, robots can be freely manipulated
and virtually all key variables of their embodiment and control programs can be
systematically varied. As such, they provide an extremely powerful tool of
investigation. We present a robotic bottom-up or developmental approach,
focusing on three stages: (a) low-level behaviors like walking and reflexes,
(b) learning regularities in sensorimotor spaces, and (c) human-like cognition.
We also show that robotic based research is not only a productive path to
deepening our understanding of cognition, but that robots can strongly benefit
from human-like cognition in order to become more autonomous, robust,
resilient, and safe.
"
On the use and abuse of Price equation concepts in ecology,"  In biodiversity and ecosystem functioning (BEF) research, the Loreau-Hector
(LH) statistical scheme is widely-used to partition the effect of biodiversity
on ecosystem properties into a ""complementarity effect"" and a ""selection
effect"". This selection effect was originally considered analogous to the
selection term in the Price equation from evolutionary biology. However, a key
paper published over thirteen years ago challenged this interpretation by
devising a new tripartite partitioning scheme that purportedly quantified the
role of selection in biodiversity experiments more accurately. This tripartite
method, as well as its recent spatiotemporal extension, were both developed as
an attempt to apply the Price equation in a BEF context. Here, we demonstrate
that the derivation of this tripartite method, as well as its spatiotemporal
extension, involve a set of incoherent and nonsensical mathematical arguments
driven largely by naïve visual analogies with the original Price equation,
that result in neither partitioning scheme quantifying any real property in the
natural world. Furthermore, we show that Loreau and Hector's original selection
effect always represented a true analog of the original Price selection term,
making the tripartite partitioning scheme a nonsensical solution to a
non-existent problem [...]
"
AI4AI: Quantitative Methods for Classifying Host Species from Avian Influenza DNA Sequence,"  Avian Influenza breakouts cause millions of dollars in damage each year
globally, especially in Asian countries such as China and South Korea. The
impact magnitude of a breakout directly correlates to time required to fully
understand the influenza virus, particularly the interspecies pathogenicity.
The procedure requires laboratory tests that require resources typically
lacking in a breakout emergency. In this study, we propose new quantitative
methods utilizing machine learning and deep learning to correctly classify host
species given raw DNA sequence data of the influenza virus, and provide
probabilities for each classification. The best deep learning models achieve
top-1 classification accuracy of 47%, and top-3 classification accuracy of 82%,
on a dataset of 11 host species classes.
"
"Response to Comment on ""Cell nuclei have lower refractive index and mass density than cytoplasm""","  In a recent study entitled ""Cell nuclei have lower refractive index and mass
density than cytoplasm"", we provided strong evidence indicating that the
nuclear refractive index (RI) is lower than the RI of the cytoplasm for several
cell lines. In a complementary study in 2017, entitled ""Is the nuclear
refractive index lower than cytoplasm? Validation of phase measurements and
implications for light scattering technologies"", Steelman et al. observed a
lower nuclear RI also for other cell lines and ruled out methodological error
sources such as phase wrapping and scattering effects. Recently, Yurkin
composed a comment on these 2 publications, entitled ""How a phase image of a
cell with nucleus refractive index smaller than that of the cytoplasm should
look like?"", putting into question the methods used for measuring the cellular
and nuclear RI in the aforementioned publications by suggesting that a lower
nuclear RI would produce a characteristic dip in the measured phase profile in
situ. We point out the difficulty of identifying this dip in the presence of
other cell organelles, noise, or blurring due to the imaging point spread
function. Furthermore, we mitigate Yurkin's concerns regarding the ability of
the simple-transmission approximation to compare cellular and nuclear RI by
analyzing a set of phase images with a novel, scattering-based approach. We
conclude that the absence of a characteristic dip in the measured phase
profiles does not contradict the usage of the simple-transmission approximation
for the determination of the average cellular or nuclear RI. Our response can
be regarded as an addition to the response by Steelman, Eldridge and Wax. We
kindly ask the reader to attend to their thorough ascertainment prior to
reading our response.
"
Autoregressive Point-Processes as Latent State-Space Models: a Moment-Closure Approach to Fluctuations and Autocorrelations,"  Modeling and interpreting spike train data is a task of central importance in
computational neuroscience, with significant translational implications. Two
popular classes of data-driven models for this task are autoregressive Point
Process Generalized Linear models (PPGLM) and latent State-Space models (SSM)
with point-process observations. In this letter, we derive a mathematical
connection between these two classes of models. By introducing an auxiliary
history process, we represent exactly a PPGLM in terms of a latent, infinite
dimensional dynamical system, which can then be mapped onto an SSM by basis
function projections and moment closure. This representation provides a new
perspective on widely used methods for modeling spike data, and also suggests
novel algorithmic approaches to fitting such models. We illustrate our results
on a phasic bursting neuron model, showing that our proposed approach provides
an accurate and efficient way to capture neural dynamics.
"
Improving brain computer interface performance by data augmentation with conditional Deep Convolutional Generative Adversarial Networks,"  One of the big restrictions in brain computer interface field is the very
limited training samples, it is difficult to build a reliable and usable system
with such limited data. Inspired by generative adversarial networks, we propose
a conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks
method to generate more artificial EEG signal automatically for data
augmentation to improve the performance of convolutional neural networks in
brain computer interface field and overcome the small training dataset
problems. We evaluate the proposed cDCGAN method on BCI competition dataset of
motor imagery. The results show that the generated artificial EEG data from
Gaussian noise can learn the features from raw EEG data and has no less than
the classification accuracy of raw EEG data in the testing dataset. Also by
using generated artificial data can effectively improve classification accuracy
at the same model with limited training data.
"
Geometric comparison of phylogenetic trees with different leaf sets,"  The metric space of phylogenetic trees defined by Billera, Holmes, and
Vogtmann, which we refer to as BHV space, provides a natural geometric setting
for describing collections of trees on the same set of taxa. However, it is
sometimes necessary to analyze collections of trees on non-identical taxa sets
(i.e., with different numbers of leaves), and in this context it is not evident
how to apply BHV space. Davidson et al. recently approached this problem by
describing a combinatorial algorithm extending tree topologies to regions in
higher dimensional tree spaces, so that one can quickly compute which
topologies contain a given tree as partial data. In this paper, we refine and
adapt their algorithm to work for metric trees to give a full characterization
of the subspace of extensions of a subtree. We describe how to apply our
algorithm to define and search a space of possible supertrees and, for a
collection of tree fragments with different leaf sets, to measure their
compatibility.
"
Single-molecule imaging of DNA gyrase activity in living Escherichia coli,"  Bacterial DNA gyrase introduces negative supercoils into chromosomal DNA and
relaxes positive supercoils introduced by replication and transiently by
transcription. Removal of these positive supercoils is essential for
replication fork progression and for the overall unlinking of the two duplex
DNA strands, as well as for ongoing transcription. To address how gyrase copes
with these topological challenges, we used high-speed single-molecule
fluorescence imaging in live Escherichia coli cells. We demonstrate that at
least 300 gyrase molecules are stably bound to the chromosome at any time, with
~12 enzymes enriched near each replication fork. Trapping of reaction
intermediates with ciprofloxacin revealed complexes undergoing catalysis. Dwell
times of ~2 s were observed for the dispersed gyrase molecules, which we
propose maintain steady-state levels of negative supercoiling of the
chromosome. In contrast, the dwell time of replisome-proximal molecules was ~8
s, consistent with these catalyzing processive positive supercoil relaxation in
front of the progressing replisome.
"
Reliable counting of weakly labeled concepts by a single spiking neuron model,"  Making an informed, correct and quick decision can be life-saving. It's
crucial for animals during an escape behaviour or for autonomous cars during
driving. The decision can be complex and may involve an assessment of the
amount of threats present and the nature of each threat. Thus, we should expect
early sensory processing to supply classification information fast and
accurately, even before relying the information to higher brain areas or more
complex system components downstream. Today, advanced convolutional artificial
neural networks can successfully solve visual detection and classification
tasks and are commonly used to build complex decision making systems. However,
in order to perform well on these tasks they require increasingly complex,
""very deep"" model structure, which is costly in inference run-time, energy
consumption and number of training samples, only trainable on cloud-computing
clusters. A single spiking neuron has been shown to be able to solve
recognition tasks for homogeneous Poisson input statistics, a commonly used
model for spiking activity in the neocortex. When modeled as leaky integrate
and fire with gradient decent learning algorithm it was shown to posses a
variety of complex computational capabilities. Here we improve its
implementation. We also account for more natural stimulus generated inputs that
deviate from this homogeneous Poisson spiking. The improved gradient-based
local learning rule allows for significantly better and stable generalization.
We also show that with its improved capabilities it can count weakly labeled
concepts by applying our model to a problem of multiple instance learning (MIL)
with counting where labels are only available for collections of concepts. In
this counting MNIST task the neuron exploits the improved implementation and
outperforms conventional ConvNet architecture under similar condtions.
"
The clock of chemical evolution,"  Chemical evolution is essential in understanding the origins of life. We
present a theory for the evolution of molecule masses and show that small
molecules grow by random diffusion and large molecules by a preferential
attachment process leading eventually to life's molecules. It reproduces
correctly the distribution of molecules found via mass spectroscopy for the
Murchison meteorite and estimates the start of chemical evolution back to 12.8
billion years following the birth of stars and supernovae. From the Frontier
mass between the random and preferential attachment dynamics the birth time of
molecule families can be estimated. Amino acids emerge about 165 million years
after the start of evolution. Using the scaling of reaction rates with the
distance of the molecules in space we recover correctly the few days emergence
time of amino acids in the Miller-Urey experiment. The distribution of
interstellar and extragalactic molecules are both consistent with the
evolutionary mass distribution, and their age is estimated to 108 and 65
million years after the start of evolution. From the model, we can determine
the number of different molecule compositions at the time of the creation of
Earth to be 1.6 million and the number of molecule compositions in interstellar
space to a mere 719.
"
Component response rate variation drives stability in large complex systems,"  The stability of a complex system generally decreases with increasing system
size and interconnectivity, a counterintuitive result of widespread importance
across the physical, life, and social sciences. Despite recent interest in the
relationship between system properties and stability, the effect of variation
in the response rate of individual system components remains unconsidered. Here
I vary the component response rates ($\boldsymbol{\gamma}$) of randomly
generated complex systems. I show that when component response rates vary, the
potential for system stability is markedly increased. Variation in
$\boldsymbol{\gamma}$ becomes increasingly important as system size increases,
such that the largest stable complex systems would be unstable if not for
$\boldsymbol{Var(\gamma)}$. My results reveal a previously unconsidered driver
of system stability that is likely to be pervasive across all complex systems.
"
A practical guide to the simultaneous determination of protein structure and dynamics using metainference,"  Accurate protein structural ensembles can be determined with metainference, a
Bayesian inference method that integrates experimental information with prior
knowledge of the system and deals with all sources of uncertainty and errors as
well as with system heterogeneity. Furthermore, metainference can be
implemented using the metadynamics approach, which enables the computational
study of complex biological systems requiring extensive conformational
sampling. In this chapter, we provide a step-by-step guide to perform and
analyse metadynamic metainference simulations using the ISDB module of the
open-source PLUMED library, as well as a series of practical tips to avoid
common mistakes. Specifically, we will guide the reader in the process of
learning how to model the structural ensemble of a small disordered peptide by
combining state-of-the-art molecular mechanics force fields with nuclear
magnetic resonance data, including chemical shifts, scalar couplings and
residual dipolar couplings.
"
Maturation Trajectories of Cortical Resting-State Networks Depend on the Mediating Frequency Band,"  The functional significance of resting state networks and their abnormal
manifestations in psychiatric disorders are firmly established, as is the
importance of the cortical rhythms in mediating these networks. Resting state
networks are known to undergo substantial reorganization from childhood to
adulthood, but whether distinct cortical rhythms, which are generated by
separable neural mechanisms and are often manifested abnormally in psychiatric
conditions, mediate maturation differentially, remains unknown. Using
magnetoencephalography (MEG) to map frequency band specific maturation of
resting state networks from age 7 to 29 in 162 participants (31 independent),
we found significant changes with age in networks mediated by the beta
(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated
networks followed an expected asymptotic trajectory, but beta band mediated
networks followed a linear trajectory. Network integration increased with age
in gamma band mediated networks, while local segregation increased with age in
beta band mediated networks. Spatially, the hubs that changed in importance
with age in the beta band mediated networks had relatively little overlap with
those that showed the greatest changes in the gamma band mediated networks.
These findings are relevant for our understanding of the neural mechanisms of
cortical maturation, in both typical and atypical development.
"
Divergence Framework for EEG based Multiclass Motor Imagery Brain Computer Interface,"  Similar to most of the real world data, the ubiquitous presence of
non-stationarities in the EEG signals significantly perturb the feature
distribution thus deteriorating the performance of Brain Computer Interface. In
this letter, a novel method is proposed based on Joint Approximate
Diagonalization (JAD) to optimize stationarity for multiclass motor imagery
Brain Computer Interface (BCI) in an information theoretic framework.
Specifically, in the proposed method, we estimate the subspace which optimizes
the discriminability between the classes and simultaneously preserve
stationarity within the motor imagery classes. We determine the subspace for
the proposed approach through optimization using gradient descent on an
orthogonal manifold. The performance of the proposed stationarity enforcing
algorithm is compared to that of baseline One-Versus-Rest (OVR)-CSP and JAD on
publicly available BCI competition IV dataset IIa. Results show that an
improvement in average classification accuracies across the subjects over the
baseline algorithms and thus essence of alleviating within session
non-stationarities.
"
Ergodicity analysis and antithetic integral control of a class of stochastic reaction networks with delays,"  Delays are an important phenomenon arising in a wide variety of real world
systems. They occur in biological models because of diffusion effects or as
simplifying modeling elements. We propose here to consider delayed stochastic
reaction networks. The difficulty here lies in the fact that the state-space of
a delayed reaction network is infinite-dimensional, which makes their analysis
more involved. We demonstrate here that a particular class of stochastic
time-varying delays, namely those that follow a phase-type distribution, can be
exactly implemented in terms of a chemical reaction network. Hence, any
delay-free network can be augmented to incorporate those delays through the
addition of delay-species and delay-reactions. Hence, for this class of
stochastic delays, which can be used to approximate any delay distribution
arbitrarily accurately, the state-space remains finite-dimensional and,
therefore, standard tools developed for standard reaction network still apply.
In particular, we demonstrate that for unimolecular mass-action reaction
networks that the delayed stochastic reaction network is ergodic if and only if
the non-delayed network is ergodic as well. Bimolecular reactions are more
difficult to consider but an analogous result is also obtained. These results
tell us that delays that are phase-type distributed, regardless of their
distribution, are not harmful to the ergodicity property of reaction networks.
We also prove that the presence of those delays adds convolution terms in the
moment equation but does not change the value of the stationary means compared
to the delay-free case. Finally, the control of a certain class of delayed
stochastic reaction network using a delayed antithetic integral controller is
considered. It is proven that this controller achieves its goal provided that
the delay-free network satisfy the conditions of ergodicity and
output-controllability.
"
"Structural subnetwork evolution across the life-span: rich-club, feeder, seeder","  The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
"
Determinants of cyclization-decyclization kinetics of short DNA with sticky ends,"  Cyclization of DNA with sticky ends is commonly used to construct DNA
minicircles and to measure DNA bendability. The cyclization probability of
short DNA (< 150 bp) has a strong length dependence, but how it depends on the
rotational positioning of the sticky ends around the helical axis is less
clear. To shed light upon the determinants of the cyclization probability of
short DNA, we measured cyclization and decyclization rates of ~100-bp DNA with
sticky ends over two helical periods using single-molecule Fluorescence
Resonance Energy Transfer (FRET). The cyclization rate increases monotonically
with length, indicating no excess twisting, while the decyclization rate
oscillates with length, higher at half-integer helical turns and lower at
integer helical turns. The oscillation profile is kinetically and
thermodynamically consistent with a three-state cyclization model in which
sticky-ended short DNA first bends into a torsionally-relaxed teardrop, and
subsequently transitions to a more stable loop upon terminal base stacking. We
also show that the looping probability density (the J factor) extracted from
this study is in good agreement with the worm-like chain model near 100 bp. For
shorter DNA, we discuss various experimental factors that prevent an accurate
measurement of the J factor.
"
Emergence of grid-like representations by training recurrent neural networks to perform spatial localization,"  Decades of research on the neural code underlying spatial navigation have
revealed a diverse set of neural response properties. The Entorhinal Cortex
(EC) of the mammalian brain contains a rich set of spatial correlates,
including grid cells which encode space using tessellating patterns. However,
the mechanisms and functional significance of these spatial representations
remain largely mysterious. As a new way to understand these neural
representations, we trained recurrent neural networks (RNNs) to perform
navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find
that grid-like spatial response patterns emerge in trained networks, along with
units that exhibit other spatial correlates, including border cells and
band-like cells. All these different functional types of neurons have been
observed experimentally. The order of the emergence of grid-like and border
cells is also consistent with observations from developmental studies.
Together, our results suggest that grid cells, border cells and others as
observed in EC may be a natural solution for representing space efficiently
given the predominant recurrent connections in the neural circuits.
"
When few survive to tell the tale: thymus and gonad as auditioning organs: historical overview,"  Unlike other organs, the thymus and gonads generate non-uniform cell
populations, many members of which perish, and a few survive. While it is
recognized that thymic cells are 'audited' to optimize an organism's immune
repertoire, whether gametogenesis could be orchestrated similarly to favour
high quality gametes is uncertain. Ideally, such quality would be affirmed at
early stages before the commitment of extensive parental resources. A case is
here made that, along the lines of a previously proposed lymphocyte quality
control mechanism, gamete quality can be registered indirectly through
detection of incompatibilities between proteins encoded by the grandparental
DNA sequences within the parent from which haploid gametes are meiotically
derived. This 'stress test' is achieved in the same way that thymic screening
for potential immunological incompatibilities is achieved - by 'promiscuous'
expression, under the influence of the AIRE protein, of the products of genes
that are not normally specific for that organ. Consistent with this, the Aire
gene is expressed in both thymus and gonads, and AIRE deficiency impedes
function in both organs. While not excluding the subsequent emergence of hybrid
incompatibilities due to the intermixing of genomic sequences from parents
(rather than grandparents), many observations, such as the number of proteins
that are aberrantly expressed during gametogenesis, can be explained on this
basis. Indeed, promiscuous expression could have first evolved in
gamete-forming cells where incompatible proteins would be manifest as aberrant
protein aggregates that cause apoptosis. This mechanism would later have been
co-opted by thymic epithelial cells which display peptides from aggregates to
remove potentially autoreactive T cells.
"
The Development of Microfluidic Systems within the Harrison Research Team,"  D. Jed Harrison is a full professor at the Department of Chemistry at the
University of Alberta. Here he describes the development of microfluidic
techniques in his lab from the initial demonstration of an integrated
separation system for samples in liquids to the recent development of methods
to fabricate crystalline packed beds with very low defect density.
"
Fluid flows shaping organism morphology,"  A dynamic self-organized morphology is the hallmark of network-shaped
organisms like slime moulds and fungi. Organisms continuously re-organize their
flexible, undifferentiated body plans to forage for food. Among these organisms
the slime mould Physarum polycephalum has emerged as a model to investigate how
organism can self-organize their extensive networks and act as a coordinated
whole. Cytoplasmic fluid flows flowing through the tubular networks have been
identified as key driver of morphological dynamics. Inquiring how fluid flows
can shape living matter from small to large scales opens up many new avenues
for research.
"
Transferrable End-to-End Learning for Protein Interface Prediction,"  While there has been an explosion in the number of experimentally determined,
atomically detailed structures of proteins, how to represent these structures
in a machine learning context remains an open research question. In this work
we demonstrate that representations learned from raw atomic coordinates can
outperform hand-engineered structural features while displaying a much higher
degree of transferrability. To do so, we focus on a central problem in biology:
predicting how proteins interact with one another--that is, which surfaces of
one protein bind to which surfaces of another protein. We present Siamese
Atomic Surfacelet Network (SASNet), the first end-to-end learning method for
protein interface prediction. Despite using only spatial coordinates and
identities of atoms as inputs, SASNet outperforms state-of-the-art methods that
rely on hand-engineered, high-level features. These results are particularly
striking because we train the method entirely on a significantly biased data
set that does not account for the fact that proteins deform when binding to one
another. Demonstrating the first successful application of transfer learning to
atomic-level data, our network maintains high performance, without retraining,
when tested on real cases in which proteins do deform.
"
How Criticality of Gene Regulatory Networks Affects the Resulting Morphogenesis under Genetic Perturbations,"  Whereas the relationship between criticality of gene regulatory networks
(GRNs) and dynamics of GRNs at a single cell level has been vigorously studied,
the relationship between the criticality of GRNs and system properties at a
higher level has remained unexplored. Here we aim at revealing a potential role
of criticality of GRNs at a multicellular level which are hard to uncover
through the single-cell-level studies, especially from an evolutionary
viewpoint. Our model simulated the growth of a cell population from a single
seed cell. All the cells were assumed to have identical GRNs. We induced
genetic perturbations to the GRN of the seed cell by adding, deleting, or
switching a regulatory link between a pair of genes. From numerical
simulations, we found that the criticality of GRNs facilitated the formation of
nontrivial morphologies when the GRNs were critical in the presence of the
evolutionary perturbations. Moreover, the criticality of GRNs produced
topologically homogenous cell clusters by adjusting the spatial arrangements of
cells, which led to the formation of nontrivial morphogenetic patterns. Our
findings corresponded to an epigenetic viewpoint that heterogeneous and complex
features emerge from homogeneous and less complex components through the
interactions among them. Thus, our results imply that highly structured tissues
or organs in morphogenesis of multicellular organisms might stem from the
criticality of GRNs.
"
Task-Driven Convolutional Recurrent Models of the Visual System,"  Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain's visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, novel cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs matched the dynamics of neural activity in the primate
visual system better than feedforward networks, suggesting a role for the
brain's recurrent connections in performing difficult visual behaviors.
"
Self-organization principles of intracellular pattern formation,"  Dynamic patterning of specific proteins is essential for the spatiotemporal
regulation of many important intracellular processes in procaryotes,
eucaryotes, and multicellular organisms. The emergence of patterns generated by
interactions of diffusing proteins is a paradigmatic example for
self-organization. In this article we review quantitative models for
intracellular Min protein patterns in E. coli, Cdc42 polarization in S.
cerevisiae, and the bipolar PAR protein patterns found in C. elegans. By
analyzing the molecular processes driving these systems we derive a theoretical
perspective on general principles underlying self-organized pattern formation.
We argue that intracellular pattern formation is not captured by concepts such
as ""activators""', ""inhibitors"", or ""substrate-depletion"". Instead,
intracellular pattern formation is based on the redistribution of proteins by
cytosolic diffusion, and the cycling of proteins between distinct
conformational states. Therefore, mass-conserving reaction-diffusion equations
provide the most appropriate framework to study intracellular pattern
formation. We conclude that directed transport, e.g. cytosolic diffusion along
an actively maintained cytosolic gradient, is the key process underlying
pattern formation. Thus the basic principle of self-organization is the
establishment and maintenance of directed transport by intracellular protein
dynamics.
"
Theory of mechano-chemical patterning in biphasic biological tissues,"  The formation of self-organized patterns is key to the morphogenesis of
multicellular organisms, although a comprehensive theory of biological pattern
formation is still lacking. Here, we propose a minimal model combining tissue
mechanics to morphogen turnover and transport in order to explore new routes to
patterning. Our active description couples morphogen reaction-diffusion, which
impact on cell differentiation and tissue mechanics, to a two-phase poroelastic
rheology, where one tissue phase consists of a poroelastic cell network and the
other of a permeating extracellular fluid, which provides a feedback by
actively transporting morphogens. While this model encompasses previous
theories approximating tissues to inert monophasic media, such as Turing's
reaction-diffusion model, it overcomes some of their key limitations permitting
pattern formation via any two-species biochemical kinetics thanks to
mechanically induced cross-diffusion flows. Moreover, we describe a
qualitatively different advection-driven Keller-Segel instability which allows
for the formation of patterns with a single morphogen, and whose fundamental
mode pattern robustly scales with tissue size. We discuss the potential
relevance of these findings for tissue morphogenesis.
"
"α7 nicotinic acetylcholine receptor signaling modulates ovine fetal brain astrocytes transcriptome in response to endotoxin: comparison to microglia, implications for prenatal stress and development of autism spectrum disorder","  Neuroinflammation in utero may result in lifelong neurological disabilities.
Astrocytes play a pivotal role, but the mechanisms are poorly understood. No
early postnatal treatment strategies exist to enhance neuroprotective potential
of astrocytes. We hypothesized that agonism on {\alpha}7 nicotinic
acetylcholine receptor ({\alpha}7nAChR) in fetal astrocytes will augment their
neuroprotective transcriptome profile, while the antagonistic stimulation of
{\alpha}7nAChR will achieve the opposite. Using an in vivo - in vitro model of
developmental programming of neuroinflammation induced by lipopolysaccharide
(LPS), we validated this hypothesis in primary fetal sheep astrocytes cultures
re-exposed to LPS in presence of a selective {\alpha}7nAChR agonist or
antagonist. Our RNAseq findings show that a pro-inflammatory astrocyte
transcriptome phenotype acquired in vitro by LPS stimulation is reversed with
{\alpha}7nAChR agonistic stimulation. Conversely, antagonistic {\alpha}7nAChR
stimulation potentiates the pro-inflammatory astrocytic transcriptome
phenotype. Furthermore, we conduct a secondary transcriptome analysis against
the identical {\alpha}7nAChR experiments in fetal sheep primary microglia
cultures and against the Simons Simplex Collection for autism spectrum disorder
and discuss the implications.
"
Computationally Inferred Genealogical Networks Uncover Long-Term Trends in Assortative Mating,"  Genealogical networks, also known as family trees or population pedigrees,
are commonly studied by genealogists wanting to know about their ancestry, but
they also provide a valuable resource for disciplines such as digital
demography, genetics, and computational social science. These networks are
typically constructed by hand through a very time-consuming process, which
requires comparing large numbers of historical records manually. We develop
computational methods for automatically inferring large-scale genealogical
networks. A comparison with human-constructed networks attests to the accuracy
of the proposed methods. To demonstrate the applicability of the inferred
large-scale genealogical networks, we present a longitudinal analysis on the
mating patterns observed in a network. This analysis shows a consistent
tendency of people choosing a spouse with a similar socioeconomic status, a
phenomenon known as assortative mating. Interestingly, we do not observe this
tendency to consistently decrease (nor increase) over our study period of 150
years.
"
"Numerical simulation of BOD5 dynamics in Igapó I lake, Londrina, Paraná, Brazil: Experimental measurement and mathematical modeling","  The concentration of biochemical oxygen demand, BOD5, was studied in order to
evaluate the water quality of the Igapó I Lake, in Londrina, Paraná State,
Brazil. The simulation was conducted by means of the discretization in
curvilinear coordinates of the geometry of Igapó I Lake, together with finite
difference and finite element methods. The evaluation of the proposed numerical
model for water quality was performed by comparing the experimental values of
BOD5 with the numerical results. The evaluation of the model showed
quantitative results compatible with the actual behavior of Igapó I Lake in
relation to the simulated parameter. The qualitative analysis of the numerical
simulations provided a better understanding of the dynamics of the BOD5
concentration at Igapó I Lake, showing that such concentrations in the
central regions of the lake have values above those allowed by Brazilian law.
The results can help to guide choices by public officials, as: (i) improve the
identification mechanisms of pollutant emitters on Lake Igapó I, (ii)
contribute to the optimal treatment of the recovery of the polluted environment
and (iii) provide a better quality of life for the regulars of the lake as well
as for the residents living on the lakeside.
"
Involvement of Surfactant Protein D in Ebola Virus Infection Enhancement via Glycoprotein Interaction,"  Since the largest 2014-2016 Ebola virus disease outbreak in West Africa,
understanding of Ebola virus infection has improved, notably the involvement of
innate immune mediators. Amongst them, collectins are important players in the
antiviral innate immune defense. A screening of Ebola glycoprotein
(GP)-collectins interactions revealed the specific interaction of human
surfactant protein D (hSP-D), a lectin expressed in lung and liver, two
compartments where Ebola was found in vivo. Further analyses have demonstrated
an involvement of hSP-D in the enhancement of virus infection in several in
vitro models. Similar effects were observed for porcine SP-D (pSP-D). In
addition, both hSP-D and pSP-D interacted with Reston virus (RESTV) GP and
enhanced pseudoviral infection in pulmonary cells. Thus, our study reveals a
novel partner of Ebola GP that may participate to enhance viral spread.
"
Asymptotic genealogies of interacting particle systems with an application to sequential Monte Carlo,"  We study weighted particle systems in which new generations are resampled
from current particles with probabilities proportional to their weights. This
covers a broad class of sequential Monte Carlo (SMC) methods, widely-used in
applied statistics and cognate disciplines. We consider the genealogical tree
embedded into such particle systems, and identify conditions, as well as an
appropriate time-scaling, under which they converge to the Kingman n-coalescent
in the infinite system size limit in the sense of finite-dimensional
distributions. Thus, the tractable n-coalescent can be used to predict the
shape and size of SMC genealogies, as we illustrate by characterising the
limiting mean and variance of the tree height. SMC genealogies are known to be
connected to algorithm performance, so that our results are likely to have
applications in the design of new methods as well. Our conditions for
convergence are strong, but we show by simulation that they do not appear to be
necessary.
"
Robust And Scalable Learning Of Complex Dataset Topologies Via Elpigraph,"  Large datasets represented by multidimensional data point clouds often
possess non-trivial distributions with branching trajectories and excluded
regions, with the recent single-cell transcriptomic studies of developing
embryo being notable examples. Reducing the complexity and producing compact
and interpretable representations of such data remains a challenging task. Most
of the existing computational methods are based on exploring the local data
point neighbourhood relations, a step that can perform poorly in the case of
multidimensional and noisy data. Here we present ElPiGraph, a scalable and
robust method for approximation of datasets with complex structures which does
not require computing the complete data distance matrix or the data point
neighbourhood graph. This method is able to withstand high levels of noise and
is capable of approximating complex topologies via principal graph ensembles
that can be combined into a consensus principal graph. ElPiGraph deals
efficiently with large and complex datasets in various fields from biology,
where it can be used to infer gene dynamics from single-cell RNA-Seq, to
astronomy, where it can be used to explore complex structures in the
distribution of galaxies.
"
Maximum entropy and population heterogeneity in continuous cell cultures,"  Continuous cultures of mammalian cells are complex systems displaying
hallmark phenomena of nonlinear dynamics, such as multi-stability, hysteresis,
as well as sharp transitions between different metabolic states. In this
context mathematical models may suggest control strategies to steer the system
towards desired states. Although even clonal populations are known to exhibit
cell-to-cell variability, most of the currently studied models assume that the
population is homogeneous. To overcome this limitation, we use the maximum
entropy principle to model the phenotypic distribution of cells in a chemostat
as a function of the dilution rate. We consider the coupling between cell
metabolism and extracellular variables describing the state of the bioreactor
and take into account the impact of toxic byproduct accumulation on cell
viability. We present a formal solution for the stationary state of the
chemostat and show how to apply it in two examples. First, a simplified model
of cell metabolism where the exact solution is tractable, and then a
genome-scale metabolic network of the Chinese hamster ovary (CHO) cell line.
Along the way we discuss several consequences of heterogeneity, such as:
qualitative changes in the dynamical landscape of the system, increasing
concentrations of byproducts that vanish in the homogeneous case, and larger
population sizes.
"
Biochemical Coupling Through Emergent Conservation Laws,"  Bazhin has analyzed ATP coupling in terms of quasiequilibrium states where
fast reactions have reached an approximate steady state while slow reactions
have not yet reached equilibrium. After an expository introduction to the
relevant aspects of reaction network theory, we review his work and explain the
role of emergent conserved quantities in coupling. These are quantities, left
unchanged by fast reactions, whose conservation forces exergonic processes such
as ATP hydrolysis to drive desired endergonic processes.
"
Sampling-based probabilistic inference emerges from learning in neural circuits with a cost on reliability,"  Neural responses in the cortex change over time both systematically, due to
ongoing plasticity and learning, and seemingly randomly, due to various sources
of noise and variability. Most previous work considered each of these
processes, learning and variability, in isolation -- here we study neural
networks exhibiting both and show that their interaction leads to the emergence
of powerful computational properties. We trained neural networks on classical
unsupervised learning tasks, in which the objective was to represent their
inputs in an efficient, easily decodable form, with an additional cost for
neural reliability which we derived from basic biophysical considerations. This
cost on reliability introduced a tradeoff between energetically cheap but
inaccurate representations and energetically costly but accurate ones. Despite
the learning tasks being non-probabilistic, the networks solved this tradeoff
by developing a probabilistic representation: neural variability represented
samples from statistically appropriate posterior distributions that would
result from performing probabilistic inference over their inputs. We provide an
analytical understanding of this result by revealing a connection between the
cost of reliability, and the objective for a state-of-the-art Bayesian
inference strategy: variational autoencoders. We show that the same cost leads
to the emergence of increasingly accurate probabilistic representations as
networks become more complex, from single-layer feed-forward, through
multi-layer feed-forward, to recurrent architectures. Our results provide
insights into why neural responses in sensory areas show signatures of
sampling-based probabilistic representations, and may inform future deep
learning algorithms and their implementation in stochastic low-precision
computing systems.
"
Topological Brain Network Distances,"  Existing brain network distances are often based on matrix norms. The
element-wise differences in the existing matrix norms may fail to capture
underlying topological differences. Further, matrix norms are sensitive to
outliers. A major disadvantage to element-wise distance calculations is that it
could be severely affected even by a small number of extreme edge weights. Thus
it is necessary to develop network distances that recognize topology. In this
paper, we provide a survey of bottleneck, Gromov-Hausdorff (GH) and
Kolmogorov-Smirnov (KS) distances that are adapted for brain networks, and
compare them against matrix-norm based network distances. Bottleneck and
GH-distances are often used in persistent homology. However, they were rarely
utilized to measure similarity between brain networks. KS-distance is recently
introduced to measure the similarity between networks across different
filtration values. The performance analysis was conducted using the random
network simulations with the ground truths. Using a twin imaging study, which
provides biological ground truth, we demonstrate that the KS distance has the
ability to determine heritability.
"
Machine-learning a virus assembly fitness landscape,"  Realistic evolutionary fitness landscapes are notoriously difficult to
construct. A recent cutting-edge model of virus assembly consists of a
dodecahedral capsid with $12$ corresponding packaging signals in three affinity
bands. This whole genome/phenotype space consisting of $3^{12}$ genomes has
been explored via computationally expensive stochastic assembly models, giving
a fitness landscape in terms of the assembly efficiency. Using latest
machine-learning techniques by establishing a neural network, we show that the
intensive computation can be short-circuited in a matter of minutes to
astounding accuracy.
"
Signal propagation in sensing and reciprocating cellular systems with spatial and structural heterogeneity,"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
"
Nutrients and biomass dynamics in photo-sequencing batch reactors treating wastewater with high nutrients loadings,"  The present study investigates different strategies for the treatment of a
mixture of digestate from an anaerobic digester diluted and secondary effluent
from a high rate algal pond. To this aim, the performance of two
photo-sequencing batch reactors (PSBRs) operated at high nutrients loading
rates and different solids retention times (SRTs) were compared with a
semi-continuous photobioreactor (SC). Performances were evaluated in terms of
wastewater treatment, biomass composition and biopolymers accumulation during
30 days of operation. PSBRs were operated at a hydraulic retention time (HRT)
of 2 days and SRTs of 10 and 5 days (PSBR2-10 and PSBR2-5, respectively),
whereas the semi-continuous reactor was operated at a coupled HRT/SRT of 10
days (SC10-10). Results showed that PSBR2-5 achieved the highest removal rates
in terms of TN (6.7 mg L-1 d-1), TP (0.31 mg L-1 d-1), TOC (29.32 mg L-1 d-1)
and TIC (3.91 mg L-1 d-1). These results were in general 3-6 times higher than
the removal rates obtained in the SC10-10 (TN 29.74 mg L-1 d-1, TP 0.96 mg L-1
d-1, TOC 29.32 mg L-1 d-1 and TIC 3.91 mg L-1 d-1). Furthermore, both PSBRs
were able to produce biomass up to 0.09 g L-1 d-1, more than twofold the
biomass produced by the semi-continuous reactor (0.04 g L-1 d-1), and achieved
a biomass settleability of 86-92%. This study also demonstrated that the
microbial composition could be controlled by the nutrients loads, since the
three reactors were dominated by different species depending on the nutritional
conditions. Concerning biopolymers accumulation, carbohydrates concentration
achieved similar values in the three reactors (11%), whereas <0.5 % of
polyhydrohybutyrates (PHB) was produced. These low values in biopolymers
production could be related to the lack of microorganisms as cyanobacteria that
are able to accumulate carbohydrates/PHB.
"
The phylogenetic effective sample size and jumps,"  The phylogenetic effective sample size is a parameter that has as its goal
the quantification of the amount of independent signal in a phylogenetically
correlated sample. It was studied for Brownian motion and Ornstein-Uhlenbeck
models of trait evolution. Here, we study this composite parameter when the
trait is allowed to jump at speciation points of the phylogeny. Our numerical
study indicates that there is a non-trivial limit as the effect of jumps grows.
The limit depends on the value of the drift parameter of the Ornstein-Uhlenbeck
process.
"
Neuromodulation of Neuromorphic Circuits,"  We present a novel methodology to enable control of a neuromorphic circuit in
close analogy with the physiological neuromodulation of a single neuron. The
methodology is general in that it only relies on a parallel interconnection of
elementary voltage-controlled current sources. In contrast to controlling a
nonlinear circuit through the parameter tuning of a state-space model, our
approach is purely input-output. The circuit elements are controlled and
interconnected to shape the current-voltage characteristics (I-V curves) of the
circuit in prescribed timescales. In turn, shaping those I-V curves determines
the excitability properties of the circuit. We show that this methodology
enables both robust and accurate control of the circuit behavior and resembles
the biophysical mechanisms of neuromodulation. As a proof of concept, we
simulate a SPICE model composed of MOSFET transconductance amplifiers operating
in the weak inversion regime.
"
Feature Decomposition Based Saliency Detection in Electron Cryo-Tomograms,"  Electron Cryo-Tomography (ECT) allows 3D visualization of subcellular
structures at the submolecular resolution in close to the native state.
However, due to the high degree of structural complexity and imaging limits,
the automatic segmentation of cellular components from ECT images is very
difficult. To complement and speed up existing segmentation methods, it is
desirable to develop a generic cell component segmentation method that is 1)
not specific to particular types of cellular components, 2) able to segment
unknown cellular components, 3) fully unsupervised and does not rely on the
availability of training data. As an important step towards this goal, in this
paper, we propose a saliency detection method that computes the likelihood that
a subregion in a tomogram stands out from the background. Our method consists
of four steps: supervoxel over-segmentation, feature extraction, feature matrix
decomposition, and computation of saliency. The method produces a distribution
map that represents the regions' saliency in tomograms. Our experiments show
that our method can successfully label most salient regions detected by a human
observer, and able to filter out regions not containing cellular components.
Therefore, our method can remove the majority of the background region, and
significantly speed up the subsequent processing of segmentation and
recognition of cellular components captured by ECT.
"
Low-temperature marginal ferromagnetism explains anomalous scale-free correlations in natural flocks,"  We introduce a new ferromagnetic model capable of reproducing one of the most
intriguing properties of collective behaviour in starling flocks, namely the
fact that strong collective order of the system coexists with scale-free
correlations of the modulus of the microscopic degrees of freedom, that is the
birds' speeds. The key idea of the new theory is that the single-particle
potential needed to bound the modulus of the microscopic degrees of freedom
around a finite value, is marginal, that is has zero curvature. We study the
model by using mean-field approximation and Monte Carlo simulations in three
dimensions, complemented by finite-size scaling analysis. While at the standard
critical temperature, $T_c$, the properties of the marginal model are exactly
the same as a normal ferromagnet with continuous symmetry-breaking, our results
show that a novel zero-temperature critical point emerges, so that in its
deeply ordered phase the marginal model develops divergent susceptibility and
correlation length of the modulus of the microscopic degrees of freedom, in
complete analogy with experimental data on natural flocks of starlings.
"
INtERAcT: Interaction Network Inference from Vector Representations of Words,"  In recent years, the number of biomedical publications has steadfastly grown,
resulting in a rich source of untapped new knowledge. Most biomedical facts are
however not readily available, but buried in the form of unstructured text, and
hence their exploitation requires the time-consuming manual curation of
published articles. Here we present INtERAcT, a novel approach to extract
protein-protein interactions from a corpus of biomedical articles related to a
broad range of scientific domains in a completely unsupervised way. INtERAcT
exploits vector representation of words, computed on a corpus of domain
specific knowledge, and implements a new metric that estimates an interaction
score between two molecules in the space where the corresponding words are
embedded. We demonstrate the power of INtERAcT by reconstructing the molecular
pathways associated to 10 different cancer types using a corpus of
disease-specific articles for each cancer type. We evaluate INtERAcT using
STRING database as a benchmark, and show that our metric outperforms currently
adopted approaches for similarity computation at the task of identifying known
molecular interactions in all studied cancer types. Furthermore, our approach
does not require text annotation, manual curation or the definition of semantic
rules based on expert knowledge, and hence it can be easily and efficiently
applied to different scientific domains. Our findings suggest that INtERAcT may
increase our capability to summarize the understanding of a specific disease
using the published literature in an automated and completely unsupervised
fashion.
"
Automated design of collective variables using supervised machine learning,"  Selection of appropriate collective variables for enhancing sampling of
molecular simulations remains an unsolved problem in computational biophysics.
In particular, picking initial collective variables (CVs) is particularly
challenging in higher dimensions. Which atomic coordinates or transforms there
of from a list of thousands should one pick for enhanced sampling runs? How
does a modeler even begin to pick starting coordinates for investigation? This
remains true even in the case of simple two state systems and only increases in
difficulty for multi-state systems. In this work, we solve the initial CV
problem using a data-driven approach inspired by the filed of supervised
machine learning. In particular, we show how the decision functions in
supervised machine learning (SML) algorithms can be used as initial CVs
(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and
Chignolin mini-protein as our test cases, we illustrate how the distance to the
Support Vector Machines' decision hyperplane, the output probability estimates
from Logistic Regression, the outputs from deep neural network classifiers, and
other classifiers may be used to reversibly sample slow structural transitions.
We discuss the utility of other SML algorithms that might be useful for
identifying CVs for accelerating molecular simulations.
"
In silico evolution of signaling networks using rule-based models: bistable response dynamics,"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
"
Solving a non-linear model of HIV infection for CD4+T cells by combining Laplace transformation and Homotopy analysis,"  The aim of this paper is to find the approximate solution of HIV infection
model of CD4+T cells. For this reason, the homotopy analysis transform method
(HATM) is applied. The presented method is combination of traditional homotopy
analysis method (HAM) and the Laplace transformation. The convergence of
presented method is discussed by preparing a theorem which shows the
capabilities of method. The numerical results are shown for different values of
iterations. Also, the regions of convergence are demonstrated by plotting
several h-curves. Furthermore in order to show the efficiency and accuracy of
method, the residual error for different iterations are presented.
"
Simulating Brain Signals: Creating Synthetic EEG Data via Neural-Based Generative Models for Improved SSVEP Classification,"  Despite significant recent progress in the area of Brain-Computer Interface,
there are numerous shortcomings associated with collecting
Electroencephalography (EEG) signals in real-world environments. These include,
but are not limited to, subject and session data variance, long and arduous
calibration processes and performance generalisation issues across
differentsubjects or sessions. This implies that many downstream applications,
including Steady State Visual Evoked Potential (SSVEP) based classification
systems, can suffer from a shortage of reliable data. Generating meaningful and
realistic synthetic data can therefore be of significant value in circumventing
this problem. We explore the use of modern neural-based generative models
trained on a limited quantity of EEG data collected from different subjects to
generate supplementary synthetic EEG signal vectors subsequently utilised to
train an SSVEP classifier. Extensive experimental analyses demonstrate the
efficacy of our generated data, leading to significant improvements across a
variety of evaluations, with the crucial task of cross-subject generalisation
improving by over 35% with the use of synthetic data.
"
A quick guide for student-driven community genome annotation,"  High quality gene models are necessary to expand the molecular and genetic
tools available for a target organism, but these are available for only a
handful of model organisms that have undergone extensive curation and
experimental validation over the course of many years. The majority of gene
models present in biological databases today have been identified in draft
genome assemblies using automated annotation pipelines that are frequently
based on orthologs from distantly related model organisms. Manual curation is
time consuming and often requires substantial expertise, but is instrumental in
improving gene model structure and identification. Manual annotation may seem
to be a daunting and cost-prohibitive task for small research communities but
involving undergraduates in community genome annotation consortiums can be
mutually beneficial for both education and improved genomic resources. We
outline a workflow for efficient manual annotation driven by a team of
primarily undergraduate annotators. This model can be scaled to large teams and
includes quality control processes through incremental evaluation. Moreover, it
gives students an opportunity to increase their understanding of genome biology
and to participate in scientific research in collaboration with peers and
senior researchers at multiple institutions.
"
Stochasticity from function - why the Bayesian brain may need no noise,"  An increasing body of evidence suggests that the trial-to-trial variability
of spiking activity in the brain is not mere noise, but rather the reflection
of a sampling-based encoding scheme for probabilistic computing. Since the
precise statistical properties of neural activity are important in this
context, many models assume an ad-hoc source of well-behaved, explicit noise,
either on the input or on the output side of single neuron dynamics, most often
assuming an independent Poisson process in either case. However, these
assumptions are somewhat problematic: neighboring neurons tend to share
receptive fields, rendering both their input and their output correlated; at
the same time, neurons are known to behave largely deterministically, as a
function of their membrane potential and conductance. We suggest that spiking
neural networks may, in fact, have no need for noise to perform sampling-based
Bayesian inference. We study analytically the effect of auto- and
cross-correlations in functionally Bayesian spiking networks and demonstrate
how their effect translates to synaptic interaction strengths, rendering them
controllable through synaptic plasticity. This allows even small ensembles of
interconnected deterministic spiking networks to simultaneously and
co-dependently shape their output activity through learning, enabling them to
perform complex Bayesian computation without any need for noise, which we
demonstrate in silico, both in classical simulation and in neuromorphic
emulation. These results close a gap between the abstract models and the
biology of functionally Bayesian spiking networks, effectively reducing the
architectural constraints imposed on physical neural substrates required to
perform probabilistic computing, be they biological or artificial.
"
A quantized physical framework for understanding the working mechanism of ion channels,"  A quantized physical framework, called the five-anchor model, is developed
for a general understanding of the working mechanism of ion channels. According
to the hypotheses of this model, the following two basic physical principles
are assigned to each anchor: the polarity change induced by an electron
transition and the mutual repulsion and attraction induced by an electrostatic
force. Consequently, many unique phenomena, such as fast and slow inactivation,
the stochastic gating pattern and constant conductance of a single ion channel,
the difference between electrical and optical stimulation (optogenetics), nerve
conduction block and the generation of an action potential, become intrinsic
features of this physical model. Moreover, this model also provides a
foundation for the probability equation used to calculate the results of
electrical stimulation in our previous C-P theory.
"
Eight-cluster structure of chloroplast genomes differs from similar one observed for bacteria,"  Previously, a seven-cluster pattern claiming to be a universal one in
bacterial genomes has been reported. Keeping in mind the most popular theory of
chloroplast origin, we checked whether a similar pattern is observed in
chloroplast genomes. Surprisingly, eight cluster structure has been found, for
chloroplasts. The pattern observed for chloroplasts differs rather
significantly, from bacterial one, and from that latter observed for
cyanobacteria. The structure is provided by clustering of the fragments of
equal length isolated within a genome so that each fragment is converted in
triplet frequency dictionary with non-overlapping triplets with no gaps in
frame tiling. The points in 63-dimensional space were clustered due to elastic
map technique. The eight cluster found in chloroplasts comprises the fragments
of a genome bearing tRNA genes and exhibiting excessively high
$\mathsf{GC}$-content, in comparison to the entire genome.
"
PEBP1/RKIP: from multiple functions to a common role in cellular processes,"  PEBPs (PhosphatidylEthanolamine Binding Proteins) form a protein family
widely present in the living world since they are encountered in
microorganisms, plants and animals. In all organisms PEBPs appear to regulate
important mechanisms that govern cell cycle, proliferation, differentiation and
motility. In humans, three PEBPs have been identified, namely PEBP1, PEBP2 and
PEBP4. PEBP1 and PEBP4 are the most studied as they are implicated in the
development of various cancers. PEBP2 is specific of testes in mammals and was
essentially studied in rats and mice where it is very abundant. A lot of
information has been gained on PEBP1 also named RKIP (Raf Kinase Inhibitory
protein) due to its role as a metastasis suppressor in cancer. PEBP1 was also
demonstrated to be implicated in Alzheimers disease, diabetes and
nephropathies. Furthermore, PEBP1 was described to be involved in many cellular
processes, among them are signal transduction, inflammation, cell cycle,
proliferation, adhesion, differentiation, apoptosis, autophagy, circadian
rhythm and mitotic spindle checkpoint. On the molecular level, PEBP1 was shown
to regulate several signaling pathways such as Raf/MEK/ERK, NFkB,
PI3K/Akt/mTOR, p38, Notch and Wnt. PEBP1 acts by inhibiting most of the kinases
of these signaling cascades. Moreover, PEBP1 is able to bind to a variety of
small ligands such as ATP, phospholipids, nucleotides, flavonoids or drugs.
Considering PEBP1 is a small cytoplasmic protein (21kDa), its involvement in so
many diseases and cellular mechanisms is amazing. The aim of this review is to
highlight the molecular systems that are common to all these cellular
mechanisms in order to decipher the specific role of PEBP1. Recent discoveries
enable us to propose that PEBP1 is a modulator of molecular interactions that
control signal transduction during membrane and cytoskeleton reorganization.
"
On the inherent competition between valid and spurious inductive inferences in Boolean data,"  Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.
"
Effect of antipsychotics on community structure in functional brain networks,"  Schizophrenia, a mental disorder that is characterized by abnormal social
behavior and failure to distinguish one's own thoughts and ideas from reality,
has been associated with structural abnormalities in the architecture of
functional brain networks. Using various methods from network analysis, we
examine the effect of two classical therapeutic antipsychotics --- Aripiprazole
and Sulpiride --- on the structure of functional brain networks of healthy
controls and patients who have been diagnosed with schizophrenia. We compare
the community structures of functional brain networks of different individuals
using mesoscopic response functions, which measure how community structure
changes across different scales of a network. We are able to do a reasonably
good job of distinguishing patients from controls, and we are most successful
at this task on people who have been treated with Aripiprazole. We demonstrate
that this increased separation between patients and controls is related only to
a change in the control group, as the functional brain networks of the patient
group appear to be predominantly unaffected by this drug. This suggests that
Aripiprazole has a significant and measurable effect on community structure in
healthy individuals but not in individuals who are diagnosed with
schizophrenia. In contrast, we find for individuals are given the drug
Sulpiride that it is more difficult to separate the networks of patients from
those of controls. Overall, we observe differences in the effects of the drugs
(and a placebo) on community structure in patients and controls and also that
this effect differs across groups. We thereby demonstrate that different types
of antipsychotic drugs selectively affect mesoscale structures of brain
networks, providing support that mesoscale structures such as communities are
meaningful functional units in the brain.
"
Priority effects between annual and perennial plants,"  Dominance by annual plants has traditionally been considered a brief early
stage of ecological succession preceding inevitable dominance by competitive
perennials. A more recent, alternative view suggests that interactions between
annuals and perennials can result in priority effects, causing annual dominance
to persist if they are initially more common. Such priority effects would
complicate restoration of native perennial grasslands that have been invaded by
exotic annuals. However, the conditions under which these priority effects
occur remain unknown. Using a simple simulation model, we show that long-term
(500 years) priority effects are possible as long as the plants have low
fecundity and show an establishment-longevity tradeoff, with annuals having
competitive advantage over perennial seedlings. We also show that short-term
(up to 50 years) priority effects arise solely due to low fitness difference in
cases where perennials dominate in the long term. These results provide a
theoretical basis for predicting when restoration of annual-invaded grasslands
requires active removal of annuals and timely reintroduction of perennials.
"
Joining and decomposing reaction networks,"  In systems and synthetic biology, much research has focused on the behavior
and design of single pathways, while, more recently, experimental efforts have
focused on how cross-talk (coupling two or more pathways) or inhibiting
molecular function (isolating one part of the pathway) affects systems-level
behavior. However, the theory for tackling these larger systems in general has
lagged behind. Here, we analyze how joining networks (e.g., cross-talk) or
decomposing networks (e.g., inhibition or knock-outs) affects three properties
that reaction networks may possess---identifiability (recoverability of
parameter values from data), steady-state invariants (relationships among
species concentrations at steady state, used in model selection), and
multistationarity (capacity for multiple steady states, which correspond to
multiple cell decisions). Specifically, we prove results that clarify, for a
network obtained by joining two smaller networks, how properties of the smaller
networks can be inferred from or can imply similar properties of the original
network. Our proofs use techniques from computational algebraic geometry,
including elimination theory and differential algebra.
"
Transfer Learning for Brain-Computer Interfaces: An Euclidean Space Data Alignment Approach,"  Almost all EEG-based brain-computer interfaces (BCIs) need some labeled
subject-specific data to calibrate a new subject, as neural responses are
different across subjects to even the same stimulus. So, a major challenge in
developing high-performance and user-friendly BCIs is to cope with such
individual differences so that the calibration can be reduced or even
completely eliminated. This paper focuses on the latter. More specifically, we
consider an offline application scenario, in which we have unlabeled EEG trials
from a new subject, and would like to accurately label them by leveraging
auxiliary labeled EEG trials from other subjects in the same task. To
accommodate the individual differences, we propose a novel unsupervised
approach to align the EEG trials from different subjects in the Euclidean space
to make them more consistent. It has three desirable properties: 1) the aligned
trial lie in the Euclidean space, which can be used by any Euclidean space
signal processing and machine learning approach; 2) it can be computed very
efficiently; and, 3) it does not need any labeled trials from the new subject.
Experiments on motor imagery and event-related potentials demonstrated the
effectiveness and efficiency of our approach.
"
Relative energetics of acetyl-histidine protomers with and without Zn2+ and a benchmark of energy methods,"  We studied acetylhistidine (AcH), bare or microsolvated with a zinc cation by
simulations in isolation. First, a global search for minima of the potential
energy surface combining both, empirical and first-principles methods, is
performed individually for either one of five possible protonation states.
Comparing the most stable structures between tautomeric forms of negatively
charged AcH shows a clear preference for conformers with the neutral imidazole
ring protonated at the N-epsilon-2 atom. When adding a zinc cation to the
system, the situation is reversed and N-delta-1-protonated structures are
energetically more favorable. Obtained minima structures then served as basis
for a benchmark study to examine the goodness of commonly applied levels of
theory, i.e. force fields, semi-empirical methods, density-functional
approximations (DFA), and wavefunction-based methods with respect to high-level
coupled-cluster calculations, i.e. the DLPNO-CCSD(T) method. All tested force
fields and semi-empirical methods show a poor performance in reproducing the
energy hierarchies of conformers, in particular of systems involving the zinc
cation. Meta-GGA, hybrid, double hybrid DFAs, and the MP2 method are able to
describe the energetics of the reference method within chemical accuracy, i.e.
with a mean absolute error of less than 1kcal/mol. Best performance is found
for the double hybrid DFA B3LYP+XYG3 with a mean absolute error of 0.7 kcal/mol
and a maximum error of 1.8 kcal/mol. While MP2 performs similarly as
B3LYP+XYG3, computational costs, i.e. timings, are increased by a factor of 4
in comparison due to the large basis sets required for accurate results.
"
Monitoring of Wild Pseudomonas Biofilm Strain Conditions Using Statistical Characterisation of Scanning Electron Microscopy Images,"  The present paper proposes a novel method of quantification of the variation
in biofilm architecture, in correlation with the alteration of growth
conditions that include, variations of substrate and conditioning layer. The
polymeric biomaterial serving as substrates are widely used in implants and
indwelling medical devices, while the plasma proteins serve as the conditioning
layer. The present method uses descriptive statistics of FESEM images of
biofilms obtained during a variety of growth conditions. We aim to explore here
the texture and fractal analysis techniques, to identify the most
discriminatory features which are capable of predicting the difference in
biofilm growth conditions. We initially extract some statistical features of
biofilm images on bare polymer surfaces, followed by those on the same
substrates adsorbed with two different types of plasma proteins, viz. Bovine
serum albumin (BSA) and Fibronectin (FN), for two different adsorption times.
The present analysis has the potential to act as a futuristic technology for
developing a computerized monitoring system in hospitals with automated image
analysis and feature extraction, which may be used to predict the growth
profile of an emerging biofilm on surgical implants or similar medical
applications.
"
Nonlinear Dynamics of Binocular Rivalry: A Comparative Study,"  When our eyes are presented with the same image, the brain processes it to
view it as a single coherent one. The lateral shift in the position of our
eyes, causes the two images to possess certain differences, which our brain
exploits for the purpose of depth perception and to gauge the size of objects
at different distances, a process commonly known as stereopsis. However, when
presented with two different visual stimuli, the visual awareness alternates.
This phenomenon of binocular rivalry is a result of competition between the
corresponding neuronal populations of the two eyes. The article presents a
comparative study of various dynamical models proposed to capture this process.
It goes on to study the effect of a certain parameter on the rate of perceptual
alternations and proceeds to disprove the initial propositions laid down to
characterise this phenomenon. It concludes with a discussion on the possible
future work that can be conducted to obtain a better picture of the neuronal
functioning behind this rivalry.
"
The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks,"  We consider the task of estimating a high-dimensional directed acyclic graph,
given observations from a linear structural equation model with arbitrary noise
distribution. By exploiting properties of common random graphs, we develop a
new algorithm that requires conditioning only on small sets of variables. The
proposed algorithm, which is essentially a modified version of the
PC-Algorithm, offers significant gains in both computational complexity and
estimation accuracy. In particular, it results in more efficient and accurate
estimation in large networks containing hub nodes, which are common in
biological systems. We prove the consistency of the proposed algorithm, and
show that it also requires a less stringent faithfulness assumption than the
PC-Algorithm. Simulations in low and high-dimensional settings are used to
illustrate these findings. An application to gene expression data suggests that
the proposed algorithm can identify a greater number of clinically relevant
genes than current methods.
"
Modeling sepsis progression using hidden Markov models,"  Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
"
Optimized Bacteria are Environmental Prediction Engines,"  Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
"
Collective Dynamics of Self-propelled Semiflexible Filaments,"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
"
Universal fitness dynamics through an adaptive resource utilization model,"  The fitness of a species determines its abundance and survival in an
ecosystem. At the same time, species take up resources for growth, so their
abundance affects the availability of resources in an ecosystem. We show here
that such species-resource coupling can be used to assign a quantitative metric
for fitness to each species. This fitness metric also allows for the modeling
of drift in species composition, and hence ecosystem evolution through
speciation and adaptation. Our results provide a foundation for an entirely
computational exploration of evolutionary ecosystem dynamics on any length or
time scale. For example, we can evolve ecosystem dynamics even by initiating
dynamics out of a single primordial ancestor and show that there exists a well
defined ecosystem-averaged fitness dynamics that is resilient against resource
shocks.
"
Protein Mutation Stability Ternary Classification using Neural Networks and Rigidity Analysis,"  Discerning how a mutation affects the stability of a protein is central to
the study of a wide range of diseases. Machine learning and statistical
analysis techniques can inform how to allocate limited resources to the
considerable time and cost associated with wet lab mutagenesis experiments. In
this work we explore the effectiveness of using a neural network classifier to
predict the change in the stability of a protein due to a mutation. Assessing
the accuracy of our approach is dependent on the use of experimental data about
the effects of mutations performed in vitro. Because the experimental data is
prone to discrepancies when similar experiments have been performed by multiple
laboratories, the use of the data near the juncture of stabilizing and
destabilizing mutations is questionable. We address this later problem via a
systematic approach in which we explore the use of a three-way classification
scheme with stabilizing, destabilizing, and inconclusive labels. For a
systematic search of potential classification cutoff values our classifier
achieved 68 percent accuracy on ternary classification for cutoff values of
-0.6 and 0.7 with a low rate of classifying stabilizing as destabilizing and
vice versa.
"
Gamma-Band Correlations in Primary Visual Cortex,"  Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
"
"A streamlined, general approach for computing ligand binding free energies and its application to GPCR-bound cholesterol","  The theory of receptor-ligand binding equilibria has long been
well-established in biochemistry, and was primarily constructed to describe
dilute aqueous solutions. Accordingly, few computational approaches have been
developed for making quantitative predictions of binding probabilities in
environments other than dilute isotropic solution. Existing techniques, ranging
from simple automated docking procedures to sophisticated thermodynamics-based
methods, have been developed with soluble proteins in mind. Biologically and
pharmacologically relevant protein-ligand interactions often occur in complex
environments, including lamellar phases like membranes and crowded, non-dilute
solutions. Here we revisit the theoretical bases of ligand binding equilibria,
avoiding overly specific assumptions that are nearly always made when
describing receptor-ligand binding. Building on this formalism, we extend the
asymptotically exact Alchemical Free Energy Perturbation technique to
quantifying occupancies of sites on proteins in a complex bulk, including
phase-separated, anisotropic, or non-dilute solutions, using a
thermodynamically consistent and easily generalized approach that resolves
several ambiguities of current frameworks. To incorporate the complex bulk
without overcomplicating the overall thermodynamic cycle, we simplify the
common approach for ligand restraints by using a single
distance-from-bound-configuration (DBC) ligand restraint during AFEP decoupling
from protein. DBC restraints should be generalizable to binding modes of most
small molecules, even those with strong orientational dependence. We apply this
approach to compute the likelihood that membrane cholesterol binds to known
crystallographic sites on 3 GPCRs at a range of concentrations. Non-ideality of
cholesterol in a binary cholesterol:POPC bilayer is characterized and
consistently incorporated into the interpretation.
"
A Rational Distributed Process-level Account of Independence Judgment,"  It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.
"
Expected Time to Extinction of SIS Epidemic Model Using Quasy Stationary Distribution,"  We study that the breakdown of epidemic depends on some parameters, that is
expressed in epidemic reproduction ratio number. It is noted that when $R_0 $
exceeds 1, the stochastic model have two different results. But, eventually the
extinction will be reached even though the major epidemic occurs. The question
is how long this process will reach extinction. In this paper, we will focus on
the Markovian process of SIS model when major epidemic occurs. Using the
approximation of quasi--stationary distribution, the expected mean time of
extinction only occurs when the process is one step away from being extinct.
Combining the theorm from Ethier and Kurtz, we use CLT to find the
approximation of this quasi distribution and successfully determine the
asymptotic mean time to extinction of SIS model without demography.
"
Chaotic Dynamics Enhance the Sensitivity of Inner Ear Hair Cells,"  Hair cells of the auditory and vestibular systems are capable of detecting
sounds that induce sub-nanometer vibrations of the hair bundle, below the
stochastic noise levels of the surrounding fluid. Hair cells of certain species
are also known to oscillate without external stimulation, indicating the
presence of an underlying active mechanism. We previously demonstrated that
these spontaneous oscillations exhibit chaotic dynamics. By varying the Calcium
concentration and the viscosity of the Endolymph solution, we are able to
modulate the degree of chaos in the hair cell dynamics. We find that the hair
cell is most sensitive to a stimulus of small amplitude when it is poised in
the weakly chaotic regime. Further, we show that the response time to a force
step decreases with increasing levels of chaos. These results agree well with
our numerical simulations of a chaotic Hopf oscillator and suggest that chaos
may be responsible for the extreme sensitivity and temporal resolution of hair
cells.
"
Distance-based Protein Folding Powered by Deep Learning,"  Contact-assisted protein folding has made very good progress, but two
challenges remain. One is accurate contact prediction for proteins lack of many
sequence homologs and the other is that time-consuming folding simulation is
often needed to predict good 3D models from predicted contacts. We show that
protein distance matrix can be predicted well by deep learning and then
directly used to construct 3D models without folding simulation at all. Using
distance geometry to construct 3D models from our predicted distance matrices,
we successfully folded 21 of the 37 CASP12 hard targets with a median family
size of 58 effective sequence homologs within 4 hours on a Linux computer of 20
CPUs. In contrast, contacts predicted by direct coupling analysis (DCA) cannot
fold any of them in the absence of folding simulation and the best CASP12 group
folded 11 of them by integrating predicted contacts into complex,
fragment-based folding simulation. The rigorous experimental validation on 15
CASP13 targets show that among the 3 hardest targets of new fold our
distance-based folding servers successfully folded 2 large ones with <150
sequence homologs while the other servers failed on all three, and that our ab
initio folding server also predicted the best, high-quality 3D model for a
large homology modeling target. Further experimental validation in CAMEO shows
that our ab initio folding server predicted correct fold for a membrane protein
of new fold with 200 residues and 229 sequence homologs while all the other
servers failed. These results imply that deep learning offers an efficient and
accurate solution for ab initio folding on a personal computer.
"
Discovery and usage of joint attention in images,"  Joint visual attention is characterized by two or more individuals looking at
a common target at the same time. The ability to identify joint attention in
scenes, the people involved, and their common target, is fundamental to the
understanding of social interactions, including others' intentions and goals.
In this work we deal with the extraction of joint attention events, and the use
of such events for image descriptions. The work makes two novel contributions.
First, our extraction algorithm is the first which identifies joint visual
attention in single static images. It computes 3D gaze direction, identifies
the gaze target by combining gaze direction with a 3D depth map computed for
the image, and identifies the common gaze target. Second, we use a human study
to demonstrate the sensitivity of humans to joint attention, suggesting that
the detection of such a configuration in an image can be useful for
understanding the image, including the goals of the agents and their joint
activity, and therefore can contribute to image captioning and related tasks.
"
Modelling of Dictyostelium Discoideum Movement in Linear Gradient of Chemoattractant,"  Chemotaxis is a ubiquitous biological phenomenon in which cells detect a
spatial gradient of chemoattractant, and then move towards the source. Here we
present a position-dependent advection-diffusion model that quantitatively
describes the statistical features of the chemotactic motion of the social
amoeba {\it Dictyostelium discoideum} in a linear gradient of cAMP (cyclic
adenosine monophosphate). We fit the model to experimental trajectories that
are recorded in a microfluidic setup with stationary cAMP gradients and extract
the diffusion and drift coefficients in the gradient direction. Our analysis
shows that for the majority of gradients, both coefficients decrease in time
and become negative as the cells crawl up the gradient. The extracted model
parameters also show that besides the expected drift in the direction of
chemoattractant gradient, we observe a nonlinear dependency of the
corresponding variance in time, which can be explained by the model.
Furthermore, the results of the model show that the non-linear term in the mean
squared displacement of the cell trajectories can dominate the linear term on
large time scales.
"
Interplay of spatial dynamics and local adaptation shapes species lifetime distributions and species-area relationships,"  The distributions of species lifetimes and species in space are related,
since species with good local survival chances have more time to colonize new
habitats and species inhabiting large areas have higher chances to survive
local disturbances. Yet, both distributions have been discussed in mostly
separate communities. Here, we study both patterns simultaneously using a
spatially explicit, evolutionary community assembly approach. We present and
investigate a metacommunity model, consisting of a grid of patches, where each
patch contains a local food web. Species survival depends on predation and
competition interactions, which in turn depend on species body masses as the
key traits. The system evolves due to the migration of species to neighboring
patches, the addition of new species as modifications of existing species, and
local extinction events. The structure of each local food web thus emerges in a
self-organized manner as the highly non-trivial outcome of the relative time
scales of these processes. Our model generates a large variety of complex,
multi-trophic networks and therefore serves as a powerful tool to investigate
ecosystems on long temporal and large spatial scales. We find that the observed
lifetime distributions and species-area relations resemble power laws over
appropriately chosen parameter ranges and thus agree qualitatively with
empirical findings. Moreover, we observe strong finite-size effects, and a
dependence of the relationships on the trophic level of the species. By
comparing our results to simple neutral models found in the literature, we
identify the features that are responsible for the values of the exponents.
"
Universal and generalizable restoration strategies for degraded ecological networks,"  Humans are increasingly stressing ecosystems via habitat destruction, climate
change and global population movements leading to the widespread loss of
biodiversity and the disruption of key ecological services. Ecosystems
characterized primarily by mutualistic relationships between species such as
plant-pollinator interactions may be particularly vulnerable to such
perturbations because the loss of biodiversity can cause extinction cascades
that can compromise the entire network. Here, we develop a general restoration
strategy based on network-science for degraded ecosystems. Specifically, we
show that network topology can be used to identify the optimal sequence of
species reintroductions needed to maximize biodiversity gains following partial
and full ecosystem collapse. This restoration strategy generalizes across
topologically-disparate and geographically-distributed ecosystems.
Additionally, we find that although higher connectance and diversity promote
persistence in pristine ecosystems, these attributes reduce the effectiveness
of restoration efforts in degraded networks. Hence, focusing on restoring the
factors that promote persistence in pristine ecosystems may yield suboptimal
recovery strategies for degraded ecosystems. Overall, our results have
important insights for designing effective ecosystem restoration strategies to
preserve biodiversity and ensure the delivery of critical natural services that
fuel economic development, food security and human health around the globe
"
Extensions and Exact Solutions to the Quaternion-Based RMSD Problem,"  We examine the problem of transforming matching collections of data points
into optimal correspondence. The classic RMSD (root-mean-square deviation)
method calculates a 3D rotation that minimizes the RMSD of a set of test data
points relative to a reference set of corresponding points. Similar literature
in aeronautics, photogrammetry, and proteomics employs numerical methods to
find the maximal eigenvalue of a particular $4\!\times\! 4$ quaternion-based
matrix, thus specifying the quaternion eigenvector corresponding to the optimal
3D rotation. Here we generalize this basic problem, sometimes referred to as
the ""Procrustes Problem,"" and present algebraic solutions that exhibit
properties that are inaccessible to traditional numerical methods. We begin
with the 4D data problem, a problem one dimension higher than the conventional
3D problem, but one that is also solvable by quaternion methods, we then study
the 3D and 2D data problems as special cases. In addition, we consider data
that are themselves quaternions isomorphic to orthonormal triads describing 3
coordinate frames (amino acids in proteins possess such frames). Adopting a
reasonable approximation to the exact quaternion-data minimization problem, we
find a novel closed form ""quaternion RMSD"" (QRMSD) solution for the optimal
rotation from a quaternion data set to a reference set. We observe that
composites of the RMSD and QRMSD measures, combined with problem-dependent
parameters including scaling factors to make their incommensurate dimensions
compatible, could be suitable for certain matching tasks.
"
BindsNET: A machine learning-oriented spiking neural networks library in Python,"  The development of spiking neural network simulation software is a critical
component enabling the modeling of neural systems and the development of
biologically inspired algorithms. Existing software frameworks support a wide
range of neural functionality, software abstraction levels, and hardware
devices, yet are typically not suitable for rapid prototyping or application to
problems in the domain of machine learning. In this paper, we describe a new
Python package for the simulation of spiking neural networks, specifically
geared towards machine learning and reinforcement learning. Our software,
called BindsNET, enables rapid building and simulation of spiking networks and
features user-friendly, concise syntax. BindsNET is built on top of the PyTorch
deep neural networks library, enabling fast CPU and GPU computation for large
spiking networks. The BindsNET framework can be adjusted to meet the needs of
other existing computing and hardware environments, e.g., TensorFlow. We also
provide an interface into the OpenAI gym library, allowing for training and
evaluation of spiking networks on reinforcement learning problems. We argue
that this package facilitates the use of spiking networks for large-scale
machine learning experimentation, and show some simple examples of how we
envision BindsNET can be used in practice. BindsNET code is available at
this https URL
"
Seemless Utilization of Heterogeneous XSede Resources to Accelerate Processing of a High Value Functional Neuroimaging Dataset,"  We describe the technical effort used to process a voluminous high value
human neuroimaging dataset on the Open Science Grid with opportunistic use of
idle HPC resources to boost computing capacity more than 5-fold. With minimal
software development effort and no discernable competitive interference with
other HPC users, this effort delivered 15,000,000 core hours over 7 months.
"
Mechanical Instability Leading Epithelial Cell Delamination,"  We theoretically investigate the mechanical stability of three-dimensional
(3D) foam geometry in a cell sheet and apply its understandings to epithelial
integrity and cell delamination. Analytical calculations revealed that the
monolayer integrity of cell sheet is lost to delamination by a spontaneous
symmetry breaking, inherently depending on the 3D foam geometry of cells; i.e.,
the instability spontaneously appears when the cell density in the sheet plane
increases and/or when the number of neighboring cells decreases, as observed in
vivo. The instability is also facilitated by the delaminated cell-specific
force generation upon lateral surfaces, which are driven by cell-intrinsic
genetic programs during cell invasion and apoptosis in physiology. In
principle, this instability emerges from the force balance on the lateral
boundaries among cells. Additionally, taking into account the cell-intrinsic
force generation on apical and basal sides, which are also broadly observed in
morphogenesis, homeostasis, and carcinogenesis, we found apically/basally
directed cell delaminations and pseudostratified structures, which could
universally explain mechanical regulations of epithelial geometries in both
physiology and pathophysiology.
"
Tier structure of strongly endotactic reaction networks,"  Reaction networks are mainly used to model the time-evolution of molecules of
interacting chemical species. Stochastic models are typically used when the
counts of the molecules are low, whereas deterministic models are used when the
counts are in high abundance. In 2011, the notion of `tiers' was introduced to
study the long time behavior of deterministically modeled reaction networks
that are weakly reversible and have a single linkage class. This `tier' based
argument was analytical in nature. Later, in 2014, the notion of a strongly
endotactic network was introduced in order to generalize the previous results
from weakly reversible networks with a single linkage class to this wider
family of networks. The point of view of this later work was more geometric and
algebraic in nature. The notion of strongly endotactic networks was later used
in 2018 to prove a large deviation principle for a class of stochastically
modeled reaction networks.
We provide an analytical characterization of strongly endotactic networks in
terms of tier structures. By doing so, we shed light on the connection between
the two points of view, and also make available a new proof technique for the
study of strongly endotactic networks. We show the power of this new technique
in two distinct ways. First, we demonstrate how the main previous results
related to strongly endotactic networks, both for the deterministic and
stochastic modeling choices, can be quickly obtained from our characterization.
Second, we demonstrate how new results can be obtained by proving that a
sub-class of strongly endotactic networks, when modeled stochastically, is
positive recurrent. Finally, and similarly to recent independent work by Agazzi
and Mattingly, we provide an example which closes a conjecture in the negative
by showing that stochastically modeled strongly endotactic networks can be
transient (and even explosive).
"
Quantum cognition goes beyond-quantum: modeling the collective participant in psychological measurements,"  In psychological measurements, two levels should be distinguished: the
'individual level', relative to the different participants in a given cognitive
situation, and the 'collective level', relative to the overall statistics of
their outcomes, which we propose to associate with a notion of 'collective
participant'. When the distinction between these two levels is properly
formalized, it reveals why the modeling of the collective participant generally
requires beyond-quantum - non-Bornian - probabilistic models, when sequential
measurements at the individual level are considered, and this though a pure
quantum description remains valid for single measurement situations.
"
Deriving mesoscopic models of collective behaviour for finite populations,"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
"
Towards Modeling the Interaction of Spatial-Associative Neural Network Representations for Multisensory Perception,"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
"
Local migration quantification method for scratch assays,"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
"
Bayesian inference in Y-linked two-sex branching processes with mutations: ABC approach,"  A Y-linked two-sex branching process with mutations and blind choice of males
is a suitable model for analyzing the evolution of the number of carriers of an
allele and its mutations of a Y-linked gene. Considering a two-sex monogamous
population, in this model each female chooses her partner from among the male
population without caring about his type (i.e., the allele he carries). In this
work, we deal with the problem of estimating the main parameters of such model
developing the Bayesian inference in a parametric framework. Firstly, we
consider, as sample scheme, the observation of the total number of females and
males up to some generation as well as the number of males of each genotype at
last generation. Later, we introduce the information of the mutated males only
in the last generation obtaining in this way a second sample scheme. For both
samples, we apply the Approximate Bayesian Computation (ABC) methodology to
approximate the posterior distributions of the main parameters of this model.
The accuracy of the procedure based on these samples is illustrated and
discussed by way of simulated examples.
"
The morphodynamics of 3D migrating cancer cells,"  Cell shape is an important biomarker. Previously extensive studies have
established the relation between cell shape and cell function. However, the
morphodynamics, namely the temporal fluctuation of cell shape is much less
understood. We study the morphodynamics of MDA-MB-231 cells in type I collagen
extracellular matrix (ECM). We find ECM mechanics, as tuned by collagen
concentration, controls the morphodynamics but not the static cell morphology.
By employing machine learning techniques, we classify cell shape into five
different morphological phenotypes corresponding to different migration modes.
As a result, cell morphodynamics is mapped into temporal evolution of
morphological phenotypes. We systematically characterize the phenotype dynamics
including occurrence probability, dwell time, transition flux, and also obtain
the invasion characteristics of each phenotype. Using a tumor organoid model,
we show that the distinct invasion potentials of each phenotype modulate the
phenotype homeostasis. Overall invasion of a tumor organoid is facilitated by
individual cells searching for and committing to phenotypes of higher invasive
potential. In conclusion, we show that 3D migrating cancer cells exhibit rich
morphodynamics that is regulated by ECM mechanics and is closely related with
cell motility. Our results pave the way to systematic characterization and
functional understanding of cell morphodynamics.
"
Classification of crystallization outcomes using deep convolutional neural networks,"  The Machine Recognition of Crystallization Outcomes (MARCO) initiative has
assembled roughly half a million annotated images of macromolecular
crystallization experiments from various sources and setups. Here,
state-of-the-art machine learning algorithms are trained and tested on
different parts of this data set. We find that more than 94% of the test images
can be correctly labeled, irrespective of their experimental origin. Because
crystal recognition is key to high-density screening and the systematic
analysis of crystallization experiments, this approach opens the door to both
industrial and fundamental research applications.
"
Designing diagnostic platforms for analysis of disease patterns and probing disease emergence,"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
"
The hypotensive effect of activated apelin receptor is correlated with \b{eta}-arrestin recruitment,"  The apelinergic system is an important player in the regulation of both
vascular tone and cardiovascular function, making this physiological system an
attractive target for drug development for hypertension, heart failure and
ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in
humans whilst reducing peripheral vascular resistance. In this study, we
investigated the signaling pathways through which apelin exerts its hypotensive
action. We synthesized a series of apelin-13 analogs whereby the C-terminal
Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells
expressing APJ, we evaluated the relative efficacy of these compounds to
activate G{\alpha}i1 and G{\alpha}oA G-proteins, recruit \b{eta}-arrestins 1
and 2 (\b{eta}arrs), and inhibit cAMP production. Calculating the transduction
ratio for each pathway allowed us to identify several analogs with distinct
signaling profiles. Furthermore, we found that these analogs delivered i.v. to
Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two
compounds lost their ability to lower blood pressure, while other analogs
significantly reduced blood pressure as apelin-13. Interestingly, analogs that
did not lower blood pressure were less effective at recruiting \b{eta}arrs.
Finally, using Spearman correlations, we established that the hypotensive
response was significantly correlated with \b{eta}arr recruitment but not with
G protein- dependent signaling. In conclusion, our results demonstrated that
the \b{eta}arr recruitment potency is involved in the hypotensive efficacy of
activated APJ.
"
Metachronal motion of artificial magnetic cilia,"  Organisms use hair-like cilia that beat in a metachronal fashion to actively
transport fluid and suspended particles. Metachronal motion emerges due to a
phase difference between beating cycles of neighboring cilia and appears as
traveling waves propagating along ciliary carpet. In this work, we demonstrate
biomimetic artificial cilia capable of metachronal motion. The cilia are
micromachined magnetic thin filaments attached at one end to a substrate and
actuated by a uniform rotating magnetic field. We show that the difference in
magnetic cilium length controls the phase of the beating motion. We use this
property to induce metachronal waves within a ciliary array and explore the
effect of operation parameters on the wave motion. The metachronal motion in
our artificial system is shown to depend on the magnetic and elastic properties
of the filaments, unlike natural cilia, where metachronal motion arises due to
fluid coupling. Our approach enables an easy integration of metachronal
magnetic cilia in lab-on-a-chip devices for enhanced fluid and particle
manipulations.
"
Navigating through the R packages for movement,"  The advent of miniaturized biologging devices has provided ecologists with
unparalleled opportunities to record animal movement across scales, and led to
the collection of ever-increasing quantities of tracking data. In parallel,
sophisticated tools to process, visualize and analyze tracking data have been
developed in abundance. Within the R software alone, we listed 57 focused on
these tasks, called here tracking packages. Here, we reviewed these tracking
packages, as an introduction to this set of packages for researchers, and to
provide feedback and recommendations to package developers, from a user
perspective. We described each package based on a workflow centered around
tracking data (i.e. (x,y,t)), broken down in three stages: pre-processing,
post-processing, and analysis (data visualization, track description, path
reconstruction, behavioral pattern identification, space use characterization,
trajectory simulation and others).
Supporting documentation is key to the accessibility of a package for users.
Based on a user survey, we reviewed the quality of packages' documentation, and
identified $12$ packages with good or excellent documentation. Links between
packages were assessed through a network graph analysis. Although a large group
of packages shows some degree of connectivity (either depending on functions or
suggesting the use of another tracking package), a third of tracking packages
work on isolation, reflecting a fragmentation in the R Movement-Ecology
programming community.
Finally, we provide recommendations for users to choose packages, and for
developers to maximize usefulness of their contribution and strengthen the
links between the programming community.
"
Spatial dynamics of flower organ formation,"  Understanding the emergence of biological structures and their changes is a
complex problem. On a biochemical level, it is based on gene regulatory
networks (GRN) consisting on interactions between the genes responsible for
cell differentiation and coupled in a greater scale with external factors. In
this work we provide a systematic methodological framework to construct
Waddington's epigenetic landscape of the GRN involved in cellular determination
during the early stages of development of angiosperms. As a specific example we
consider the flower of the plant \textit{Arabidopsis thaliana}. Our model,
which is based on experimental data, recovers accurately the spatial
configuration of the flower during cell fate determination, not only for the
wild type, but for its homeotic mutants as well. The method developed in this
project is general enough to be used in the study of the relationship between
genotype-phenotype in other living organisms.
"
On the scaling patterns of infectious disease incidence in cities,"  Urban areas with larger and more connected populations offer an auspicious
environment for contagion processes such as the spread of pathogens. Empirical
evidence reveals a systematic increase in the rates of certain sexually
transmitted diseases (STDs) with larger urban population size. However, the
main drivers of these systemic infection patterns are still not well
understood, and rampant urbanization rates worldwide makes it critical to
advance our understanding on this front. Using confirmed-cases data for three
STDs in US metropolitan areas, we investigate the scaling patterns of
infectious disease incidence in urban areas. The most salient features of these
patterns are that, on average, the incidence of infectious diseases that
transmit with less ease-- either because of a lower inherent transmissibility
or due to a less suitable environment for transmission-- scale more steeply
with population size, are less predictable across time and more variable across
cities of similar size. These features are explained, first, using a simple
mathematical model of contagion, and then through the lens of a new theory of
urban scaling. These theoretical frameworks help us reveal the links between
the factors that determine the transmissibility of infectious diseases and the
properties of their scaling patterns across cities.
"
Mean field repulsive Kuramoto models: Phase locking and spatial signs,"  The phenomenon of self-synchronization in populations of oscillatory units
appears naturally in neurosciences. However, in some situations, the formation
of a coherent state is damaging. In this article we study a repulsive
mean-field Kuramoto model that describes the time evolution of n points on the
unit circle, which are transformed into incoherent phase-locked states. It has
been recently shown that such systems can be reduced to a three-dimensional
system of ordinary differential equations, whose mathematical structure is
strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical
system are then described by a ow of Möbius transformations. We show this
underlying dynamic performs statistical inference by computing dynamically
M-estimates of scatter matrices. We also describe the limiting phase-locked
states for random initial conditions using Tyler's transformation matrix.
Moreover, we show the repulsive Kuramoto model performs dynamically not only
robust covariance matrix estimation, but also data processing: the initial
configuration of the n points is transformed by the dynamic into a limiting
phase-locked state that surprisingly equals the spatial signs from
nonparametric statistics. That makes the sign empirical covariance matrix to
equal 1 2 id2, the variance-covariance matrix of a random vector that is
uniformly distributed on the unit circle.
"
Exploring the Psychological Basis for Transitions in the Archaeological Record,"  In lieu of an abstract here is the first paragraph: No other species remotely
approaches the human capacity for the cultural evolution of novelty that is
accumulative, adaptive, and open-ended (i.e., with no a priori limit on the
size or scope of possibilities). By culture we mean extrasomatic
adaptations--including behavior and technology--that are socially rather than
sexually transmitted. This chapter synthesizes research from anthropology,
psychology, archaeology, and agent-based modeling into a speculative yet
coherent account of two fundamental cognitive transitions underlying human
cultural evolution that is consistent with contemporary psychology. While the
chapter overlaps with a more technical paper on this topic (Gabora & Smith
2018), it incorporates new research and elaborates a genetic component to our
overall argument. The ideas in this chapter grew out of a non-Darwinian
framework for cultural evolution, referred to as the Self-other Reorganization
(SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which
was inspired by research on the origin and earliest stage in the evolution of
life (Cornish-Bowden & Cárdenas 2017; Goldenfeld, Biancalani, & Jafarpour,
2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges
psychological research on fundamental aspects of our human nature such as
creativity and our proclivity to reflect on ideas from different perspectives,
with the literature on evolutionary approaches to cultural evolution that
aspire to synthesize the behavioral sciences much as has been done for the
biological scientists. The current chapter is complementary to this effort, but
less abstract; it attempts to ground the theory of cultural evolution in terms
of cognitive transitions as suggested by archaeological evidence.
"
Disentangling top-down vs. bottom-up and low-level vs. high-level influences on eye movements over time,"  Bottom-up and top-down, as well as low-level and high-level factors influence
where we fixate when viewing natural scenes. However, the importance of each of
these factors and how they interact remains a matter of debate. Here, we
disentangle these factors by analysing their influence over time. For this
purpose we develop a saliency model which is based on the internal
representation of a recent early spatial vision model to measure the low-level
bottom-up factor. To measure the influence of high-level bottom-up features, we
use a recent DNN-based saliency model. To account for top-down influences, we
evaluate the models on two large datasets with different tasks: first, a
memorisation task and, second, a search task. Our results lend support to a
separation of visual scene exploration into three phases: The first saccade, an
initial guided exploration characterised by a gradual broadening of the
fixation density, and an steady state which is reached after roughly 10
fixations. Saccade target selection during the initial exploration and in the
steady state are related to similar areas of interest, which are better
predicted when including high-level features. In the search dataset, fixation
locations are determined predominantly by top-down processes. In contrast, the
first fixation follows a different fixation density and contains a strong
central fixation bias. Nonetheless, first fixations are guided strongly by
image properties and as early as 200 ms after image onset, fixations are better
predicted by high-level information. We conclude that any low-level bottom-up
factors are mainly limited to the generation of the first saccade. All saccades
are better explained when high-level features are considered, and later this
high-level bottom-up control can be overruled by top-down influences.
"
A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception,"  While deep neural networks take loose inspiration from neuroscience, it is an
open question how seriously to take the analogies between artificial deep
networks and biological neuronal systems. Interestingly, recent work has shown
that deep convolutional neural networks (CNNs) trained on large-scale image
recognition tasks can serve as strikingly good models for predicting the
responses of neurons in visual cortex to visual stimuli, suggesting that
analogies between artificial and biological neural networks may be more than
superficial. However, while CNNs capture key properties of the average
responses of cortical neurons, they fail to explain other properties of these
neurons. For one, CNNs typically require large quantities of labeled input data
for training. Our own brains, in contrast, rarely have access to this kind of
supervision, so to the extent that representations are similar between CNNs and
brains, this similarity must arise via different training paths. In addition,
neurons in visual cortex produce complex time-varying responses even to static
inputs, and they dynamically tune themselves to temporal regularities in the
visual environment. We argue that these differences are clues to fundamental
differences between the computations performed in the brain and in deep
networks. To begin to close the gap, here we study the emergent properties of a
previously-described recurrent generative network that is trained to predict
future video frames in a self-supervised manner. Remarkably, the model is able
to capture a wide variety of seemingly disparate phenomena observed in visual
cortex, ranging from single unit response dynamics to complex perceptual motion
illusions. These results suggest potentially deep connections between recurrent
predictive neural network models and the brain, providing new leads that can
enrich both fields.
"
3D spatial exploration by E. coli echoes motor temporal variability,"  Unraveling bacterial strategies for spatial exploration is crucial to
understand the complexity of the organi- zation of life. Currently, a
cornerstone for quantitative modeling of bacterial transport, is their
run-and-tumble strategy to explore their environment. For Escherichia coli, the
run time distribution was reported to follow a Poisson process with a single
characteristic time related to the rotational switching of the flagellar motor.
Direct measurements on flagellar motors show, on the contrary, heavy-tailed
distributions of rotation times stemming from the intrinsic noise in the
chemotactic mechanism. The crucial role of stochasticity on the chemotactic
response has also been highlighted by recent modeling, suggesting its
determinant influence on motility. In stark contrast with the accepted vision
of run-and-tumble, here we report a large behavioral variability of wild-type
E. coli, revealed in their three-dimensional trajectories. At short times, a
broad distribution of run times is measured on a population and attributed to
the slow fluctuations of a signaling protein triggering the flagellar motor
reversal. Over long times, individual bacteria undergo significant changes in
motility. We demonstrate that such a large distribution introduces measurement
biases in most practical situations. These results reconcile the notorious
conundrum between run time observations and motor switching statistics. We
finally propose that statistical modeling of transport properties currently
undertaken in the emerging framework of active matter studies should be
reconsidered under the scope of this large variability of motility features.
"
Detecting Multiple Change Points Using Adaptive Regression Splines with Application to Neural Recordings,"  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
"
Evolution of macromolecular structure: a 'double tale' of biological accretion,"  The evolution of structure in biology is driven by accretion and change.
Accretion brings together disparate parts to form bigger wholes. Change
provides opportunities for growth and innovation. Here we review patterns and
processes that are responsible for a 'double tale' of evolutionary accretion at
various levels of complexity, from proteins and nucleic acids to high-rise
building structures in cities. Parts are at first weakly linked and associate
variously. As they diversify, they compete with each other and are selected for
performance. The emerging interactions constrain their structure and
associations. This causes parts to self-organize into modules with tight
linkage. In a second phase, variants of the modules evolve and become new parts
for a new generative cycle of higher-level organization. Evolutionary genomics
and network biology support the 'double tale' of structural module creation and
validate an evolutionary principle of maximum abundance that drives the gain
and loss of modules.
"
A theoretical framework for retinal computations: insights from textbook knowledge,"  Neural circuits in the retina divide the incoming visual scene into more than
a dozen distinct representations that are sent on to central brain areas, such
as the lateral geniculate nucleus and the superior colliculus. The retina can
be viewed as a parallel image processor made of a multitude of small
computational devices. Neural circuits of the retina are constituted by various
cell types that separate the incoming visual information in different channels.
Visual information is processed by retinal neural circuits and several
computations are performed extracting distinct features from the visual scene.
The aim of this article is to understand the computational basis involved in
processing visual information which finally leads to several feature detectors.
Therefore, the elements that form the basis of retinal computations will be
explored by explaining how oscillators can lead to a final output with
computational meaning. Linear versus nonlinear systems will be presented and
the retina will be placed in the context of a nonlinear system. Finally,
simulations will be presented exploring the concept of the retina as a
nonlinear system which can perform understandable computations converting a
known input into a predictable output.
"
Gene Shaving using influence function of a kernel method,"  Identifying significant subsets of the genes, gene shaving is an essential
and challenging issue for biomedical research for a huge number of genes and
the complex nature of biological networks,. Since positive definite kernel
based methods on genomic information can improve the prediction of diseases, in
this paper we proposed a new method, ""kernel gene shaving (kernel canonical
correlation analysis (kernel CCA) based gene shaving). This problem is
addressed using the influence function of the kernel CCA. To investigate the
performance of the proposed method in a comparison of three popular gene
selection methods (T-test, SAM and LIMMA), we were used extensive simulated and
real microarray gene expression datasets. The performance measures AUC was
computed for each of the methods. The achievement of the proposed method has
improved than the three well-known gene selection methods. In real data
analysis, the proposed method identified a subsets of $210$ genes out of $2000$
genes. The network of these genes has significantly more interactions than
expected, which indicates that they may function in a concerted effort on colon
cancer.
"
Analyzing biological and artificial neural networks: challenges with opportunities for synergy?,"  Deep neural networks (DNNs) transform stimuli across multiple processing
stages to produce representations that can be used to solve complex tasks, such
as object recognition in images. However, a full understanding of how they
achieve this remains elusive. The complexity of biological neural networks
substantially exceeds the complexity of DNNs, making it even more challenging
to understand the representations that they learn. Thus, both machine learning
and computational neuroscience are faced with a shared challenge: how can we
analyze their representations in order to understand how they solve complex
tasks?
We review how data-analysis concepts and techniques developed by
computational neuroscientists can be useful for analyzing representations in
DNNs, and in turn, how recently developed techniques for analysis of DNNs can
be useful for understanding representations in biological neural networks. We
explore opportunities for synergy between the two fields, such as the use of
DNNs as in-silico model systems for neuroscience, and how this synergy can lead
to new hypotheses about the operating principles of biological neural networks.
"
A mechanism of synaptic clock underlying subjective time perception,"  Temporal resolution of visual information processing is thought to be an
important factor in predator-prey interactions, shaped in the course of
evolution by animals' ecology. Here I show that light can be considered to have
a dual role of a source of information, which guides motor actions, and an
environmental feedback for those actions. I consequently show how temporal
perception might depend on behavioral adaptations realized by the nervous
system. I propose an underlying mechanism of synaptic clock, with every synapse
having its characteristic time unit, determined by the persistence of memory
traces of synaptic inputs, which is used by the synapse to tell time. The
present theory offers a testable framework, which may account for numerous
experimental findings, including the interspecies variation in temporal
resolution and the properties of subjective time perception, specifically the
variable speed of perceived time passage, depending on emotional and
attentional states or tasks performed.
"
Discrete structure of the brain rhythms,"  Neuronal activity in the brain generates synchronous oscillations of the
Local Field Potential (LFP). The traditional analyses of the LFPs are based on
decomposing the signal into simpler components, such as sinusoidal harmonics.
However, a common drawback of such methods is that the decomposition primitives
are usually presumed from the onset, which may bias our understanding of the
signal's structure. Here, we introduce an alternative approach that allows an
impartial, high resolution, hands-off decomposition of the brain waves into a
small number of discrete, frequency-modulated oscillatory processes, which we
call oscillons. In particular, we demonstrate that mouse hippocampal LFP
contain a single oscillon that occupies the $\theta$-frequency band and a
couple of $\gamma$-oscillons that correspond, respectively, to slow and fast
$\gamma$-waves. Since the oscillons were identified empirically, they may
represent the actual, physical structure of synchronous oscillations in
neuronal ensembles, whereas Fourier-defined ""brain waves"" are nothing but
poorly resolved oscillons.
"
Fading of collective attention shapes the evolution of linguistic variants,"  Language change involves the competition between alternative linguistic forms
(1). The spontaneous evolution of these forms typically results in monotonic
growths or decays (2, 3) like in winner-take-all attractor behaviors. In the
case of the Spanish past subjunctive, the spontaneous evolution of its two
competing forms (ended in -ra and -se) was perturbed by the appearance of the
Royal Spanish Academy in 1713, which enforced the spelling of both forms as
perfectly interchangeable variants (4), at a moment in which the -ra form was
dominant (5). Time series extracted from a massive corpus of books (6) reveal
that this regulation in fact produced a transient renewed interest for the old
form -se which, once faded, left the -ra again as the dominant form up to the
present day. We show that time series are successfully explained by a
two-dimensional linear model that integrates an imitative and a novelty
component. The model reveals that the temporal scale over which collective
attention fades is in inverse proportion to the verb frequency. The integration
of the two basic mechanisms of imitation and attention to novelty allows to
understand diverse competing objects, with lifetimes that range from hours for
memes and news (7, 8) to decades for verbs, suggesting the existence of a
general mechanism underlying cultural evolution.
"
Bridging trees for posterior inference on Ancestral Recombination Graphs,"  We present a new Markov chain Monte Carlo algorithm, implemented in software
Arbores, for inferring the history of a sample of DNA sequences. Our principal
innovation is a bridging procedure, previously applied only for simple
stochastic processes, in which the local computations within a bridge can
proceed independently of the rest of the DNA sequence, facilitating large-scale
parallelisation.
"
Integrating Flexible Normalization into Mid-Level Representations of Deep Convolutional Neural Networks,"  Deep convolutional neural networks (CNNs) are becoming increasingly popular
models to predict neural responses in visual cortex. However, contextual
effects, which are prevalent in neural processing and in perception, are not
explicitly handled by current CNNs, including those used for neural prediction.
In primary visual cortex, neural responses are modulated by stimuli spatially
surrounding the classical receptive field in rich ways. These effects have been
modeled with divisive normalization approaches, including flexible models,
where spatial normalization is recruited only to the degree responses from
center and surround locations are deemed statistically dependent. We propose a
flexible normalization model applied to mid-level representations of deep CNNs
as a tractable way to study contextual normalization mechanisms in mid-level
cortical areas. This approach captures non-trivial spatial dependencies among
mid-level features in CNNs, such as those present in textures and other visual
stimuli, that arise from tiling high order features, geometrically. We expect
that the proposed approach can make predictions about when spatial
normalization might be recruited in mid-level cortical areas. We also expect
this approach to be useful as part of the CNN toolkit, therefore going beyond
more restrictive fixed forms of normalization.
"
Unveiled electric profiles within hydrogen bonds suggest DNA base pairs with similar bond strengths,"  Electrical forces are the background of all the interactions occurring in
biochemical systems. From here and by using a combination of ab-initio and
ad-hoc models, we introduce the first description of electric field profiles
with intrabond resolution to support a characterization of single bond forces
attending to its electrical origin. This fundamental issue has eluded a
physical description so far. Our method is applied to describe hydrogen bonds
(HB) in DNA base pairs. Numerical results reveal that base pairs in DNA could
be equivalent considering HB strength contributions, which challenges previous
interpretations of thermodynamic properties of DNA based on the assumption that
Adenine/Thymine pairs are weaker than Guanine/Cytosine pairs due to the sole
difference in the number of HB. Thus, our methodology provides solid
foundations to support the development of extended models intended to go deeper
into the molecular mechanisms of DNA functioning.
"
Fixation probabilities for the Moran process in evolutionary games with two strategies: graph shapes and large population asymptotics,"  This paper is based on the complete classification of evolutionary scenarios
for the Moran process with two strategies given by Taylor et al. (B. Math.
Biol. 66(6): 1621--1644, 2004). Their classification is based on whether each
strategy is a Nash equilibrium and whether the fixation probability for a
single individual of each strategy is larger or smaller than its value for
neutral evolution. We improve on this analysis by showing that each
evolutionary scenario is characterized by a definite graph shape for the
fixation probability function. A second class of results deals with the
behavior of the fixation probability when the population size tends to
infinity. We develop asymptotic formulae that approximate the fixation
probability in this limit and conclude that some of the evolutionary scenarios
cannot exist when the population size is large.
"
Combining Alchemical Transformation with Physical Pathway to Accurately Compute Absolute Binding Free Energy,"  We present a new method that combines alchemical transformation with physical
pathway to accurately and efficiently compute the absolute binding free energy
of receptor-ligand complex. Currently, the double decoupling method (DDM) and
the potential of mean force approach (PMF) methods are widely used to compute
the absolute binding free energy of biomolecules. The DDM relies on
alchemically decoupling the ligand from its environments, which can be
computationally challenging for large ligands and charged ligands because of
the large magnitude of the decoupling free energies involved. On the other
hand, the PMF approach uses physical pathway to extract the ligand out of the
binding site, thus avoids the alchemical decoupling of the ligand. However, the
PMF method has its own drawback because of the reliance on a ligand
binding/unbinding pathway free of steric obstruction from the receptor atoms.
Therefore, in the presence of deeply buried ligand functional groups the
convergence of the PMF calculation can be very slow leading to large errors in
the computed binding free energy. Here we develop a new method called AlchemPMF
by combining alchemical transformation with physical pathway to overcome the
major drawback in the PMF method. We have tested the new approach on the
binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20
ns of simulation per umbrella sampling window, the new method yields absolute
binding free energies within ~1 kcal/mol from the experimental result, whereas
the standard PMF approach and the DDM calculations result in errors of ~5
kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy
computed using the new method is associated with smaller statistical error
compared with those obtained from the existing methods.
"
Short DNA persistence length in a mesoscopic helical model,"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
"
The origin and early evolution of life in chemical complexity space,"  Life can be viewed as a localized chemical system that sits on, or in the
basin of attraction of, a metastable dynamical attractor state that remains out
of equilibrium with the environment. Such a view of life allows that new living
states can arise through chance changes in local chemical concentration
(=mutations) that move points in space into the basin of attraction of a life
state - the attractor being an autocatalytic sets whose essential (=keystone)
species are produced at a higher rate than they are lost to the environment by
diffusion, such that growth in expected. This conception of life yields several
new insights and conjectures. (1) This framework suggests that the first new
life states to arise are likely at interfaces where the rate of diffusion of
keystone species is tied to a low-diffusion regime, while precursors and waste
products diffuse at a higher rate. (2) There are reasons to expect that once
the first life state arises, most likely on a mineral surface, additional
mutations will generate derived life states with which the original state will
compete. (3) I propose that in the resulting adaptive process there is a
general tendency for higher complexity life states (i.e., ones that are further
from being at equilibrium with the environment) to dominate a given mineral
surface. (4) The framework suggests a simple and predictable path by which
cells evolve and provides pointers on why such cells are likely to acquire
particulate inheritance. Overall, the dynamical systems theoretical framework
developed provides an integrated view of the origin and early evolution of life
and supports novel empirical approaches.
"
Origin of soft glassy rheology in the cytoskeleton,"  Dynamically crosslinked semiflexible biopolymers such as the actin
cytoskeleton govern the mechanical behavior of living cells. Semiflexible
biopolymers stiffen nonlinearly in response to mechanical loads, whereas the
crosslinker dynamics allow for stress relaxation over time. Here we show,
through rheology and theoretical modeling, that the combined nonlinearity in
time and stress leads to an unexpectedly slow stress relaxation, similar to the
dynamics of disordered systems close to the glass transition. Our work suggests
that transient crosslinking combined with internal stress is the microscopic
origin for the universal glassy dynamics as frequently observed in cellular
mechanics.
"
Impossibility results on stability of phylogenetic consensus methods,"  We answer two questions raised by Bryant, Francis and Steel in their work on
consensus methods in phylogenetics. Consensus methods apply to every practical
instance where it is desired to aggregate a set of given phylogenetic trees
(say, gene evolution trees) into a resulting, ""consensus"" tree (say, a species
tree). Various stability criteria have been explored in this context, seeking
to model desirable consistency properties of consensus methods as the
experimental data is updated (e.g., more taxa, or more trees, are mapped).
However, such stability conditions can be incompatible with some basic
regularity properties that are widely accepted to be essential in any
meaningful consensus method. Here, we prove that such an incompatibility does
arise in the case of extension stability on binary trees and in the case of
associative stability. Our methods combine general theoretical considerations
with the use of computer programs tailored to the given stability requirements.
"
Gridbot: An autonomous robot controlled by a Spiking Neural Network mimicking the brain's navigational system,"  It is true that the ""best"" neural network is not necessarily the one with the
most ""brain-like"" behavior. Understanding biological intelligence, however, is
a fundamental goal for several distinct disciplines. Translating our
understanding of intelligence to machines is a fundamental problem in robotics.
Propelled by new advancements in Neuroscience, we developed a spiking neural
network (SNN) that draws from mounting experimental evidence that a number of
individual neurons is associated with spatial navigation. By following the
brain's structure, our model assumes no initial all-to-all connectivity, which
could inhibit its translation to a neuromorphic hardware, and learns an
uncharted territory by mapping its identified components into a limited number
of neural representations, through spike-timing dependent plasticity (STDP). In
our ongoing effort to employ a bioinspired SNN-controlled robot to real-world
spatial mapping applications, we demonstrate here how an SNN may robustly
control an autonomous robot in mapping and exploring an unknown environment,
while compensating for its own intrinsic hardware imperfections, such as
partial or total loss of visual input.
"
A Data-Driven Approach to Extract Connectivity Structures from Diffusion Tensor Imaging Data,"  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of
structural brain connectivity in normal development and in a broad range of
brain disorders. However efforts to derive inherent characteristics of
structural brain networks have been hampered by the very high dimensionality of
the data, relatively small sample sizes, and the lack of widely acceptable
connectivity-based regions of interests (ROIs). Typical approaches have focused
either on regions defined by standard anatomical atlases that do not
incorporate anatomical connectivity, or have been based on voxel-wise analysis,
which results in loss of statistical power relative to structure-wise
connectivity analysis. In this work, we propose a novel, computationally
efficient iterative clustering method to generate connectivity-based
whole-brain parcellations that converge to a stable parcellation in a few
iterations. Our algorithm is based on a sparse representation of the whole
brain connectivity matrix, which reduces the number of edges from around a half
billion to a few million while incorporating the necessary spatial constraints.
We show that the resulting regions in a sense capture the inherent connectivity
information present in the data, and are stable with respect to initialization
and the randomization scheme within the algorithm. These parcellations provide
consistent structural regions across the subjects of population samples that
are homogeneous with respect to anatomic connectivity. Our method also derives
connectivity structures that can be used to distinguish between population
samples with known different structural connectivity. In particular, new
results in structural differences for different population samples such as
Females vs Males, Normal Controls vs Schizophrenia, and different age groups in
Normal Controls are also shown.
"
Controlling seizure propagation in large-scale brain networks,"  Information transmission in the human brain is a fundamentally dynamic
network process. In partial epilepsy, this process is perturbed and highly
synchronous seizures originate in a local network, the so-called epileptogenic
zone (EZ), before recruiting other close or distant brain regions. We studied
patient-specific brain network models of 15 drug-resistant epilepsy patients
with implanted stereotactic electroencephalography (SEEG) electrodes. Each
personalized brain model was derived from structural data of magnetic resonance
imaging (MRI) and diffusion tensor weighted imaging (DTI), comprising 88 nodes
equipped with region specific neural mass models capable of demonstrating a
range of epileptiform discharges. Each patients virtual brain was further
personalized through the integration of the clinically hypothesized EZ.
Subsequent simulations and connectivity modulations were performed and
uncovered a finite repertoire of seizure propagation patterns. Across patients,
we found that (i) patient-specific network connectivity is predictive for the
subsequent seizure propagation pattern; (ii)seizure propagation is
characterized by a systematic sequence of brain states; (iii) propagation can
be controlled by an optimal intervention on the connectivity matrix; (iv) the
degree of invasiveness can be significantly reduced via the here proposed
seizure control as compared to traditional resective surgery. To stop seizures,
neurosurgeons typically resect the EZ completely. We showed that stability
analysis of the network dynamics using graph theoretical metrics estimates
reliably the spatiotemporal properties of seizure propagation. This suggests
novel less invasive paradigms of surgical interventions to treat and manage
partial epilepsy.
"
Concerning the Neural Code,"  The central problem with understanding brain and mind is the neural code
issue: understanding the matter of our brain as basis for the phenomena of our
mind. The richness with which our mind represents our environment, the
parsimony of genetic data, the tremendous efficiency with which the brain
learns from scant sensory input and the creativity with which our mind
constructs mental worlds all speak in favor of mind as an emergent phenomenon.
This raises the further issue of how the neural code supports these processes
of organization. The central point of this communication is that the neural
code has the form of structured net fragments that are formed by network
self-organization, activate and de-activate on the functional time scale, and
spontaneously combine to form larger nets with the same basic structure.
"
Dynamic-sensitive cooperation in the presence of multiple strategy updating rules,"  The importance of microscopic details on cooperation level is an intensively
studied aspect of evolutionary game theory. Interestingly, these details become
crucial on heterogeneous populations where individuals may possess diverse
traits. By introducing a coevolutionary model in which not only strategies but
also individual dynamical features may evolve we revealed that the formerly
established conclusion is not necessarily true when different updating rules
are on stage. In particular, we apply two strategy updating rules, imitation
and Death-Birth rule, which allow local selection in a spatial system. Our
observation highlights that the microscopic feature of dynamics, like the level
of learning activity, could be a fundamental factor even if all players share
the same trait uniformly.
"
On the ground state of spiking network activity in mammalian cortex,"  Electrophysiological recordings of spiking activity are limited to a small
number of neurons. This spatial subsampling has hindered characterizing even
most basic properties of collective spiking in cortical networks. In
particular, two contradictory hypotheses prevailed for over a decade: the first
proposed an asynchronous irregular state, the second a critical state. While
distinguishing them is straightforward in models, we show that in experiments
classical approaches fail to correctly infer network dynamics because of
subsampling. Deploying a novel, subsampling-invariant estimator, we find that
in vivo dynamics do not comply with either hypothesis, but instead occupy a
narrow ""reverberating"" state consistently across multiple mammalian species and
cortical areas. A generic model tuned to this reverberating state predicts
single neuron, pairwise, and population properties. With these predictions we
first validate the model and then deduce network properties that are
challenging to obtain experimentally, like the network timescale and strength
of cortical input.
"
Inferring Information Flow in Spike-train Data Sets using a Trial-Shuffle Method,"  Understanding information processing in the brain requires the ability to
determine the functional connectivity between the different regions of the
brain. We present a method using transfer entropy to extract this flow of
information between brain regions from spike-train data commonly obtained in
neurological experiments. Transfer entropy is a statistical measure based in
information theory that attempts to quantify the information flow from one
process to another, and has been applied to find connectivity in simulated
spike-train data. Due to statistical error in the estimator, inferring
functional connectivity requires a method for determining significance in the
transfer entropy values. We discuss the issues with numerical estimation of
transfer entropy and resulting challenges in determining significance before
presenting the trial-shuffle method as a viable option. The trial-shuffle
method, for spike-train data that is split into multiple trials, determines
significant transfer entropy values independently for each individual pair of
neurons by comparing to a created baseline distribution using a rigorous
statistical test. This is in contrast to either globally comparing all neuron
transfer entropy values or comparing pairwise values to a single baseline
value.
In establishing the viability of this method by comparison to several
alternative approaches in the literature, we find evidence that preserving the
inter-spike-interval timing is important.
We then use the trial-shuffle method to investigate information flow within a
model network as we vary model parameters. This includes investigating the
global flow of information within a connectivity network divided into two
well-connected subnetworks, going beyond local transfer of information between
pairs of neurons.
"
Pharmacokinetics Simulations for Studying Correlates of Prevention Efficacy of Passive HIV-1 Antibody Prophylaxis in the Antibody Mediated Prevention (AMP) Study,"  A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate
whether drug concentration over time, as estimated by non-linear mixed effects
pharmacokinetics (PK) models, is associated with HIV infection rate. We
conducted a simulation study of marker sampling designs, and evaluated the
effect of study adherence and sub-cohort sample size on PK model estimates in
multiple-dose studies. With m=120, even under low adherence (about half of
study visits missing per participant), reasonably unbiased and consistent
estimates of most fixed and random effect terms were obtained. Coarsened marker
sampling schedules were also studied.
"
"Properties of interaction networks, structure coefficients, and benefit-to-cost ratios","  In structured populations the spatial arrangement of cooperators and
defectors on the interaction graph together with the structure of the graph
itself determines the game dynamics and particularly whether or not fixation of
cooperation (or defection) is favored. For a single cooperator (and a single
defector) and a network described by a regular graph the question of fixation
can be addressed by a single parameter, the structure coefficient. As this
quantity is generic for any regular graph, we may call it the generic structure
coefficient. For two and more cooperators (or several defectors) fixation
properties can also be assigned by structure coefficients. These structure
coefficients, however, depend on the arrangement of cooperators and defectors
which we may interpret as a configuration of the game. Moreover, the
coefficients are specific for a given interaction network modeled as regular
graph, which is why we may call them specific structure coefficients. In this
paper, we study how specific structure coefficients vary over interaction
graphs and link the distributions obtained over different graphs to spectral
properties of interaction networks. We also discuss implications for the
benefit-to-cost ratios of donation games.
"
A Survey of the State-of-the-Art Parallel Multiple Sequence Alignment Algorithms on Multicore Systems,"  Evolutionary modeling applications are the best way to provide full
information to support in-depth understanding of evaluation of organisms. These
applications mainly depend on identifying the evolutionary history of existing
organisms and understanding the relations between them, which is possible
through the deep analysis of their biological sequences. Multiple Sequence
Alignment (MSA) is considered an important tool in such applications, where it
gives an accurate representation of the relations between different biological
sequences. In literature, many efforts have been put into presenting a new MSA
algorithm or even improving existing ones. However, little efforts on
optimizing parallel MSA algorithms have been done. Nowadays, large datasets
become a reality, and big data become a primary challenge in various fields,
which should be also a new milestone for new bioinformatics algorithms. This
survey presents four of the state-of-the-art parallel MSA algorithms, TCoffee,
MAFFT, MSAProbs, and M2Align. We provide a detailed discussion of each
algorithm including its strengths, weaknesses, and implementation details and
the effectiveness of its parallel implementation compared to the other
algorithms, taking into account the MSA accuracy on two different datasets,
BAliBASE and OXBench.
"
Learning a Generative Model of Cancer Metastasis,"  We introduce a Unified Disentanglement Network (UFDN) trained on The Cancer
Genome Atlas (TCGA). We demonstrate that the UFDN learns a biologically
relevant latent space of gene expression data by applying our network to two
classification tasks of cancer status and cancer type. Our UFDN specific
algorithms perform comparably to random forest methods. The UFDN allows for
continuous, partial interpolation between distinct cancer types. Furthermore,
we perform an analysis of differentially expressed genes between skin cutaneous
melanoma(SKCM) samples and the same samples interpolated into glioblastoma
(GBM). We demonstrate that our interpolations learn relevant metagenes that
recapitulate known glioblastoma mechanisms and suggest possible starting points
for investigations into the metastasis of SKCM into GBM.
"
A model of reward-modulated motor learning with parallelcortical and basal ganglia pathways,"  Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons' firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson's disease and its treatment.
"
Storing and retrieving long-term memories: cooperation and competition in synaptic dynamics,"  We first review traditional approaches to memory storage and formation,
drawing on the literature of quantitative neuroscience as well as statistical
physics. These have generally focused on the fast dynamics of neurons; however,
there is now an increasing emphasis on the slow dynamics of synapses, whose
weight changes are held to be responsible for memory storage. An important
first step in this direction was taken in the context of Fusi's cascade model,
where complex synaptic architectures were invoked, in particular, to store
long-term memories. No explicit synaptic dynamics were, however, invoked in
that work. These were recently incorporated theoretically using the techniques
used in agent-based modelling, and subsequently, models of competing and
cooperating synapses were formulated. It was found that the key to the storage
of long-term memories lay in the competitive dynamics of synapses. In this
review, we focus on models of synaptic competition and cooperation, and look at
the outstanding challenges that remain.
"
City-wide Analysis of Electronic Health Records Reveals Gender and Age Biases in the Administration of Known Drug-Drug Interactions,"  The occurrence of drug-drug-interactions (DDI) from multiple drug
prescriptions is a serious problem, both for individuals and health-care
systems, since patients with complications due to DDI are likely to re-enter
the system at a costlier level. We present a large-scale longitudinal study of
the DDI phenomenon at the primary- and secondary-care level using electronic
health records from the city of Blumenau in Southern Brazil (pop. ~340,000).
This is the first study of DDI we are aware of that follows an entire city
longitudinally for 18 months. We found that 181 distinct drug pairs known to
interact were dispensed concomitantly to 12% of the patients in the city's
public health-care system. Further, 4% of the patients were dispensed major DDI
combinations, likely to result in very serious adverse reactions and costs we
estimate to be larger than previously reported. DDI results are integrated into
associative networks for inference and visualization, revealing key medications
and interactions. Analysis reveals that women have a 60% increased risk of DDI
as compared to men; the increase becomes 90% when only major DDI are
considered. Furthermore, DDI risk increases substantially with age. Patients
aged 70-79 years have a 34% risk of DDI when they are prescribed two or more
drugs concomitantly. Interestingly, a null model demonstrates that age and
women-specific risks from increased polypharmacy far exceed expectations in
those populations. This suggests that social and biological factors are at
play. Finally, we demonstrate that machine learning classifiers accurately
predict patients likely to be administered DDI given their history of
prescribed drugs, gender, and age (MCC=.7,AUC=.97). These results demonstrate
that accurate warning systems for known DDI can be devised for health-care
systems leading to substantial reduction of DDI-related adverse reactions and
health-care savings.
"
A remark on the disorienting of species due to the fluctuating environment,"  In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
"
Can Transfer Entropy Infer Causality in Neuronal Circuits for Cognitive Processing?,"  Finding the causes to observed effects and establishing causal relationships
between events is (and has been) an essential element of science and
philosophy. Automated methods that can detect causal relationships would be
very welcome, but practical methods that can infer causality are difficult to
find, and the subject of ongoing research. While Shannon information only
detects correlation, there are several information-theoretic notions of
""directed information"" that have successfully detected causality in some
systems, in particular in the neuroscience community. However, recent work has
shown that some directed information measures can sometimes inadequately
estimate the extent of causal relations, or even fail to identify existing
cause-effect relations between components of systems, especially if neurons
contribute in a cryptographic manner to influence the effector neuron. Here, we
test how often cryptographic logic emerges in an evolutionary process that
generates artificial neural circuits for two fundamental cognitive tasks:
motion detection and sound localization. Our results suggest that whether or
not transfer entropy measures of causality are misleading depends strongly on
the cognitive task considered. These results emphasize the importance of
understanding the fundamental logic processes that contribute to cognitive
processing, and quantifying their relevance in any given nervous system.
"
Drive and measurement electrode patterns for electrode impedance tomography (EIT) imaging of neural activity in peripheral nerve,"  Objective: To establish the performance of several drive and measurement
patterns in EIT imaging of neural activity in peripheral nerve, which involves
large impedance change in the nerve's anisotropic length axis. Approach: Eight
drive and measurement electrode patterns are compared using a finite element
(FE) four cylindrical shell model of a peripheral nerve and a 32 channel
dual-ring nerve cuff. The central layer of the FE model contains impedance
changes representative of neural activity of -0.3 in the length axis and -8.8 x
10-4 in the radial axis. Four of the electrode patterns generate longitudinal
drive current, which runs perpendicular to the anisotropic axis. Main results:
Transverse current patterns produce higher resolution than longitudinal
patterns but are also more susceptible to noise and errors, and exhibit poorer
sensitivity to impedance changes in central sample locations. Three of the four
longitudinal current patterns considered can reconstruct fascicle level
impedance changes with up to 0.2 mV noise and error, which corresponds to
between -5.5 and +0.18 dB of the normalised signal standard deviation. Reducing
the spacing between the two electrode rings in all longitudinal current
patterns reduced the signal to error ratio across all depth locations of the
sample. Significance: Electrode patterns which target the large impedance
change in the anisotropic length axis can provide improved robustness against
noise and errors, which is a critical step towards real time EIT imaging of
neural activity in peripheral nerve.
"
Nerve impulse propagation and wavelet theory,"  A luminous stimulus which penetrates in a retina is converted to a nerve
message. Ganglion cells give a response that may be approximated by a wavelet.
We determine a function PSI which is associated with the propagation of nerve
impulses along an axon. Each kind of channel (inward and outward) may be open
or closed, depending on the transmembrane potential. The transition between
these states is a random event. Using quantum relations, we estimate the number
of channels susceptible to switch between the closed and open states. Our
quantum approach was first to calculate the energy level distribution in a
channel. We obtain, for each kind of channel, the empty level density and the
filled level density of the open and closed conformations. The joint density of
levels provides the transition number between the closed and open
conformations. The algebraic sum of inward and outward open channels is a
function PSI of the normalized energy E. The function PSI verifies the major
properties of a wavelet. We calculate the functional dependence of the axon
membrane conductance with the transmembrane energy.
"
The inseparability of sampling and time and its influence on attempts to unify the molecular and fossil records,"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
"
White Matter Network Architecture Guides Direct Electrical Stimulation Through Optimal State Transitions,"  Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
"
Morphology and Motility of Cells on Soft Substrates,"  Recent experiments suggest that the interplay between cells and the mechanics
of their substrate gives rise to a diversity of morphological and migrational
behaviors. Here, we develop a Cellular Potts Model of polarizing cells on a
visco-elastic substrate. We compare our model with experiments on endothelial
cells plated on polyacrylamide hydrogels to constrain model parameters and test
predictions. Our analysis reveals that morphology and migratory behavior are
determined by an intricate interplay between cellular polarization and
substrate strain gradients generated by traction forces exerted by cells
(self-haptotaxis).
"
Collective search with finite perception: transient dynamics and search efficiency,"  Motile organisms often use finite spatial perception of their surroundings to
navigate and search their habitats. Yet standard models of search are usually
based on purely local sensory information. To model how a finite perceptual
horizon affects ecological search, we propose a framework for optimal
navigation that combines concepts from random walks and optimal control theory.
We show that, while local strategies are optimal on asymptotically long and
short search times, finite perception yields faster convergence and increased
search efficiency over transient time scales relevant in biological systems.
The benefit of the finite horizon can be maintained by the searchers tuning
their response sensitivity to the length scale of the stimulant in the
environment, and is enhanced when the agents interact as a result of increased
consensus within subpopulations. Our framework sheds light on the role of
spatial perception and transients in search movement and collective sensing of
the environment.
"
Exact traveling wave solutions of 1D model of cancer invasion,"  In this paper we consider the continuous mathematical model of tumour growth
and invasion based on the model introduced by Anderson, Chaplain et al.
\cite{Anderson&Chaplain2000}, for the case of one space dimension. The model
consists of a system of three coupled nonlinear reaction-diffusion-taxis
partial differential equations describing the interactions between cancer
cells, the matrix degrading enzyme and the tissue. For this model under certain
conditions on the model parameters we obtain the exact analytical solutions in
terms of traveling wave variables. These solutions are smooth positive definite
functions whose profiles agree with those obtained from numerical computations
\cite{Chaplain&Lolas2006} for not very large time intervals.
"
EEG machine learning with Higuchi fractal dimension and Sample Entropy as features for successful detection of depression,"  Reliable diagnosis of depressive disorder is essential for both optimal
treatment and prevention of fatal outcomes. In this study, we aimed to
elucidate the effectiveness of two non-linear measures, Higuchi Fractal
Dimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders
when applied on EEG. HFD and SampEn of EEG signals were used as features for
seven machine learning algorithms including Multilayer Perceptron, Logistic
Regression, Support Vector Machines with the linear and polynomial kernel,
Decision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG
between healthy control subjects and patients diagnosed with depression. We
confirmed earlier observations that both non-linear measures can discriminate
EEG signals of patients from healthy control subjects. The results suggest that
good classification is possible even with a small number of principal
components. Average accuracy among classifiers ranged from 90.24% to 97.56%.
Among the two measures, SampEn had better performance. Using HFD and SampEn and
a variety of machine learning techniques we can accurately discriminate
patients diagnosed with depression vs controls which can serve as a highly
sensitive, clinically relevant marker for the diagnosis of depressive
disorders.
"
Protein Classification using Machine Learning and Statistical Techniques: A Comparative Analysis,"  In recent era prediction of enzyme class from an unknown protein is one of
the challenging tasks in bioinformatics. Day to day the number of proteins is
increases as result the prediction of enzyme class gives a new opportunity to
bioinformatics scholars. The prime objective of this article is to implement
the machine learning classification technique for feature selection and
predictions also find out an appropriate classification technique for function
prediction. In this article the seven different classification technique like
CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has
been implemented on 4368 protein data that has been extracted from UniprotKB
databank and categories into six different class. The proteins data is high
dimensional sequence data and contain a maximum of 48 features.To manipulate
the high dimensional sequential protein data with different classification
technique, the SPSS has been used as an experimental tool. Different
classification techniques give different results for every model and shows that
the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the
performance of model. In these three classes the precision and recall value is
very less or negligible. The experimental results highlight that the C5.0
classification technique accuracy is more suited for protein feature
classification and predictions. The C5.0 classification technique gives 95.56%
accuracy and also gives high precision and recall value. Finally, we conclude
that the features that is selected can be used for function prediction.
"
The Importance of Constraint Smoothness for Parameter Estimation in Computational Cognitive Modeling,"  Psychiatric neuroscience is increasingly aware of the need to define
psychopathology in terms of abnormal neural computation. The central tool in
this endeavour is the fitting of computational models to behavioural data. The
most prominent example of this procedure is fitting reinforcement learning (RL)
models to decision-making data collected from mentally ill and healthy subject
populations. These models are generative models of the decision-making data
themselves, and the parameters we seek to infer can be psychologically and
neurobiologically meaningful. Currently, the gold standard approach to this
inference procedure involves Monte-Carlo sampling, which is robust but
computationally intensive---rendering additional procedures, such as
cross-validation, impractical. Searching for point estimates of model
parameters using optimization procedures remains a popular and interesting
option. On a novel testbed simulating parameter estimation from a common RL
task, we investigated the effects of smooth vs. boundary constraints on
parameter estimation using interior point and deterministic direct search
algorithms for optimization. Ultimately, we show that the use of boundary
constraints can lead to substantial truncation effects. Our results discourage
the use of boundary constraints for these applications.
"
Inferring network connectivity from event timing patterns,"  Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
"
The Impact of Information Dissemination on Vaccinating Epidemics in Multiplex Networks,"  The impact of information dissemination on epidemic control essentially
affects individual behaviors. Among the information-driven behaviors,
vaccination is determined by the cost-related factors, and the correlation with
information dissemination is not clear yet. To this end, we present a model to
integrate the information-epidemic spread process into an evolutionary
vaccination game in multiplex networks, and explore how the spread of
information on epidemic influences the vaccination behavior. We propose a
two-layer coupled susceptible-alert-infected-susceptible (SAIS) model on a
multiplex network, where the strength coefficient is defined to characterize
the tendency and intensity of information dissemination. By means of the
evolutionary game theory, we get the equilibrium vaccination level (the
evolutionary stable strategy) for the vaccination game. After exploring the
influence of the strength coefficient on the equilibrium vaccination level, we
reach a counter-intuitive conclusion that more information transmission cannot
promote vaccination. Specifically, when the vaccination cost is within a
certain range, increasing information dissemination even leads to a decline in
the equilibrium vaccination level. Moreover, we study the influence of the
strength coefficient on the infection density and social cost, and unveil the
role of information dissemination in controlling the epidemic with numerical
simulations.
"
An In Vitro Vascularized Tumor Platform for Modeling Breast Tumor Stromal Interactions and Characterizing the Subsequent Response,"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
"
Locally-adaptive Bayesian nonparametric inference for phylodynamics,"  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
"
Out of sight out of mind: Perceived physical distance between the observer and someone in pain shapes observer's neural empathic reactions,"  Social and affective relations may shape empathy to others' affective states.
Previous studies also revealed that people tend to form very different mental
representations of stimuli on the basis of their physical distance. In this
regard, embodied cognition proposes that different physical distances between
individuals activate different interpersonal processing modes, such that close
physical distance tends to activate the interpersonal processing mode typical
of socially and affectively close relationships. In Experiment 1, two groups of
participants were administered a pain decision task involving upright and
inverted face stimuli painfully or neutrally stimulated, and we monitored their
neural empathic reactions by means of event-related potentials (ERPs)
technique. Crucially, participants were presented with face stimuli of one of
two possible sizes in order to manipulate retinal size and perceived physical
distance, roughly corresponding to the close and far portions of social
distance. ERPs modulations compatible with an empathic reaction were observed
only for the group exposed to face stimuli appearing to be at a close social
distance from the participants. This reaction was absent in the group exposed
to smaller stimuli corresponding to face stimuli observed from a far social
distance. In Experiment 2, one different group of participants was engaged in a
match-to-sample task involving the two-size upright face stimuli of Experiment
1 to test whether the modulation of neural empathic reaction observed in
Experiment 1 could be ascribable to differences in the ability to identify
faces of the two different sizes. Results suggested that face stimuli of the
two sizes could be equally identifiable. In line with the Construal Level and
Embodied Simulation theoretical frameworks, we conclude that perceived physical
distance may shape empathy as well as social and affective distance.
"
Latent Molecular Optimization for Targeted Therapeutic Design,"  We devise an approach for targeted molecular design, a problem of interest in
computational drug discovery: given a target protein site, we wish to generate
a chemical with both high binding affinity to the target and satisfactory
pharmacological properties. This problem is made difficult by the enormity and
discreteness of the space of potential therapeutics, as well as the
graph-structured nature of biomolecular surface sites. Using a dataset of
protein-ligand complexes, we surmount these issues by extracting a signature of
the target site with a graph convolutional network and by encoding the discrete
chemical into a continuous latent vector space. The latter embedding permits
gradient-based optimization in molecular space, which we perform using learned
differentiable models of binding affinity and other pharmacological properties.
We show that our approach is able to efficiently optimize these multiple
objectives and discover new molecules with potentially useful binding
properties, validated via docking methods.
"
Mathematical model of immune response to hepatitis B,"  A new detailed mathematical model for dynamics of immune response to
hepatitis B is proposed, which takes into account contributions from innate and
adaptive immune responses, as well as cytokines. Stability analysis of
different steady states is performed to identify parameter regions where the
model exhibits clearance of infection, maintenance of a chronic infection, or
periodic oscillations. Effects of nucleoside analogues and interferon
treatments are analysed, and the critical drug efficiency is determined.
"
Dynamics of the brain extracellular matrix governed by interactions with neural cells,"  Neuronal and glial cells release diverse proteoglycans and glycoproteins,
which aggregate in the extracellular space and form the extracellular matrix
(ECM) that may in turn regulate major cellular functions. Brain cells also
release extracellular proteases that may degrade the ECM, and both synthesis
and degradation of ECM are activity-dependent. In this study we introduce a
mathematical model describing population dynamics of neurons interacting with
ECM molecules over extended timescales. It is demonstrated that depending on
the prevalent biophysical mechanism of ECM-neuronal interactions, different
dynamical regimes of ECM activity can be observed, including bistable states
with stable stationary levels of ECM molecule concentration, spontaneous ECM
oscillations, and coexistence of ECM oscillations and a stationary state,
allowing dynamical switches between activity regimes.
"
"Search for Food of Birds, Fish and Insects","  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
"
Modeling the Formation of Social Conventions in Multi-Agent Populations,"  In order to understand the formation of social conventions we need to know
the specific role of control and learning in multi-agent systems. To advance in
this direction, we propose, within the framework of the Distributed Adaptive
Control (DAC) theory, a novel Control-based Reinforcement Learning architecture
(CRL) that can account for the acquisition of social conventions in multi-agent
populations that are solving a benchmark social decision-making problem. Our
new CRL architecture, as a concrete realization of DAC multi-agent theory,
implements a low-level sensorimotor control loop handling the agent's reactive
behaviors (pre-wired reflexes), along with a layer based on model-free
reinforcement learning that maximizes long-term reward. We apply CRL in a
multi-agent game-theoretic task in which coordination must be achieved in order
to find an optimal solution. We show that our CRL architecture is able to both
find optimal solutions in discrete and continuous time and reproduce human
experimental data on standard game-theoretic metrics such as efficiency in
acquiring rewards, fairness in reward distribution and stability of convention
formation.
"
Evidence accumulation in a Laplace domain decision space,"  Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
"
"A unified, mechanistic framework for developmental and evolutionary change","  The two most fundamental processes describing change in biology -development
and evolution- occur over drastically different timescales, difficult to
reconcile within a unified framework. Development involves a temporal sequence
of cell states controlled by a hierarchy of regulatory structures. It occurs
over the lifetime of a single individual, and is associated to the gene
expression level change of a given genotype. Evolution, by contrast entails
genotypic change through the acquisition or loss of genes, and involves the
emergence of new, environmentally selected phenotypes over the lifetimes of
many individ- uals. Here we present a model of regulatory network evolution
that accounts for both timescales. We extend the framework of boolean models of
gene regulatory network (GRN)-currently only applicable to describing
development-to include evolutionary processes. As opposed to one-to-one maps to
specific attractors, we identify the phenotypes of the cells as the relevant
macrostates of the GRN. A pheno- type may now correspond to multiple
attractors, and its formal definition no longer require a fixed size for the
genotype. This opens the possibility for a quantitative study of the phenotypic
change of a genotype, which is itself changing over evolutionary timescales. We
show how the realization of specific phenotypes can be controlled by gene
duplication events, and how successive events of gene duplication lead to new
regulatory structures via selection. It is these structures that enable control
of macroscale patterning, as in development. The proposed framework therefore
provides a mechanistic explanation for the emergence of regulatory structures
controlling development over evolutionary time.
"
On the Simpson index for the Moran process with random selection and immigration,"  Moran or Wright-Fisher processes are probably the most well known model to
study the evolution of a population under various effects. Our object of study
will be the Simpson index which measures the level of diversity of the
population, one of the key parameter for ecologists who study for example
forest dynamics. Following ecological motivations, we will consider here the
case where there are various species with fitness and immigration parameters
being random processes (and thus time evolving). To measure biodiversity,
ecologists generally use the Simpson index, who has no closed formula, except
in the neutral (no selection) case via a backward approach, and which is
difficult to evaluate even numerically when the population size is large. Our
approach relies on the large population limit in the ""weak"" selection case, and
thus to give a procedure which enable us to approximate, with controlled rate,
the expectation of the Simpson index at fixed time. Our approach will be
forward and valid for all time, which is the main difference with the
historical approach of Kingman, or Krone-Neuhauser. We will also study the long
time behaviour of the Wright-Fisher process in a simplified setting, allowing
us to get a full picture for the approximation of the expectation of the
Simpson index.
"
Pre-Synaptic Pool Modification (PSPM): A Supervised Learning Procedure for Spiking Neural Networks,"  A central question in neuroscience is how to develop realistic models that
predict output firing behavior based on provided external stimulus. Given a set
of external inputs and a set of output spike trains, the objective is to
discover a network structure which can accomplish the transformation as
accurately as possible. Due to the difficulty of this problem in its most
general form, approximations have been made in previous work. Past
approximations have sacrificed network size, recurrence, allowed spiked count,
or have imposed layered network structure. Here we present a learning rule
without these sacrifices, which produces a weight matrix of a leaky
integrate-and-fire (LIF) network to match the output activity of both
deterministic LIF networks as well as probabilistic integrate-and-fire (PIF)
networks. Inspired by synaptic scaling, our pre-synaptic pool modification
(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks
that can provide a novel generative model for given spike trains. Similarity in
output spike trains is evaluated with a variety of metrics including a
van-Rossum like measure and a numerical comparison of inter-spike interval
distributions. Application of our algorithm to randomly generated networks
improves similarity to the reference spike trains on both of these stated
measures. In addition, we generated LIF networks that operate near criticality
when trained on critical PIF outputs. Our results establish that learning rules
based on synaptic homeostasis can be used to represent input-output
relationships in fully recurrent spiking neural networks.
"
Ensemble Inhibition and Excitation in the Human Cortex: an Ising Model Analysis with Uncertainties,"  The pairwise maximum entropy model, also known as the Ising model, has been
widely used to analyze the collective activity of neurons. However, controversy
persists in the literature about seemingly inconsistent findings, whose
significance is unclear due to lack of reliable error estimates. We therefore
develop a method for accurately estimating parameter uncertainty based on
random walks in parameter space using adaptive Markov Chain Monte Carlo after
the convergence of the main optimization algorithm. We apply our method to the
spiking patterns of excitatory and inhibitory neurons recorded with
multielectrode arrays in the human temporal cortex during the wake-sleep cycle.
Our analysis shows that the Ising model captures neuronal collective behavior
much better than the independent model during wakefulness, light sleep, and
deep sleep when both excitatory (E) and inhibitory (I) neurons are modeled;
ignoring the inhibitory effects of I-neurons dramatically overestimates
synchrony among E-neurons. Furthermore, information-theoretic measures reveal
that the Ising model explains about 80%-95% of the correlations, depending on
sleep state and neuron type. Thermodynamic measures show signatures of
criticality, although we take this with a grain of salt as it may be merely a
reflection of long-range neural correlations.
"
Combined MEG and fMRI Exponential Random Graph Modeling for inferring functional Brain Connectivity,"  Estimated connectomes by the means of neuroimaging techniques have enriched
our knowledge of the organizational properties of the brain leading to the
development of network-based clinical diagnostics. Unfortunately, to date, many
of those network-based clinical diagnostics tools, based on the mere
description of isolated instances of observed connectomes are noisy estimates
of the true connectivity network. Modeling brain connectivity networks is
therefore important to better explain the functional organization of the brain
and allow inference of specific brain properties. In this report, we present
pilot results on the modeling of combined MEG and fMRI neuroimaging data
acquired during an n-back memory task experiment. We adopted a pooled
Exponential Random Graph Model (ERGM) as a network statistical model to capture
the underlying process in functional brain networks of 9 subjects MEG and fMRI
data out of 32 during a 0-back vs 2-back memory task experiment. Our results
suggested strong evidence that all the functional connectomes of the 9 subjects
have small world properties. A group level comparison using comparing the
conditions pairwise showed no significant difference in the functional
connectomes across the subjects. Our pooled ERGMs successfully reproduced
important brain properties such as functional segregation and functional
integration. However, the ERGMs reproducing the functional segregation of the
brain networks discriminated between the 0-back and 2-back conditions while the
models reproducing both properties failed to successfully discriminate between
both conditions. Our results are promising and would improve in robustness with
a larger sample size. Nevertheless, our pilot results tend to support previous
findings that functional segregation and integration are sufficient to
statistically reproduce the main properties of brain network.
"
On sound-based interpretation of neonatal EEG,"  Significant training is required to visually interpret neonatal EEG signals.
This study explores alternative sound-based methods for EEG interpretation
which are designed to allow for intuitive and quick differentiation between
healthy background activity and abnormal activity such as seizures. A novel
method based on frequency and amplitude modulation (FM/AM) is presented. The
algorithm is tuned to facilitate the audio domain perception of rhythmic
activity which is specific to neonatal seizures. The method is compared with
the previously developed phase vocoder algorithm for different time compressing
factors. A survey is conducted amongst a cohort of non-EEG experts to
quantitatively and qualitatively examine the performance of sound-based methods
in comparison with the visual interpretation. It is shown that both
sonification methods perform similarly well, with a smaller inter-observer
variability in comparison with visual. A post-survey analysis of results is
performed by examining the sensitivity of the ear to frequency evolution in
audio.
"
Aspiration dynamics generate robust predictions in structured populations,"  Evolutionary game dynamics in structured populations are strongly affected by
updating rules. Previous studies usually focus on imitation-based rules, which
rely on payoff information of social peers. Recent behavioral experiments
suggest that whether individuals use such social information for strategy
updating may be crucial to the outcomes of social interactions. This hints at
the importance of considering updating rules without dependence on social
peers' payoff information, which, however, is rarely investigated. Here, we
study aspiration-based self-evaluation rules, with which individuals
self-assess the performance of strategies by comparing own payoffs with an
imaginary value they aspire, called the aspiration level. We explore the fate
of strategies on population structures represented by graphs or networks. Under
weak selection, we analytically derive the condition for strategy dominance,
which is found to coincide with the classical condition of risk-dominance. This
condition holds for all networks and all distributions of aspiration levels,
and for individualized ways of self-evaluation. Our condition can be
intuitively interpreted: one strategy prevails over the other if the strategy
brings more satisfaction to individuals than the other does. Our work thus
sheds light on the intrinsic difference between evolutionary dynamics induced
by aspiration-based and imitation-based rules.
"
Mathematical Analysis of Anthropogenic Signatures: The Great Deceleration,"  Distributions of anthropogenic signatures (impacts and activities) are
mathematically analysed. The aim is to understand the Anthropocene and to see
whether anthropogenic signatures could be used to determine its beginning. A
total of 23 signatures were analysed and results are presented in 31 diagrams.
Some of these signatures contain undistinguishable natural components but most
of them are of purely anthropogenic origin. Great care was taken to identify
abrupt accelerations, which could be used to determine the beginning of the
Anthropocene. Results of the analysis can be summarised in three conclusions.
1. Anthropogenic signatures cannot be used to determine the beginning of the
Anthropocene. 2. There was no abrupt Great Acceleration around 1950 or around
any other time. 3. Anthropogenic signatures are characterised by the Great
Deceleration in the second half of the 20th century. The second half of the
20th century does not mark the beginning of the Anthropocene but most likely
the beginning of the end of the strong anthropogenic impacts, maybe even the
beginning of a transition to a sustainable future. The Anthropocene is a unique
stage in human experience but it has no clearly marked beginning and it is
probably not a new geological epoch.
"
Modeling human intuitions about liquid flow with particle-based simulation,"  Humans can easily describe, imagine, and, crucially, predict a wide variety
of behaviors of liquids--splashing, squirting, gushing, sloshing, soaking,
dripping, draining, trickling, pooling, and pouring--despite tremendous
variability in their material and dynamical properties. Here we propose and
test a computational model of how people perceive and predict these liquid
dynamics, based on coarse approximate simulations of fluids as collections of
interacting particles. Our model is analogous to a ""game engine in the head"",
drawing on techniques for interactive simulations (as in video games) that
optimize for efficiency and natural appearance rather than physical accuracy.
In two behavioral experiments, we found that the model accurately captured
people's predictions about how liquids flow among complex solid obstacles, and
was significantly better than two alternatives based on simple heuristics and
deep neural networks. Our model was also able to explain how people's
predictions varied as a function of the liquids' properties (e.g., viscosity
and stickiness). Together, the model and empirical results extend the recent
proposal that human physical scene understanding for the dynamics of rigid,
solid objects can be supported by approximate probabilistic simulation, to the
more complex and unexplored domain of fluid dynamics.
"
A Connectome Based Hexagonal Lattice Convolutional Network Model of the Drosophila Visual System,"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
"
Some simple rules for estimating reproduction numbers in the presence of reservoir exposure or imported cases,"  The basic reproduction number ($R_0$) is a threshold parameter for disease
extinction or survival in isolated populations. However no human population is
fully isolated from other human or animal populations. We use compartmental
models to derive simple rules for the basic reproduction number for populations
with local person-to-person transmission and exposure from some other source:
either a reservoir exposure or imported cases. We introduce the idea of a
reservoir-driven or importation-driven disease: diseases that would become
extinct in the population of interest without reservoir exposure or imported
cases (since $R_0<1$), but nevertheless may be sufficiently transmissible that
many or most infections are acquired from humans in that population. We show
that in the simplest case, $R_0<1$ if and only if the proportion of infections
acquired from the external source exceeds the disease prevalence and explore
how population heterogeneity and the interactions of multiple strains affect
this rule. We apply these rules in two cases studies of Clostridium difficile
infection and colonisation: C. difficile in the hospital setting accounting for
imported cases, and C. difficile in the general human population accounting for
exposure to animal reservoirs. We demonstrate that even the hospital-adapted,
highly-transmissible NAP1/RT027 strain of C. difficile had a reproduction
number <1 in a landmark study of hospitalised patients and therefore was
sustained by colonised and infected admissions to the study hospital. We argue
that C. difficile should be considered reservoir-driven if as little as 13.0%
of transmission can be attributed to animal reservoirs.
"
The paradox of Vito Volterra's predator-prey model,"  This article is dedicated to the late Giorgio Israel. R{é}sum{é}. The aim
of this article is to propose on the one hand a brief history of modeling
starting from the works of Fibonacci, Robert Malthus, Pierre Francis Verhulst
and then Vito Volterra and, on the other hand, to present the main hypotheses
of the very famous but very little known predator-prey model elaborated in the
1920s by Volterra in order to solve a problem posed by his son-in-law, Umberto
D'Ancona. It is thus shown that, contrary to a widely-held notion, Volterra's
model is realistic and his seminal work laid the groundwork for modern
population dynamics and mathematical ecology, including seasonality, migration,
pollution and more. 1. A short history of modeling 1.1. The Malthusian model.
If the rst scientic view of population growth seems to be that of Leonardo
Fibonacci [2], also called Leonardo of Pisa, whose famous sequence of numbers
was presented in his Liber abaci (1202) as a solution to a population growth
problem, the modern foundations of population dynamics clearly date from Thomas
Robert Malthus [20]. Considering an ideal population consisting of a single
homogeneous animal species, that is, neglecting the variations in age, size and
any periodicity for birth or mortality, and which lives alone in an invariable
environment or coexists with other species without any direct or indirect
inuence, he founded in 1798, with his celebrated claim Population, when
unchecked, increases in a geometrical ratio, the paradigm of exponential
growth. This consists in assuming that the increase of the number N (t) of
individuals of this population, during a short interval of time, is
proportional to N (t). This translates to the following dierential equation :
(1) dN (t) dt = $\epsilon$N (t) where $\epsilon$ is a constant factor of
proportionality that represents the growth coe-cient or growth rate. By
integrating (1) we obtain the law of exponential growth or law of Malthusian
growth (see Fig. 1). This law, which does not take into account the limits
imposed by the environment on growth and which is in disagreement with the
actual facts, had a profound inuence on Charles Darwin's work on natural
selection. Indeed, Darwin [1] founded the idea of survival of the ttest on the
1. According to Frontier and Pichod-Viale [3] the correct terminology should be
population kinetics, since the interaction between species cannot be
represented by forces. 2. A population is dened as the set of individuals of
the same species living on the same territory and able to reproduce among
themselves.
"
Knowing the past improves cooperation in the future,"  Cooperation is the cornerstone of human evolutionary success. Like no other
species, we champion the sacrifice of personal benefits for the common good,
and we work together to achieve what we are unable to achieve alone. Knowledge
and information from past generations is thereby often instrumental in ensuring
we keep cooperating rather than deteriorating to less productive ways of
coexistence. Here we present a mathematical model based on evolutionary game
theory that shows how using the past as the benchmark for evolutionary success,
rather than just current performance, significantly improves cooperation in the
future. Interestingly, the details of just how the past is taken into account
play only second-order importance, whether it be a weighted average of past
payoffs or just a single payoff value from the past. Cooperation is promoted
because information from the past disables fast invasions of defectors, thus
enhancing the long-term benefits of cooperative behavior.
"
Protein Pattern Formation,"  Protein pattern formation is essential for the spatial organization of many
intracellular processes like cell division, flagellum positioning, and
chemotaxis. A prominent example of intracellular patterns are the oscillatory
pole-to-pole oscillations of Min proteins in \textit{E. coli} whose biological
function is to ensure precise cell division. Cell polarization, a prerequisite
for processes such as stem cell differentiation and cell polarity in yeast, is
also mediated by a diffusion-reaction process. More generally, these functional
modules of cells serve as model systems for self-organization, one of the core
principles of life. Under which conditions spatio-temporal patterns emerge, and
how these patterns are regulated by biochemical and geometrical factors are
major aspects of current research. Here we review recent theoretical and
experimental advances in the field of intracellular pattern formation, focusing
on general design principles and fundamental physical mechanisms.
"
A Hybrid Multiscale Model for Cancer Invasion of the Extracellular Matrix,"  The ability to locally degrade the extracellular matrix (ECM) and interact
with the tumour microenvironment is a key process distinguishing cancer from
normal cells, and is a critical step in the metastatic spread of the tumour.
The invasion of the surrounding tissue involves the coordinated action between
cancer cells, the ECM, the matrix degrading enzymes, and the
epithelial-to-mesenchymal transition (EMT). This is a regulatory process
through which epithelial cells (ECs) acquire mesenchymal characteristics and
transform to mesenchymal-like cells (MCs). In this paper, we present a new
mathematical model which describes the transition from a collective invasion
strategy for the ECs to an individual invasion strategy for the MCs. We achieve
this by formulating a coupled hybrid system consisting of partial and
stochastic differential equations that describe the evolution of the ECs and
the MCs, respectively. This approach allows one to reproduce in a very natural
way fundamental qualitative features of the current biomedical understanding of
cancer invasion that are not easily captured by classical modelling approaches,
for example, the invasion of the ECM by self-generated gradients and the
appearance of EC invasion islands outside of the main body of the tumour.
"
Vulnerability to pandemics in a rapidly urbanizing society,"  We examine salient trends of influenza pandemics in Australia, a rapidly
urbanizing nation. To do so, we implement state-of-the-art influenza
transmission and progression models within a large-scale stochastic computer
simulation, generated using comprehensive Australian census datasets from 2006,
2011, and 2016. Our results offer the first simulation-based investigation of a
population's sensitivity to pandemics across multiple historical time points,
and highlight three significant trends in pandemic patterns over the years:
increased peak prevalence, faster spreading rates, and decreasing
spatiotemporal bimodality. We attribute these pandemic trends to increases in
two key quantities indicative of urbanization: population fraction residing in
major cities, and international air traffic. In addition, we identify features
of the pandemic's geographic spread that can only be attributed to changes in
the commuter mobility network. The generic nature of our model and the ubiquity
of urbanization trends around the world make it likely for our results to be
applicable in other rapidly urbanizing nations.
"
Theoretical aspects of microscale acoustofluidics,"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
"
Measuring Integrated Information: Comparison of Candidate Measures in Theory and Simulation,"  Integrated Information Theory (IIT) is a prominent theory of consciousness
that has at its centre measures that quantify the extent to which a system
generates more information than the sum of its parts. While several candidate
measures of integrated information (`$\Phi$') now exist, little is known about
how they compare, especially in terms of their behaviour on non-trivial network
models. In this article we provide clear and intuitive descriptions of six
distinct candidate measures. We then explore the properties of each of these
measures in simulation on networks consisting of eight interacting nodes,
animated with Gaussian linear autoregressive dynamics. We find a striking
diversity in the behaviour of these measures -- no two measures show consistent
agreement across all analyses. Further, only a subset of the measures appear to
genuinely reflect some form of dynamical complexity, in the sense of
simultaneous segregation and integration between system components. Our results
help guide the operationalisation of IIT and advance the development of
measures of integrated information that may have more general applicability.
"
Epidemiological impact of waning immunization on a vaccinated population,"  This is an epidemiological SIRV model based study that is designed to analyze
the impact of vaccination in containing infection spread, in a 4-tiered
population compartment comprised of susceptible, infected, recovered and
vaccinated agents. While many models assume a lifelong protection through
vaccination, we focus on the impact of waning immunization due to conversion of
vaccinated and recovered agents back to susceptible ones. Two asymptotic states
exist, the ""disease-free equilibrium"" and the ""endemic equilibrium""; we express
the transitions between these states as function of the vaccination and
conversion rates using the basic reproduction number as a descriptor. We find
that the vaccination of newborns and adults have different consequences in
controlling epidemics. We also find that a decaying disease protection within
the recovered sub-population is not sufficient to trigger an epidemic at the
linear level. Our simulations focus on parameter sets that could model a
disease with waning immunization like pertussis. For a diffusively coupled
population, a transition to the endemic state can be initiated via the
propagation of a traveling infection wave, described successfully within a
Fisher-Kolmogorov framework.
"
Nonautonomous Dynamics of Acute Cell Injury,"  Clinically-relevant forms of acute cell injury, which include stroke and
myocardial infarction, have been of long-lasting challenge in terms of
successful intervention and treatments. Although laboratory studies have shown
it is possible to decrease cell death after such injuries, human clinical
trials based on laboratory therapies have generally failed. We suggested these
failures are due, at least partially, to the lack of a quantitative theoretical
framework for acute cell injury. Here we provide a systematic study on a
nonlinear dynamical model of acute cell injury and characterize the global
dynamics of a nonautonomous version of the theory. The nonautonomous model
gives rise to four qualitative types of dynamical patterns that can be mapped
to the behavior of cells after clinical acute injuries. In addition, the
concept of a maximum total intrinsic stress response, $S_{max}^*$, emerges from
the nonautonomous theory. A continuous transition across the four qualitative
patterns has been observed, which sets a natural range for initial conditions.
Under these initial conditions in the parameter space tested, the total induced
stress response can be increased to 2.5-11 folds of $S_{max}^*$. This result
indicates that cells possess a reserve stress response capacity which provides
a theoretical explanation of how therapies can prevent cell death after lethal
injuries. This nonautonomous theory of acute cell injury thus provides a
quantitative framework for understanding cell death and recovery and developing
effective therapeutics for acute injury.
"
Transferable neural networks for enhanced sampling of protein dynamics,"  Variational auto-encoder frameworks have demonstrated success in reducing
complex nonlinear dynamics in molecular simulation to a single non-linear
embedding. In this work, we illustrate how this non-linear latent embedding can
be used as a collective variable for enhanced sampling, and present a simple
modification that allows us to rapidly perform sampling in multiple related
systems. We first demonstrate our method is able to describe the effects of
force field changes in capped alanine dipeptide after learning a model using
AMBER99. We further provide a simple extension to variational dynamics encoders
that allows the model to be trained in a more efficient manner on larger
systems by encoding the outputs of a linear transformation using time-structure
based independent component analysis (tICA). Using this technique, we show how
such a model trained for one protein, the WW domain, can efficiently be
transferred to perform enhanced sampling on a related mutant protein, the GTT
mutation. This method shows promise for its ability to rapidly sample related
systems using a single transferable collective variable and is generally
applicable to sets of related simulations, enabling us to probe the effects of
variation in increasingly large systems of biophysical interest.
"
Convolutional Neural Networks In Classifying Cancer Through DNA Methylation,"  DNA Methylation has been the most extensively studied epigenetic mark.
Usually a change in the genotype, DNA sequence, leads to a change in the
phenotype, observable characteristics of the individual. But DNA methylation,
which happens in the context of CpG (cytosine and guanine bases linked by
phosphate backbone) dinucleotides, does not lead to a change in the original
DNA sequence but has the potential to change the phenotype. DNA methylation is
implicated in various biological processes and diseases including cancer. Hence
there is a strong interest in understanding the DNA methylation patterns across
various epigenetic related ailments in order to distinguish and diagnose the
type of disease in its early stages. In this work, the relationship between
methylated versus unmethylated CpG regions and cancer types is explored using
Convolutional Neural Networks (CNNs). A CNN based Deep Learning model that can
classify the cancer of a new DNA methylation profile based on the learning from
publicly available DNA methylation datasets is then proposed.
"
Identification of Key Proteins Involved in Axon Guidance Related Disorders: A Systems Biology Approach,"  Axon guidance is a crucial process for growth of the central and peripheral
nervous systems. In this study, 3 axon guidance related disorders, namely-
Duane Retraction Syndrome (DRS) , Horizontal Gaze Palsy with Progressive
Scoliosis (HGPPS) and Congenital fibrosis of the extraocular muscles type 3
(CFEOM3) were studied using various Systems Biology tools to identify the genes
and proteins involved with them to get a better idea about the underlying
molecular mechanisms including the regulatory mechanisms. Based on the analyses
carried out, 7 significant modules have been identified from the PPI network.
Five pathways/processes have been found to be significantly associated with
DRS, HGPPS and CFEOM3 associated genes. From the PPI network, 3 have been
identified as hub proteins- DRD2, UBC and CUL3.
"
Nonlinear demixed component analysis for neural population data as a low-rank kernel regression problem,"  Here I introduce an extension to demixed principal component analysis (dPCA),
a linear dimensionality reduction technique for analyzing the activity of
neural populations, to the case of nonlinear dimensions. This is accomplished
using kernel methods, resulting in kernel demixed principal component analysis
(kdPCA). This extension resembles kernel-based extensions to standard principal
component analysis and canonical correlation analysis. kdPCA includes dPCA as a
special case when the kernel is linear. I present examples of simulated neural
activity that follows different low dimensional configurations and compare the
results of kdPCA to dPCA. These simulations demonstrate that nonlinear
interactions can impede the ability of dPCA to demix neural activity
corresponding to experimental parameters, but kdPCA can still recover
interpretable components. Additionally, I compare kdPCA and dPCA to a neural
population from rat orbitofrontal cortex during an odor classification task in
recovering decision-related activity.
"
Continual Lifelong Learning with Neural Networks: A Review,"  Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to the long-term memory consolidation and
retrieval without catastrophic forgetting. Consequently, lifelong learning
capabilities are crucial for autonomous agents interacting in the real world
and processing continuous streams of information. However, lifelong learning
remains a long-standing challenge for machine learning and neural network
models since the continual acquisition of incrementally available information
from non-stationary data distributions generally leads to catastrophic
forgetting or interference. This limitation represents a major drawback for
state-of-the-art deep neural network models that typically learn
representations from stationary batches of training data, thus without
accounting for situations in which information becomes incrementally available
over time. In this review, we critically summarize the main challenges linked
to lifelong learning for artificial learning systems and compare existing
neural network approaches that alleviate, to different extents, catastrophic
forgetting. We discuss well-established and emerging research motivated by
lifelong learning factors in biological systems such as structural plasticity,
memory replay, curriculum and transfer learning, intrinsic motivation, and
multisensory integration.
"
Improving inference of the dynamic biological network underlying aging via network propagation,"  Gene expression (GE) data capture valuable condition-specific information
(""condition"" can mean a biological process, disease stage, age, patient, etc.)
However, GE analyses ignore physical interactions between gene products, i.e.,
proteins. Since proteins function by interacting with each other, and since
biological networks (BNs) capture these interactions, BN analyses are
promising. However, current BN data fail to capture condition-specific
information. Recently, GE and BN data have been integrated using network
propagation (NP) to infer condition-specific BNs. However, existing NP-based
studies result in a static condition-specific network, even though cellular
processes are dynamic. A dynamic process of our interest is aging. We use
prominent existing NP methods in a new task of inferring a dynamic rather than
static condition-specific (aging-related) network. Then, we study evolution of
network structure with age - we identify proteins whose network positions
significantly change with age and predict them as new aging-related candidates.
We validate the predictions via e.g., functional enrichment analyses and
literature search. Dynamic network inference via NP yields higher prediction
quality than the only existing method for inferring a dynamic aging-related BN,
which does not use NP.
"
Machine learning in protein engineering,"  Machine learning-guided protein engineering is a new paradigm that enables
the optimization of complex protein functions. Machine-learning methods use
data to predict protein function without requiring a detailed model of the
underlying physics or biological pathways. They accelerate protein engineering
by learning from information contained in all measured variants and using it to
select variants that are likely to be improved. In this review, we introduce
the steps required to collect protein data, train machine-learning models, and
use trained models to guide engineering. We make recommendations at each stage
and look to future opportunities for machine learning to enable the discovery
of new protein functions and uncover the relationship between protein sequence
and function.
"
Why a Population Genetics Framework is Inappropriate for Cultural Evolution,"  Although Darwinian models are rampant in the social sciences, social
scientists do not face the problem that motivated Darwin's theory of natural
selection: the problem of explaining how lineages evolve despite that any
traits they acquire are regularly discarded at the end of the lifetime of the
individuals that acquired them. While the rationale for framing culture as an
evolutionary process is correct, it does not follow that culture is a Darwinian
or selectionist process, or that population genetics and phylogenetics provide
viable starting points for modeling cultural change. This paper lays out
step-by-step arguments as to why this approach is ill-conceived, focusing on
the lack of randomness and lack of a self-assembly code in cultural evolution,
and summarizes an alternative approach.
"
Optimizing deep video representation to match brain activity,"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
"
Cell Identity Codes: Understanding Cell Identity from Gene Expression Profiles using Deep Neural Networks,"  Understanding cell identity is an important task in many biomedical areas.
Expression patterns of specific marker genes have been used to characterize
some limited cell types, but exclusive markers are not available for many cell
types. A second approach is to use machine learning to discriminate cell types
based on the whole gene expression profiles (GEPs). The accuracies of simple
classification algorithms such as linear discriminators or support vector
machines are limited due to the complexity of biological systems. We used deep
neural networks to analyze 1040 GEPs from 16 different human tissues and cell
types. After comparing different architectures, we identified a specific
structure of deep autoencoders that can encode a GEP into a vector of 30
numeric values, which we call the cell identity code (CIC). The original GEP
can be reproduced from the CIC with an accuracy comparable to technical
replicates of the same experiment. Although we use an unsupervised approach to
train the autoencoder, we show different values of the CIC are connected to
different biological aspects of the cell, such as different pathways or
biological processes. This network can use CIC to reproduce the GEP of the cell
types it has never seen during the training. It also can resist some noise in
the measurement of the GEP. Furthermore, we introduce classifier autoencoder,
an architecture that can accurately identify cell type based on the GEP or the
CIC.
"
On the origin of super-diffusive behavior in a class of non-equilibrium systems,"  Experiments and simulations have established that dynamics in a class of
living and abiotic systems that are far from equilibrium exhibit super
diffusive behavior at long times, which in some cases (for example evolving
tumor) is preceded by slow glass-like dynamics. By using the evolution of a
collection of tumor cells, driven by mechanical forces and subject to cell
birth and apoptosis, as a case study we show theoretically that on short time
scales the mean square displacement is sub-diffusive due to jamming, whereas at
long times it is super diffusive. The results obtained using stochastic
quantization method, which is needed because of the absence of
fluctuation-dissipation theorem (FDT), show that the super-diffusive behavior
is universal and impervious to the nature of cell-cell interactions.
Surprisingly, the theory also quantitatively accounts for the non-trivial
dynamics observed in simulations of a model soap foam characterized by creation
and destruction of spherical bubbles, which suggests that the two
non-equilibrium systems belong to the same universality class. The theoretical
prediction for the super diffusion exponent is in excellent agreement with
simulations for collective motion of tumor cells and dynamics associated with
soap bubbles.
"
Identification of a complete YPT1 Rab GTPase sequence from the fungal pathogen Colletotrichum incanum,"  Colletotrichum represent a genus of fungal species primarily known as plant
pathogens with severe economic impacts in temperate, subtropical and tropical
climates Consensus taxonomy and classification systems for Colletotrichum
species have been undergoing revision as high resolution genomic data becomes
available. Here we propose an alternative annotation that provides a complete
sequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun
sequence of Colletotrichum incanum isolated from soybean crops in Illinois,
USA.
"
"Social Innovation and the Evolution of Creative, Sustainable Worldviews","  The ideas that we forge creatively as individuals and groups build on one
another in a manner that is cumulative and adaptive, forming open-ended
lineages across space and time. Thus, human culture is believed to evolve. The
pervasiveness of cross-domain creativity--as when a song inspires a
painting--would appear indicative of discontinuities in cultural lineages.
However, if what evolves through culture is our worldviews--the webs of
thoughts, ideas, and attitudes that constitutes our way of seeing being in the
world--then the problem of discontinuities is solved. The state of a worldview
can be affected by information assimilated in one domain, and this
change-of-state can be expressed in another domain. In this view, the gesture,
narrative, or artifact that constitutes a specific creative act is not what is
evolving; it is merely the external manifestation of the state of an evolving
worldview. Like any evolutionary process, cultural evolution requires a balance
between novelty, via the generation of variation, and continuity, via the
preservation of variants that are adaptive. In cultural evolution, novelty is
generated through creativity, and continuity is provided by social learning
processes, e.g., imitation. Both the generative and imitative aspects of
cultural evolution are affected by social media. We discuss the trajectory from
social ideation to social innovation, focusing on the role of
self-organization, renewal, and perspective-taking at the individual and social
group level.
"
Local reservoir model for choice-based learning,"  Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
"
Dose finding for new vaccines: the role for immunostimulation/immunodynamic modelling,"  Current methods to optimize vaccine dose are purely empirically based,
whereas in the drug development field, dosing determinations use far more
advanced quantitative methodology to accelerate decision-making. Applying these
established methods in the field of vaccine development may reduce the
currently large clinical trial sample sizes, long time frames, high costs, and
ultimately have a better potential to save lives. We propose the field of
immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate
mathematical frameworks used for drug dosing towards optimizing vaccine dose
decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply
mathematical models to describe the underlying mechanisms by which the immune
response is stimulated by vaccination (IS) and the resulting measured immune
response dynamics (ID). To move IS/ID modelling forward, existing datasets and
further data on vaccine allometry and dose-dependent dynamics need to be
generated and collate, requiring a collaborative environment with input from
academia, industry, regulators, governmental and non-governmental agencies to
share modelling expertise, and connect modellers to vaccine data.
"
Network topology of neural systems supporting avalanche dynamics predicts stimulus propagation and recovery,"  Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
"
Drug response prediction by ensemble learning and drug-induced gene expression signatures,"  Chemotherapeutic response of cancer cells to a given compound is one of the
most fundamental information one requires to design anti-cancer drugs. Recent
advances in producing large drug screens against cancer cell lines provided an
opportunity to apply machine learning methods for this purpose. In addition to
cytotoxicity databases, considerable amount of drug-induced gene expression
data has also become publicly available. Following this, several methods that
exploit omics data were proposed to predict drug activity on cancer cells.
However, due to the complexity of cancer drug mechanisms, none of the existing
methods are perfect. One possible direction, therefore, is to combine the
strengths of both the methods and the databases for improved performance. We
demonstrate that integrating a large number of predictions by the proposed
method improves the performance for this task. The predictors in the ensemble
differ in several aspects such as the method itself, the number of tasks method
considers (multi-task vs. single-task) and the subset of data considered
(sub-sampling). We show that all these different aspects contribute to the
success of the final ensemble. In addition, we attempt to use the drug screen
data together with two novel signatures produced from the drug-induced gene
expression profiles of cancer cell lines. Finally, we evaluate the method
predictions by in vitro experiments in addition to the tests on data sets.The
predictions of the methods, the signatures and the software are available from
\url{this http URL}.
"
Multistationarity and Bistability for Fewnomial Chemical Reaction Networks,"  Bistability and multistationarity are properties of reaction networks linked
to switch-like responses and connected to cell memory and cell decision making.
Determining whether and when a network exhibits bistability is a hard and open
mathematical problem. One successful strategy consists of analyzing small
networks and deducing that some of the properties are preserved upon passage to
the full network. Motivated by this we study chemical reaction networks with
few chemical complexes. Under mass-action kinetics the steady states of these
networks are described by fewnomial systems, that is polynomial systems having
few distinct monomials. Such systems of polynomials are often studied in real
algebraic geometry by the use of Gale dual systems. Using this Gale duality we
give precise conditions in terms of the reaction rate constants for the number
and stability of the steady states of families of reaction networks with one
non-flow reaction.
"
Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification,"  Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fréchet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
"
Analog control with two Artificial Axons,"  The artificial axon is a recently introduced synthetic assembly of supported
lipid bilayers and voltage gated ion channels, displaying the basic
electrophysiology of nerve cells. Here we demonstrate the use of two artificial
axons as control elements to achieve a simple task. Namely, we steer a remote
control car towards a light source, using the sensory input dependent firing
rate of the axons as the control signal for turning left or right. We present
the result in the form of the analysis of a movie of the car approaching the
light source. In general terms, with this work we pursue a constructivist
approach to exploring the nexus between machine language at the nerve cell
level and behavior.
"
Time pressure and honesty in a deception game,"  Previous experiments have found mixed results on whether honesty is intuitive
or requires deliberation. Here we add to this literature by building on prior
work of Capraro (2017). We report a large study (N=1,389) manipulating time
pressure vs time delay in a deception game. We find that, in this setting,
people are more honest under time pressure, and that this result is not driven
by confounds present in earlier work.
"
Computation of life expectancy from incomplete data,"  Estimating the human longevity and computing of life expectancy are central
to the population dynamics. These aspects were studied seriously by scientists
since fifteenth century, including renowned astronomer Edmund Halley. From
basic principles of population dynamics, we propose a method to compute life
expectancy from incomplete data.
"
Attentive cross-modal paratope prediction,"  Antibodies are a critical part of the immune system, having the function of
directly neutralising or tagging undesirable objects (the antigens) for future
destruction. Being able to predict which amino acids belong to the paratope,
the region on the antibody which binds to the antigen, can facilitate antibody
design and contribute to the development of personalised medicine. The
suitability of deep neural networks has recently been confirmed for this task,
with Parapred outperforming all prior physical models. Our contribution is
twofold: first, we significantly outperform the computational efficiency of
Parapred by leveraging à trous convolutions and self-attention. Secondly, we
implement cross-modal attention by allowing the antibody residues to attend
over antigen residues. This leads to new state-of-the-art results on this task,
along with insightful interpretations.
"
Brain networks reveal the effects of antipsychotic drugs on schizophrenia patients and controls,"  The study of brain networks, including derived from functional neuroimaging
data, attracts broad interest and represents a rapidly growing
interdisciplinary field. Comparing networks of healthy volunteers with those of
patients can potentially offer new, quantitative diagnostic methods, and a
framework for better understanding brain and mind disorders. We explore resting
state fMRI data through network measures, and demonstrate that not only is
there a distinctive network architecture in the healthy brain that is disrupted
in schizophrenia, but also that both networks respond to medication. We
construct networks representing 15 healthy individuals and 12 schizophrenia
patients (males and females), all of whom are administered three drug
treatments: (i) a placebo; and two antipsychotic medications (ii) aripiprazole
and; (iii) sulpiride. We first reproduce the established finding that brain
networks of schizophrenia patients exhibit increased efficiency and reduced
clustering compared to controls. Our data then reveals that the antipsychotic
medications mitigate this effect, shifting the metrics towards those observed
in healthy volunteers, with a marked difference in efficacy between the two
drugs. Additionally, we find that aripiprazole considerably alters the network
statistics of healthy controls. Using a test of cognitive ability, we establish
that aripiprazole also adversely affects their performance. This provides
evidence that changes to macroscopic brain network architecture result in
measurable behavioural differences. This is the first time different
medications have been assessed in this way. Our results lay the groundwork for
an objective methodology with which to calculate and compare the efficacy of
different treatments of mind and brain disorders.
"
"$S$-Leaping: An adaptive, accelerated stochastic simulation algorithm, bridging $τ$-leaping and $R$-leaping","  We propose the $S$-leaping algorithm for the acceleration of Gillespie's
stochastic simulation algorithm that combines the advantages of the two main
accelerated methods; the $\tau$-leaping and $R$-leaping algorithms. These
algorithms are known to be efficient under different conditions; the
$\tau$-leaping is efficient for non-stiff systems or systems with partial
equilibrium, while the $R$-leaping performs better in stiff system thanks to an
efficient sampling procedure. However, even a small change in a system's set up
can critically affect the nature of the simulated system and thus reduce the
efficiency of an accelerated algorithm. The proposed algorithm combines the
efficient time step selection from the $\tau$-leaping with the effective
sampling procedure from the $R$-leaping algorithm. The $S$-leaping is shown to
maintain its efficiency under different conditions and in the case of large and
stiff systems or systems with fast dynamics, the $S$-leaping outperforms both
methods. We demonstrate the performance and the accuracy of the $S$-leaping in
comparison with the $\tau$-leaping and $R$-leaping on a number of benchmark
systems involving biological reaction networks.
"
A multiple attribute model resolves a conflict between additive and multiplicative models of incentive salience,"  A model of incentive salience as a function of stimulus value and
interoceptive state has been previously proposed. In that model, the function
differs depending on whether the stimulus is appetitive or aversive; it is
multiplicative for appetitive stimuli and additive for aversive stimuli. The
authors argued it was necessary to capture data on how extreme changes in salt
appetite could move evaluation of an extreme salt solution from negative to
positive. We demonstrate that arbitrarily varying this function is unnecessary,
and that a multiplicative function is sufficient if one assumes the incentive
salience function for an incentive (such as salt) is comprised of multiple
stimulus features and multiple interoceptive signals. We show that it is also
unnecessary considering the dual-structure approach-aversive nature of the
reward system, which results in separate weighting of appetitive and aversive
stimulus features.
"
System Identification of a Multi-timescale Adaptive Threshold Neuronal Model,"  In this paper, the parameter estimation problem for a multi-timescale
adaptive threshold (MAT) neuronal model is investigated. By manipulating the
system dynamics, which comprise of a non-resetting leaky integrator coupled
with an adaptive threshold, the threshold voltage can be obtained as a
realizable model that is linear in the unknown parameters. This linearly
parametrized realizable model is then utilized inside a prediction error based
framework to identify the threshold parameters with the purpose of predicting
single neuron precise firing times. The iterative linear least squares
estimation scheme is evaluated using both synthetic data obtained from an exact
model as well as experimental data obtained from in vitro rat somatosensory
cortical neurons. Results show the ability of this approach to fit the MAT
model to different types of fluctuating reference data. The performance of the
proposed approach is seen to be superior when comparing with existing
identification approaches used by the neuronal community.
"
"Refractive index measurements of single, spherical cells using digital holographic microscopy","  In this chapter, we introduce digital holographic microscopy (DHM) as a
marker-free method to determine the refractive index of single, spherical cells
in suspension. The refractive index is a conclusive measure in a biological
context. Cell conditions, such as differentiation or infection, are known to
yield significant changes in the refractive index. Furthermore, the refractive
index of biological tissue determines the way it interacts with light. Besides
the biological relevance of this interaction in the retina, a lot of methods
used in biology, including microscopy, rely on light-tissue or light-cell
interactions. Hence, determining the refractive index of cells using DHM is
valuable in many biological applications. This chapter covers the main topics
which are important for the implementation of DHM: setup, sample preparation
and analysis. First, the optical setup is described in detail including notes
and suggestions for the implementation. Following that, a protocol for the
sample and measurement preparation is explained. In the analysis section, an
algorithm for the determination of the quantitative phase map is described.
Subsequently, all intermediate steps for the calculation of the refractive
index of suspended cells are presented, exploiting their spherical shape. In
the last section, a discussion of possible extensions to the setup, further
measurement configurations and additional analysis methods are given.
Throughout this chapter, we describe a simple, robust, and thus easily
reproducible implementation of DHM. The different possibilities for extensions
show the diverse fields of application for this technique.
"
Optimizing Channel Selection for Seizure Detection,"  Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.
"
Connectivity-Driven Brain Parcellation via Consensus Clustering,"  We present two related methods for deriving connectivity-based brain atlases
from individual connectomes. The proposed methods exploit a previously proposed
dense connectivity representation, termed continuous connectivity, by first
performing graph-based hierarchical clustering of individual brains, and
subsequently aggregating the individual parcellations into a consensus
parcellation. The search for consensus minimizes the sum of cluster membership
distances, effectively estimating a pseudo-Karcher mean of individual
parcellations. We assess the quality of our parcellations using (1)
Kullback-Liebler and Jensen-Shannon divergence with respect to the dense
connectome representation, (2) inter-hemispheric symmetry, and (3) performance
of the simplified connectome in a biological sex classification task. We find
that the parcellation based-atlas computed using a greedy search at a
hierarchical depth 3 outperforms all other parcellation-based atlases as well
as the standard Dessikan-Killiany anatomical atlas in all three assessments.
"
Fiber plucking by molecular motors yields large emergent contractility in stiff biopolymer networks,"  The mechanical properties of the cell depend crucially on the tension of its
cytoskeleton, a biopolymer network that is put under stress by active motor
proteins. While the fibrous nature of the network is known to strongly affect
the transmission of these forces to the cellular scale, our understanding of
this process remains incomplete. Here we investigate the transmission of forces
through the network at the individual filament level, and show that active
forces can be geometrically amplified as a transverse motor-generated force
force ""plucks"" the fiber and induces a nonlinear tension. In stiff and densely
connnected networks, this tension results in large network-wide tensile
stresses that far exceed the expectation drawn from a linear elastic theory.
This amplification mechanism competes with a recently characterized
network-level amplification due to fiber buckling, suggesting that that fiber
networks provide several distinct pathways for living systems to amplify their
molecular forces.
"
Templated ligation can create a hypercycle replication network,"  The stability of sequence replication was crucial for the emergence of
molecular evolution and early life. Exponential replication with a first-order
growth dynamics show inherent instabilities such as the error catastrophe and
the dominance by the fastest replicators. This favors less structured and short
sequences. The theoretical concept of hypercycles has been proposed to solve
these problems. Their higher-order growth kinetics leads to frequency-dependent
selection and stabilizes the replication of majority molecules. However, many
implementations of hypercycles are unstable or require special sequences with
catalytic activity. Here, we demonstrate the spontaneous emergence of
higher-order cooperative replication from a network of simple ligation chain
reactions (LCR). We performed long-term LCR experiments from a mixture of
sequences under molecule degrading conditions with a ligase protein. At the
chosen temperature cycling, a network of positive feedback loops arose from
both the cooperative ligation of matching sequences and the emerging increase
in sequence length. It generated higher-order replication with
frequency-dependent selection. The experiments matched a complete simulation
using experimentally determined ligation rates and the hypercycle mechanism was
also confirmed by abstracted modeling. Since templated ligation is a most basic
reaction of oligonucleotides, the described mechanism could have been
implemented under microthermal convection on early Earth.
"
The common patterns of abundance: the log series and Zipf's law,"  In a language corpus, the probability that a word occurs $n$ times is often
proportional to $1/n^2$. Assigning rank, $s$, to words according to their
abundance, $\log s$ vs $\log n$ typically has a slope of minus one. That simple
Zipf's law pattern also arises in the population sizes of cities, the sizes of
corporations, and other patterns of abundance. By contrast, for the abundances
of different biological species, the probability of a population of size $n$ is
typically proportional to $1/n$, declining exponentially for larger $n$, the
log series pattern. This article shows that the differing patterns of Zipf's
law and the log series arise as the opposing endpoints of a more general
theory. The general theory follows from the generic form of all probability
patterns as a consequence of conserved average values and the associated
invariances of scale. To understand the common patterns of abundance, the
generic form of probability distributions plus the conserved average abundance
is sufficient. The general theory includes cases that are between the Zipf and
log series endpoints, providing a broad framework for analyzing widely observed
abundance patterns.
"
A Bayesian Approach for Inferring Local Causal Structure in Gene Regulatory Networks,"  Gene regulatory networks play a crucial role in controlling an organism's
biological processes, which is why there is significant interest in developing
computational methods that are able to extract their structure from
high-throughput genetic data. A typical approach consists of a series of
conditional independence tests on the covariance structure meant to
progressively reduce the space of possible causal models. We propose a novel
efficient Bayesian method for discovering the local causal relationships among
triplets of (normally distributed) variables. In our approach, we score the
patterns in the covariance matrix in one go and we incorporate the available
background knowledge in the form of priors over causal structures. Our method
is flexible in the sense that it allows for different types of causal
structures and assumptions. We apply the approach to the task of inferring gene
regulatory networks by learning regulatory relationships between gene
expression levels. We show that our algorithm produces stable and conservative
posterior probability estimates over local causal structures that can be used
to derive an honest ranking of the most meaningful regulatory relationships. We
demonstrate the stability and efficacy of our method both on simulated data and
on real-world data from an experiment on yeast.
"
DNA Base Pair Mismatches Induce Structural Changes and Alter the Free Energy Landscape of Base Flip,"  Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick
pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse
consequences for human health. We utilize molecular dynamics and metadynamics
computer simulations to study the equilibrium structure and dynamics for both
matched and mismatched base pairs. We discover significant differences between
matched and mismatched pairs in structure, hydrogen bonding, and base flip work
profiles. Mismatched pairs shift further in the plane normal to the DNA strand
and are more likely to exhibit non-canonical structures, including the e-motif.
We discuss potential implications on mismatch repair enzymes' detection of DNA
mismatches.
"
Comparing multiple networks using the Co-expression Differential Network Analysis (CoDiNA),"  Biomedical sciences are increasingly recognising the relevance of gene
co-expression-networks for analysing complex-systems, phenotypes or diseases.
When the goal is investigating complex-phenotypes under varying conditions, it
comes naturally to employ comparative network methods. While approaches for
comparing two networks exist, this is not the case for multiple networks. Here
we present a method for the systematic comparison of an unlimited number of
networks: Co-expression Differential Network Analysis (CoDiNA) for detecting
links and nodes that are common, specific or different to the networks.
Applying CoDiNA to a neurogenesis study identified genes for neuron
differentiation. Experimentally overexpressing one candidate resulted in
significant disturbance in the underlying neurogenesis' gene regulatory
network. We compared data from adults and children with active tuberculosis to
test for signatures of HIV. We also identified common and distinct network
features for particular cancer types with CoDiNA. These studies show that
CoDiNA successfully detects genes associated with the diseases.
"
Rapid behavioral transitions produce chaotic mixing by a planktonic microswimmer,"  Despite their vast morphological diversity, many invertebrates have similar
larval forms characterized by ciliary bands, innervated arrays of beating cilia
that facilitate swimming and feeding. Hydrodynamics suggests that these bands
should tightly constrain the behavioral strategies available to the larvae;
however, their apparent ubiquity suggests that these bands also confer
substantial adaptive advantages. Here, we use hydrodynamic techniques to
investigate ""blinking,"" an unusual behavioral phenomenon observed in many
invertebrate larvae in which ciliary bands across the body rapidly change
beating direction and produce transient rearrangement of the local flow field.
Using a general theoretical model combined with quantitative experiments on
starfish larvae, we find that the natural rhythm of larval blinking is
hydrodynamically optimal for inducing strong mixing of the local fluid
environment due to transient streamline crossing, thereby maximizing the
larvae's overall feeding rate. Our results are consistent with previous
hypotheses that filter feeding organisms may use chaotic mixing dynamics to
overcome circulation constraints in viscous environments, and it suggests
physical underpinnings for complex neurally-driven behaviors in early-divergent
animals.
"
A pathway-based kernel boosting method for sample classification using genomic data,"  The analysis of cancer genomic data has long suffered ""the curse of
dimensionality"". Sample sizes for most cancer genomic studies are a few
hundreds at most while there are tens of thousands of genomic features studied.
Various methods have been proposed to leverage prior biological knowledge, such
as pathways, to more effectively analyze cancer genomic data. Most of the
methods focus on testing marginal significance of the associations between
pathways and clinical phenotypes. They can identify relevant pathways, but do
not involve predictive modeling. In this article, we propose a Pathway-based
Kernel Boosting (PKB) method for integrating gene pathway information for
sample classification, where we use kernel functions calculated from each
pathway as base learners and learn the weights through iterative optimization
of the classification loss function. We apply PKB and several competing methods
to three cancer studies with pathological and clinical information, including
tumor grade, stage, tumor sites, and metastasis status. Our results show that
PKB outperforms other methods, and identifies pathways relevant to the outcome
variables.
"
"Analysis of evolutionary origins of genomic loci harboring 59,732 candidate human-specific regulatory sequences identifies genetic divergence patterns during evolution of Great Apes","  Our view of the universe of genomic regions harboring various types of
candidate human-specific regulatory sequences (HSRS) has been markedly expanded
in recent years. To infer the evolutionary origins of loci harboring HSRS,
analyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,
Bonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two
major evolutionary pathways have been identified comprising thousands of
sequences that were either inherited from extinct common ancestors (ECAs) or
created de novo in humans after human/chimpanzee split. Thousands of HSRS
appear inherited from ECAs yet bypassed genomes of our closest evolutionary
relatives, presumably due to the incomplete lineage sorting and/or
species-specific loss or regulatory DNA. The bypassing pattern is prominent for
HSRS associated with development and functions of human brain. Common genomic
loci that may contributed to speciation during evolution of Great Apes comprise
248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p
= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for
HSRS associated with human-specific (HS) changes of gene expression in cerebral
organoids. Among non-human primates (NHP), most significant fractions of
candidate HSRS associated with HS expression changes in both excitatory neurons
(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla
genome. Modern Humans acquired unique combinations of regulatory sequences
highly conserved in distinct species of six NHP separated by 30 million years
of evolution. Concurrently, this unique mosaic of regulatory sequences
inherited from ECAs was supplemented with 12,486 created de novo HSRS. These
observations support the model of complex continuous speciation process during
evolution of Great Apes that is not likely to occur as an instantaneous event.
"
Conformational dynamics of a single protein monitored for 24 hours at video rate,"  We use plasmon rulers to follow the conformational dynamics of a single
protein for up to 24 h at a video rate. The plasmon ruler consists of two gold
nanospheres connected by a single protein linker. In our experiment, we follow
the dynamics of the molecular chaperone heat shock protein 90, which is known
to show open and closed conformations. Our measurements confirm the previously
known conformational dynamics with transition times in the second to minute
time scale and reveals new dynamics on the time scale of minutes to hours.
Plasmon rulers thus extend the observation bandwidth 3/4 orders of magnitude
with respect to single-molecule fluorescence resonance energy transfer and
enable the study of molecular dynamics with unprecedented precision.
"
What deep learning can tell us about higher cognitive functions like mindreading?,"  Can deep learning (DL) guide our understanding of computations happening in
biological brain? We will first briefly consider how DL has contributed to the
research on visual object recognition. In the main part we will assess whether
DL could also help us to clarify the computations underlying higher cognitive
functions such as Theory of Mind. In addition, we will compare the objectives
and learning signals of brains and machines, leading us to conclude that simply
scaling up the current DL algorithms will not lead to human level mindreading
skills. We then provide some insights about how to fairly compare human and DL
performance. In the end we find that DL can contribute to our understanding of
biological computations by providing an example of an end-to-end algorithm that
solves the same problems the biological agents face.
"
"Comments on the National Toxicology Program Report on Cancer, Rats and Cell Phone Radiation","  With the National Toxicology Program issuing its final report on cancer, rats
and cell phone radiation, one can draw the following conclusions from their
data. There is a roughly linear relationship between gliomas (brain cancers)
and schwannomas (cancers of the nerve sheaths around the heart) with increased
absorption of 900 MHz radiofrequency radiation for male rats. The rate of these
cancers in female rats is about one third the rate in male rats; the rate of
gliomas in female humans is about two thirds the rate in male humans. Both of
these observations can be explained by a decrease in sensitivity to chemical
carcinogenesis in both female rats and female humans. The increase in male rat
life spans with increased radiofrequency absorption is due to a reduction in
kidney failure from a decrease in food intake. No such similar increase in the
life span of humans who use cell phones is expected.
"
An evolutionary game model for behavioral gambit of loyalists: Global awareness and risk-aversion,"  We study the phase diagram of a minority game where three classes of agents
are present. Two types of agents play a risk-loving game that we model by the
standard Snowdrift Game. The behaviour of the third type of agents is coded by
{\em indifference} w.r.t. the game at all: their dynamics is designed to
account for risk-aversion as an innovative behavioral gambit. From this point
of view, the choice of this solitary strategy is enhanced when innovation
starts, while is depressed when it becomes the majority option. This implies
that the payoff matrix of the game becomes dependent on the global awareness of
the agents measured by the relevance of the population of the indifferent
players. The resulting dynamics is non-trivial with different kinds of phase
transition depending on a few model parameters. The phase diagram is studied on
regular as well as complex networks.
"
Theory of circadian metabolism,"  Many organisms repartition their proteome in a circadian fashion in response
to the daily nutrient changes in their environment. A striking example is
provided by cyanobacteria, which perform photosynthesis during the day to fix
carbon. These organisms not only face the challenge of rewiring their proteome
every 12 hours, but also the necessity of storing the fixed carbon in the form
of glycogen to fuel processes during the night. In this manuscript, we extend
the framework developed by Hwa and coworkers (Scott et al., Science 330, 1099
(2010)) for quantifying the relatinship between growth and proteome composition
to circadian metabolism. We then apply this framework to investigate the
circadian metabolism of the cyanobacterium Cyanothece, which not only fixes
carbon during the day, but also nitrogen during the night, storing it in the
polymer cyanophycin. Our analysis reveals that the need to store carbon and
nitrogen tends to generate an extreme growth strategy, in which the cells
predominantly grow during the day, as observed experimentally. This strategy
maximizes the growth rate over 24 hours, and can be quantitatively understood
by the bacterial growth laws. Our analysis also shows that the slow relaxation
of the proteome, arising from the slow growth rate, puts a severe constraint on
implementing this optimal strategy. Yet, the capacity to estimate the time of
the day, enabled by the circadian clock, makes it possible to anticipate the
daily changes in the environment and mount a response ahead of time. This
significantly enhances the growth rate by counteracting the detrimental effects
of the slow proteome relaxation.
"
Machine learning regression on hyperspectral data to estimate multiple water parameters,"  In this paper, we present a regression framework involving several machine
learning models to estimate water parameters based on hyperspectral data.
Measurements from a multi-sensor field campaign, conducted on the River Elbe,
Germany, represent the benchmark dataset. It contains hyperspectral data and
the five water parameters chlorophyll a, green algae, diatoms, CDOM and
turbidity. We apply a PCA for the high-dimensional data as a possible
preprocessing step. Then, we evaluate the performance of the regression
framework with and without this preprocessing step. The regression results of
the framework clearly reveal the potential of estimating water parameters based
on hyperspectral data with machine learning. The proposed framework provides
the basis for further investigations, such as adapting the framework to
estimate water parameters of different inland waters.
"
Non-hermitian operator modelling of basic cancer cell dynamics,"  We propose a dynamical system of tumor cells proliferation based on
operatorial methods. The approach we propose is quantum-like: we use ladder and
number operators to describe healthy and tumor cells birth and death, and the
evolution is ruled by a non-hermitian Hamiltonian which includes, in a non
reversible way, the basic biological mechanisms we consider for the system. We
show that this approach is rather efficient in describing some processes of the
cells. We further add some medical treatment, described by adding a suitable
term in the Hamiltonian, which controls and limits the growth of tumor cells,
and we propose an optimal approach to stop, and reverse, this growth.
"
Are theoretical results 'Results'?,"  Yes.
"
Analysis of bacterial population growth using extended logistic growth model with distributed delay,"  In the present work, we develop a delayed Logistic growth model to study the
effects of decontamination on the bacterial population in the ambient
environment. Using the linear stability analysis, we study different case
scenarios, where bacterial population may establish at the positive equilibrium
or go extinct due to increased decontamination. The results are verified using
numerical simulation of the model.
"
"Sparse distributed representation, hierarchy, critical periods, metaplasticity: the keys to lifelong fixed-time learning and best-match retrieval","  Among the more important hallmarks of human intelligence, which any
artificial general intelligence (AGI) should have, are the following. 1. It
must be capable of on-line learning, including with single/few trials. 2.
Memories/knowledge must be permanent over lifelong durations, safe from
catastrophic forgetting. Some confabulation, i.e., semantically plausible
retrieval errors, may gradually accumulate over time. 3. The time to both: a)
learn a new item, and b) retrieve the best-matching / most relevant item(s),
i.e., do similarity-based retrieval, must remain constant throughout the
lifetime. 4. The system should never become full: it must remain able to store
new information, i.e., make new permanent memories, throughout very long
lifetimes. No artificial computational system has been shown to have all these
properties. Here, we describe a neuromorphic associative memory model, Sparsey,
which does, in principle, possess them all. We cite prior results supporting
possession of hallmarks 1 and 3 and sketch an argument, hinging on strongly
recursive, hierarchical, part-whole compositional structure of natural data,
that Sparsey also possesses hallmarks 2 and 4.
"
Optimal stimulation protocol in a bistable synaptic consolidation model,"  Consolidation of synaptic changes in response to neural activity is thought
to be fundamental for memory maintenance over a timescale of hours. In
experiments, synaptic consolidation can be induced by repeatedly stimulating
presynaptic neurons. However, the effectiveness of such protocols depends
crucially on the repetition frequency of the stimulations and the mechanisms
that cause this complex dependence are unknown. Here we propose a simple
mathematical model that allows us to systematically study the interaction
between the stimulation protocol and synaptic consolidation. We show the
existence of optimal stimulation protocols for our model and, similarly to LTP
experiments, the repetition frequency of the stimulation plays a crucial role
in achieving consolidation. Our results show that the complex dependence of LTP
on the stimulation frequency emerges naturally from a model which satisfies
only minimal bistability requirements.
"
Collective irregular dynamics in balanced networks of leaky integrate-and-fire neurons,"  We extensively explore networks of weakly unbalanced, leaky
integrate-and-fire (LIF) neurons for different coupling strength, connectivity,
and by varying the degree of refractoriness, as well as the delay in the spike
transmission. We find that the neural network does not only exhibit a
microscopic (single-neuron) stochastic-like evolution, but also a collective
irregular dynamics (CID). Our analysis is based on the computation of a
suitable order parameter, typically used to characterize synchronization
phenomena and on a detailed scaling analysis (i.e. simulations of different
network sizes). As a result, we can conclude that CID is a true thermodynamic
phase, intrinsically different from the standard asynchronous regime.
"
Multi-SpaM: a Maximum-Likelihood approach to Phylogeny reconstruction based on Multiple Spaced-Word Matches,"  Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
"
Early warning signals in plant disease outbreaks,"  Summary
1. Infectious disease outbreaks in plants threaten ecosystems, agricultural
crops and food trade. Currently, several fungal diseases are affecting forests
worldwide, posing a major risk to tree species, habitats and consequently
ecosystem decay. Prediction and control of disease spread are difficult, mainly
due to the complexity of the interaction between individual components
involved.
2. In this work, we introduce a lattice-based epidemic model coupled with a
stochastic process that mimics, in a very simplified way, the interaction
between the hosts and pathogen. We studied the disease spread by measuring the
propagation velocity of the pathogen on the susceptible hosts. Quantitative
results indicate the occurrence of a critical transition between two stable
phases: local confinement and an extended epiphytotic outbreak that depends on
the density of the susceptible individuals.
3. Quantitative predictions of epiphytotics are performed using the framework
early-warning indicators for impending regime shifts, widely applied on
dynamical systems. These signals forecast successfully the outcome of the
critical shift between the two stable phases before the system enters the
epiphytotic regime.
4. Synthesis: Our study demonstrates that early-warning indicators could be
useful for the prediction of forest disease epidemics through mathematical and
computational models suited to more specific pathogen-host-environmental
interactions.
"
Homeostatic plasticity and external input shape neural network dynamics,"  In vitro and in vivo spiking activity clearly differ. Whereas networks in
vitro develop strong bursts separated by periods of very little spiking
activity, in vivo cortical networks show continuous activity. This is puzzling
considering that both networks presumably share similar single-neuron dynamics
and plasticity rules. We propose that the defining difference between in vitro
and in vivo dynamics is the strength of external input. In vitro, networks are
virtually isolated, whereas in vivo every brain area receives continuous input.
We analyze a model of spiking neurons in which the input strength, mediated by
spike rate homeostasis, determines the characteristics of the dynamical state.
In more detail, our analytical and numerical results on various network
topologies show consistently that under increasing input, homeostatic
plasticity generates distinct dynamic states, from bursting, to
close-to-critical, reverberating and irregular states. This implies that the
dynamic state of a neural network is not fixed but can readily adapt to the
input strengths. Indeed, our results match experimental spike recordings in
vitro and in vivo: the in vitro bursting behavior is consistent with a state
generated by very low network input (< 0.1%), whereas in vivo activity suggests
that on the order of 1% recorded spikes are input-driven, resulting in
reverberating dynamics. Importantly, this predicts that one can abolish the
ubiquitous bursts of in vitro preparations, and instead impose dynamics
comparable to in vivo activity by exposing the system to weak long-term
stimulation, thereby opening new paths to establish an in vivo-like assay in
vitro for basic as well as neurological studies.
"
Measures of Tractography Convergence,"  In the present work, we use information theory to understand the empirical
convergence rate of tractography, a widely-used approach to reconstruct
anatomical fiber pathways in the living brain. Based on diffusion MRI data,
tractography is the starting point for many methods to study brain
connectivity. Of the available methods to perform tractography, most
reconstruct a finite set of streamlines, or 3D curves, representing probable
connections between anatomical regions, yet relatively little is known about
how the sampling of this set of streamlines affects downstream results, and how
exhaustive the sampling should be. Here we provide a method to measure the
information theoretic surprise (self-cross entropy) for tract sampling schema.
We then empirically assess four streamline methods. We demonstrate that the
relative information gain is very low after a moderate number of streamlines
have been generated for each tested method. The results give rise to several
guidelines for optimal sampling in brain connectivity analyses.
"
From synaptic interactions to collective dynamics in random neuronal networks models: critical role of eigenvectors and transient behavior,"  The study of neuronal interactions is currently at the center of several
neuroscience big collaborative projects (including the Human Connectome, the
Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the
entire brain matrix. Under certain constraints, mathematical theory can advance
predictions of the expected neural dynamics based solely on the statistical
properties of such synaptic interaction matrix. This work explores the
application of free random variables (FRV) to the study of large synaptic
interaction matrices. Besides recovering in a straightforward way known results
on eigenspectra of neural networks, we extend them to heavy-tailed
distributions of interactions. More importantly, we derive analytically the
behavior of eigenvector overlaps, which determine stability of the spectra. We
observe that upon imposing the neuronal excitation/inhibition balance, although
the eigenvalues remain unchanged, their stability dramatically decreases due to
strong non-orthogonality of associated eigenvectors. It leads us to the
conclusion that the understanding of the temporal evolution of asymmetric
neural networks requires considering the entangled dynamics of both
eigenvectors and eigenvalues, which might bear consequences for learning and
memory processes in these models. Considering the success of FRV analysis in a
wide variety of branches disciplines, we hope that the results presented here
foster additional application of these ideas in the area of brain sciences.
"
Discrete-attractor-like Tracking in Continuous Attractor Neural Networks,"  Continuous attractor neural networks generate a set of smoothly connected
attractor states. In memory systems of the brain, these attractor states may
represent continuous pieces of information such as spatial locations and head
directions of animals. However, during the replay of previous experiences,
hippocampal neurons show a discontinuous sequence in which discrete transitions
of neural state are phase-locked with the slow-gamma (30-40 Hz) oscillation.
Here, we explored the underlying mechanisms of the discontinuous sequence
generation. We found that a continuous attractor neural network has several
phases depending on the interactions between external input and local
inhibitory feedback. The discrete-attractor-like behavior naturally emerges in
one of these phases without any discreteness assumption. We propose that the
dynamics of continuous attractor neural networks is the key to generate
discontinuous state changes phase-locked to the brain rhythm.
"
Modeling epidemics on d-cliqued graphs,"  Since social interactions have been shown to lead to symmetric clusters, we
propose here that symmetries play a key role in epidemic modeling. Mathematical
models on d-ary tree graphs were recently shown to be particularly effective
for modeling epidemics in simple networks [Seibold & Callender, 2016]. To
account for symmetric relations, we generalize this to a new type of networks
modeled on d-cliqued tree graphs, which are obtained by adding edges to regular
d-trees to form d-cliques. This setting gives a more realistic model for
epidemic outbreaks originating, for example, within a family or classroom and
which could reach a population by transmission via children in schools.
Specifically, we quantify how an infection starting in a clique (e.g. family)
can reach other cliques through the body of the graph (e.g. public places).
Moreover, we propose and study the notion of a safe zone, a subset that has a
negligible probability of infection.
"
Phylogeny-based tumor subclone identification using a Bayesian feature allocation model,"  Tumor cells acquire different genetic alterations during the course of
evolution in cancer patients. As a result of competition and selection, only a
few subgroups of cells with distinct genotypes survive. These subgroups of
cells are often referred to as subclones. In recent years, many statistical and
computational methods have been developed to identify tumor subclones, leading
to biologically significant discoveries and shedding light on tumor
progression, metastasis, drug resistance and other processes. However, most
existing methods are either not able to infer the phylogenetic structure among
subclones, or not able to incorporate copy number variations (CNV). In this
article, we propose SIFA (tumor Subclone Identification by Feature Allocation),
a Bayesian model which takes into account both CNV and tumor phylogeny
structure to infer tumor subclones. We compare the performance of SIFA with two
other commonly used methods using simulation studies with varying sequencing
depth, evolutionary tree size, and tree complexity. SIFA consistently yields
better results in terms of Rand Index and cellularity estimation accuracy. The
usefulness of SIFA is also demonstrated through its application to whole genome
sequencing (WGS) samples from four patients in a breast cancer study.
"
The Dynamics of Norm Change in the Cultural Evolution of Language,"  What happens when a new social convention replaces an old one? While the
possible forces favoring norm change - such as institutions or committed
activists - have been identified since a long time, little is known about how a
population adopts a new convention, due to the difficulties of finding
representative data. Here we address this issue by looking at changes occurred
to 2,541 orthographic and lexical norms in English and Spanish through the
analysis of a large corpora of books published between the years 1800 and 2008.
We detect three markedly distinct patterns in the data, depending on whether
the behavioral change results from the action of a formal institution, an
informal authority or a spontaneous process of unregulated evolution. We
propose a simple evolutionary model able to capture all the observed behaviors
and we show that it reproduces quantitatively the empirical data. This work
identifies general mechanisms of norm change and we anticipate that it will be
of interest to researchers investigating the cultural evolution of language
and, more broadly, human collective behavior.
"
Communication via FRET in Nanonetworks of Mobile Proteins,"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
"
Algorithmic Bio-surveillance For Precise Spatio-temporal Prediction of Zoonotic Emergence,"  Viral zoonoses have emerged as the key drivers of recent pandemics. Human
infection by zoonotic viruses are either spillover events -- isolated
infections that fail to cause a widespread contagion -- or species jumps, where
successful adaptation to the new host leads to a pandemic. Despite expensive
bio-surveillance efforts, historically emergence response has been reactive,
and post-hoc. Here we use machine inference to demonstrate a high accuracy
predictive bio-surveillance capability, designed to pro-actively localize an
impending species jump via automated interrogation of massive sequence
databases of viral proteins. Our results suggest that a jump might not purely
be the result of an isolated unfortunate cross-infection localized in space and
time; there are subtle yet detectable patterns of genotypic changes
accumulating in the global viral population leading up to emergence. Using tens
of thousands of protein sequences simultaneously, we train models that track
maximum achievable accuracy for disambiguating host tropism from the primary
structure of surface proteins, and show that the inverse classification
accuracy is a quantitative indicator of jump risk. We validate our claim in the
context of the 2009 swine flu outbreak, and the 2004 emergence of H5N1
subspecies of Influenza A from avian reservoirs; illustrating that
interrogation of the global viral population can unambiguously track a near
monotonic risk elevation over several preceding years leading to eventual
emergence.
"
Routing in FRET-based Nanonetworks,"  Nanocommunications, understood as communications between nanoscale devices,
is commonly regarded as a technology essential for cooperation of large groups
of nanomachines and thus crucial for development of the whole area of
nanotechnology. While solutions for point-to-point nanocommunications have been
already proposed, larger networks cannot function properly without routing. In
this article we focus on the nanocommunications via Forster Resonance Energy
Transfer (FRET), which was found to be a technique with a very high signal
propagation speed, and discuss how to route signals through nanonetworks. We
introduce five new routing mechanisms, based on biological properties of
specific molecules. We experimentally validate one of these mechanisms.
Finally, we analyze open issues showing the technical challenges for signal
transmission and routing in FRET-based nanocommunications.
"
Spatial Models of Vector-Host Epidemics with Directed Movement of Vectors Over Long Distances,"  We investigate a time-dependent spatial vector-host epidemic model with
non-coincident domains for the vector and host populations. The host population
resides in small non-overlapping sub-regions, while the vector population
resides throughout a much larger region. The dynamics of the populations are
modeled by a reaction-diffusion-advection compartmental system of partial
differential equations. The disease is transmitted through vector and host
populations in criss-cross fashion. We establish global well-posedness and
uniform a prior bounds as well as the long-term behavior. The model is applied
to simulate the outbreak of bluetongue disease in sheep transmitted by midges
infected with bluetongue virus. We show that the long-range directed movement
of the midge population, due to wind-aided movement, enhances the transmission
of the disease to sheep in distant sites.
"
Active matter invasion of a viscous fluid: unstable sheets and a no-flow theorem,"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
"
Parallelized Linear Classification with Volumetric Chemical Perceptrons,"  In this work, we introduce a new type of linear classifier that is
implemented in a chemical form. We propose a novel encoding technique which
simultaneously represents multiple datasets in an array of microliter-scale
chemical mixtures. Parallel computations on these datasets are performed as
robotic liquid handling sequences, whose outputs are analyzed by
high-performance liquid chromatography. As a proof of concept, we chemically
encode several MNIST images of handwritten digits and demonstrate successful
chemical-domain classification of the digits using volumetric perceptrons. We
additionally quantify the performance of our method with a larger dataset of
binary vectors and compare the experimental measurements against predicted
results. Paired with appropriate chemical analysis tools, our approach can work
on increasingly parallel datasets. We anticipate that related approaches will
be scalable to multilayer neural networks and other more complex algorithms.
Much like recent demonstrations of archival data storage in DNA, this work
blurs the line between chemical and electrical information systems, and offers
early insight into the computational efficiency and massive parallelism which
may come with computing in chemical domains.
"
Can a heart rate variability biomarker identify the presence of autism spectrum disorder in eight year old children?,"  Autonomic nervous system (ANS) activity is altered in autism spectrum
disorder (ASD). Heart rate variability (HRV) derived from electrocardiogram
(ECG) has been a powerful tool to identify alterations in ANS due to a plethora
of pathophysiological conditions, including psychological ones such as
depression. ECG-derived HRV thus carries a yet to be explored potential to be
used as a diagnostic and follow-up biomarker of ASD. However, few studies have
explored this potential. In a cohort of boys (ages 8 - 11 years) with (n=18)
and without ASD (n=18), we tested a set of linear and nonlinear HRV measures,
including phase rectified signal averaging (PRSA), applied to a segment of ECG
collected under resting conditions for their predictive properties of ASD. We
identified HRV measures derived from time, frequency and geometric
signal-analytical domains which are changed in ASD children relative to peers
without ASD and correlate to psychometric scores (p<0.05 for each). Receiver
operating curves area ranged between 0.71 - 0.74 for each HRV measure. Despite
being a small cohort lacking external validation, these promising preliminary
results warrant larger prospective validation studies.
"
Complex pattern formation driven by the interaction of stable fronts in a competition-diffusion system,"  The ecological invasion problem in which a weaker exotic species invades an
ecosystem inhabited by two strongly competing native species is modelled by a
three-species competition-diffusion system. It is known that for a certain
range of parameter values competitor-mediated coexistence occurs and complex
spatio-temporal patterns are observed in two spatial dimensions. In this paper
we uncover the mechanism which generates such patterns. Under some assumptions
on the parameters the three-species competition-diffusion system admits two
planarly stable travelling waves. Their interaction in one spatial dimension
may result in either reflection or merging into a single homoclinic wave,
depending on the strength of the invading species. This transition can be
understood by studying the bifurcation structure of the homoclinic wave. In
particular, a time-periodic homoclinic wave (breathing wave) is born from a
Hopf bifurcation and its unstable branch acts as a separator between the
reflection and merging regimes. The same transition occurs in two spatial
dimensions: the stable regular spiral associated to the homoclinic wave
destabilizes, giving rise first to an oscillating breathing spiral and then
breaking up producing a dynamic pattern characterized by many spiral cores. We
find that these complex patterns are generated by the interaction of two
planarly stable travelling waves, in contrast with many other well known cases
of pattern formation where planar instability plays a central role.
"
Co-evolution of nodes and links: diversity driven coexistence in cyclic competition of three species,"  When three species compete cyclically in a well-mixed, stochastic system of
$N$ individuals, extinction is known to typically occur at times scaling as the
system size $N$. This happens, for example, in rock-paper-scissors games or
conserved Lotka-Volterra models in which every pair of individuals can interact
on a complete graph. Here we show that if the competing individuals also have a
""social temperament"" to be either introverted or extroverted, leading them to
cut or add links respectively, then long-living state in which all species
coexist can occur when both introverts and extroverts are present. These states
are non-equilibrium quasi-steady states, maintained by a subtle balance between
species competition and network dynamcis. Remarkably, much of the phenomena is
embodied in a mean-field description. However, an intuitive understanding of
why diversity stabilizes the co-evolving node and link dynamics remains an open
issue.
"
Deep learning based supervised semantic segmentation of Electron Cryo-Subtomograms,"  Cellular Electron Cryo-Tomography (CECT) is a powerful imaging technique for
the 3D visualization of cellular structure and organization at submolecular
resolution. It enables analyzing the native structures of macromolecular
complexes and their spatial organization inside single cells. However, due to
the high degree of structural complexity and practical imaging limitations,
systematic macromolecular structural recovery inside CECT images remains
challenging. Particularly, the recovery of a macromolecule is likely to be
biased by its neighbor structures due to the high molecular crowding. To reduce
the bias, here we introduce a novel 3D convolutional neural network inspired by
Fully Convolutional Network and Encoder-Decoder Architecture for the supervised
segmentation of macromolecules of interest in subtomograms. The tests of our
models on realistically simulated CECT data demonstrate that our new approach
has significantly improved segmentation performance compared to our baseline
approach. Also, we demonstrate that the proposed model has generalization
ability to segment new structures that do not exist in training data.
"
What does the free energy principle tell us about the brain?,"  The free energy principle has been proposed as a unifying theory of brain
function. It is closely related, and in some cases subsumes, earlier unifying
ideas such as Bayesian inference, predictive coding, and active learning. This
article clarifies these connections, teasing apart distinctive and shared
predictions.
"
Localization Algorithm with Circular Representation in 2D and its Similarity to Mammalian Brains,"  Extended Kalman filter (EKF) does not guarantee consistent mean and
covariance under linearization, even though it is the main framework for
robotic localization. While Lie group improves the modeling of the state space
in localization, the EKF on Lie group still relies on the arbitrary Gaussian
assumption in face of nonlinear models. We instead use von Mises filter for
orientation estimation together with the conventional Kalman filter for
position estimation, and thus we are able to characterize the first two moments
of the state estimates. Since the proposed algorithm holds a solid
probabilistic basis, it is fundamentally relieved from the inconsistency
problem. Furthermore, we extend the localization algorithm to fully circular
representation even for position, which is similar to grid patterns found in
mammalian brains and in recurrent neural networks. The applicability of the
proposed algorithms is substantiated not only by strong mathematical foundation
but also by the comparison against other common localization methods.
"
Spatial heterogeneities shape collective behavior of signaling amoeboid cells,"  We present novel experimental results on pattern formation of signaling
Dictyostelium discoideum amoeba in the presence of a periodic array of
millimeter-sized pillars. We observe concentric cAMP waves that initiate almost
synchronously at the pillars and propagate outwards. These waves have higher
frequency than the other firing centers and dominate the system dynamics. The
cells respond chemotactically to these circular waves and stream towards the
pillars, forming periodic Voronoi domains that reflect the periodicity of the
underlying lattice. We performed comprehensive numerical simulations of a
reaction-diffusion model to study the characteristics of the boundary
conditions given by the obstacles. Our simulations show that, the obstacles can
act as the wave source depending on the imposed boundary condition.
Interestingly, a critical minimum accumulation of cAMP around the obstacles is
needed for the pillars to act as the wave source. This critical value is lower
at smaller production rates of the intracellular cAMP which can be controlled
in our experiments using caffeine. Experiments and simulations also show that
in the presence of caffeine the number of firing centers is reduced which is
crucial in our system for circular waves emitted from the pillars to
successfully take over the dynamics. These results are crucial to understand
the signaling mechanism of Dictyostelium cells that experience spatial
heterogeneities in its natural habitat.
"
Simulation assisted machine learning,"  Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
"
The Bayesian optimist's guide to adaptive immune receptor repertoire analysis,"  Probabilistic modeling is fundamental to the statistical analysis of complex
data. In addition to forming a coherent description of the data-generating
process, probabilistic models enable parameter inference about given data sets.
This procedure is well-developed in the Bayesian perspective, in which one
infers probability distributions describing to what extent various possible
parameters agree with the data. In this paper we motivate and review
probabilistic modeling for adaptive immune receptor repertoire data then
describe progress and prospects for future work, from germline haplotyping to
adaptive immune system deployment across tissues. The relevant quantities in
immune sequence analysis include not only continuous parameters such as gene
use frequency, but also discrete objects such as B cell clusters and lineages.
Throughout this review, we unravel the many opportunities for probabilistic
modeling in adaptive immune receptor analysis, including settings for which the
Bayesian approach holds substantial promise (especially if one is optimistic
about new computational methods). From our perspective the greatest prospects
for progress in probabilistic modeling for repertoires concern ancestral
sequence estimation for B cell receptor lineages, including uncertainty from
germline genotype, rearrangement, and lineage development.
"
Sparse Coding Predicts Optic Flow Specificities of Zebrafish Pretectal Neurons,"  Zebrafish pretectal neurons exhibit specificities for large-field optic flow
patterns associated with rotatory or translatory body motion. We investigate
the hypothesis that these specificities reflect the input statistics of natural
optic flow. Realistic motion sequences were generated using computer graphics
simulating self-motion in an underwater scene. Local retinal motion was
estimated with a motion detector and encoded in four populations of
directionally tuned retinal ganglion cells, represented as two signed input
variables. This activity was then used as input into one of two learning
networks: a sparse coding network (competitive learning) and backpropagation
network (supervised learning). Both simulations develop specificities for optic
flow which are comparable to those found in a neurophysiological study (Kubo et
al. 2014), and relative frequencies of the various neuronal responses are best
modeled by the sparse coding approach. We conclude that the optic flow neurons
in the zebrafish pretectum do reflect the optic flow statistics. The predicted
vectorial receptive fields show typical optic flow fields but also ""Gabor"" and
dipole-shaped patterns that likely reflect difference fields needed for
reconstruction by linear superposition.
"
Decision-making processes underlying pedestrian behaviours at signalised crossings: Part 2. Do pedestrians show cultural herding behaviour ?,"  Followership is generally defined as a strategy that evolved to solve social
coordination problems, and particularly those involved in group movement.
Followership behaviour is particularly interesting in the context of
road-crossing behaviour because it involves other principles such as
risk-taking and evaluating the value of social information. This study sought
to identify the cognitive mechanisms underlying decision-making by pedestrians
who follow another person across the road at the green or at the red light in
two different countries (France and Japan). We used agent-based modelling to
simulate the road-crossing behaviours of pedestrians. This study showed that
modelling is a reliable means to test different hypotheses and find the exact
processes underlying decision-making when crossing the road. We found that two
processes suffice to simulate pedestrian behaviours. Importantly, the study
revealed differences between the two nationalities and between sexes in the
decision to follow and cross at the green and at the red light. Japanese
pedestrians are particularly attentive to the number of already departed
pedestrians and the number of waiting pedestrians at the red light, whilst
their French counterparts only consider the number of pedestrians that have
already stepped off the kerb, thus showing the strong conformism of Japanese
people. Finally, the simulations are revealed to be similar to observations,
not only for the departure latencies but also for the number of crossing
pedestrians and the rates of illegal crossings. The conclusion suggests new
solutions for safety in transportation research.
"
Building Models for Biopathway Dynamics Using Intrinsic Dimensionality Analysis,"  An important task for many if not all the scientific domains is efficient
knowledge integration, testing and codification. It is often solved with model
construction in a controllable computational environment. In spite of that, the
throughput of in-silico simulation-based observations become similarly
intractable for thorough analysis. This is especially the case in molecular
biology, which served as a subject for this study. In this project, we aimed to
test some approaches developed to deal with the curse of dimensionality. Among
these we found dimension reduction techniques especially appealing. They can be
used to identify irrelevant variability and help to understand critical
processes underlying high-dimensional datasets. Additionally, we subjected our
data sets to nonlinear time series analysis, as those are well established
methods for results comparison. To investigate the usefulness of dimension
reduction methods, we decided to base our study on a concrete sample set. The
example was taken from the domain of systems biology concerning dynamic
evolution of sub-cellular signaling. Particularly, the dataset relates to the
yeast pheromone pathway and is studied in-silico with a stochastic model. The
model reconstructs signal propagation stimulated by a mating pheromone. In the
paper, we elaborate on the reason of multidimensional analysis problem in the
context of molecular signaling, and next, we introduce the model of choice,
simulation details and obtained time series dynamics. A description of used
methods followed by a discussion of results and their biological interpretation
finalize the paper.
"
Biologically Plausible Online Principal Component Analysis Without Recurrent Neural Dynamics,"  Artificial neural networks that learn to perform Principal Component Analysis
(PCA) and related tasks using strictly local learning rules have been
previously derived based on the principle of similarity matching: similar pairs
of inputs should map to similar pairs of outputs. However, the operation of
these networks (and of similar networks) requires a fixed-point iteration to
determine the output corresponding to a given input, which means that dynamics
must operate on a faster time scale than the variation of the input. Further,
during these fast dynamics such networks typically ""disable"" learning, updating
synaptic weights only once the fixed-point iteration has been resolved. Here,
we derive a network for PCA-based dimensionality reduction that avoids this
fast fixed-point iteration. The key novelty of our approach is a modification
of the similarity matching objective to encourage near-diagonality of a
synaptic weight matrix. We then approximately invert this matrix using a Taylor
series approximation, replacing the previous fast iterations. In the offline
setting, our algorithm corresponds to a dynamical system, the stability of
which we rigorously analyze. In the online setting (i.e., with stochastic
gradients), we map our algorithm to a familiar neural network architecture and
give numerical results showing that our method converges at a competitive rate.
The computational complexity per iteration of our online algorithm is linear in
the total degrees of freedom, which is in some sense optimal.
"
Psychophysical laws as reflection of mental space properties,"  The paper is devoted to the relationship between psychophysics and physics of
mind. The basic trends in psychophysics development are briefly discussed with
special attention focused on Teghtsoonian's hypotheses. These hypotheses pose
the concept of the universality of inner psychophysics and enable to speak
about psychological space as an individual object with its own properties.
Turning to the two-component description of human behavior (I. Lubashevsky,
Physics of the Human Mind, Springer, 2017) the notion of mental space is
formulated and human perception of external stimuli is treated as the emergence
of the corresponding images in the mental space. On one hand, these images are
caused by external stimuli and their magnitude bears the information about the
intensity of the corresponding stimuli. On the other hand, the individual
structure of such images as well as their subsistence after emergence is
determined only by the properties of mental space on its own. Finally, the
mental operations of image comparison and their scaling are defined in a way
allowing for the bounded capacity of human cognition. As demonstrated, the
developed theory of stimulus perception is able to explain the basic
regularities of psychophysics, e.g., (i) the regression and range effects
leading to the overestimation of weak stimuli and the underestimation of strong
stimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\textit{iii})
the sequential (memory) effects. As the final result, a solution to the
Fechner-Stevens dilemma is proposed. This solution posits that Fechner's
logarithmic law is not a consequences of Weber's law but stems from the
interplay of uncertainty in evaluating stimulus intensities and the multi-step
scaling required to overcome the stimulus incommensurability.
"
Domain Adaptation for Infection Prediction from Symptoms Based on Data from Different Study Designs and Contexts,"  Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
"
The bromodomain-containing protein Ibd1 links multiple chromatin related protein complexes to highly expressed genes in Tetrahymena thermophila,"  Background: The chromatin remodelers of the SWI/SNF family are critical
transcriptional regulators. Recognition of lysine acetylation through a
bromodomain (BRD) component is key to SWI/SNF function; in most eukaryotes,
this function is attributed to SNF2/Brg1.
Results: Using affinity purification coupled to mass spectrometry (AP-MS) we
identified members of a SWI/SNF complex (SWI/SNFTt) in Tetrahymena thermophila.
SWI/SNFTt is composed of 11 proteins, Snf5Tt, Swi1Tt, Swi3Tt, Snf12Tt, Brg1Tt,
two proteins with potential chromatin interacting domains and four proteins
without orthologs to SWI/SNF proteins in yeast or mammals. SWI/SNFTt subunits
localize exclusively to the transcriptionally active macronucleus (MAC) during
growth and development, consistent with a role in transcription. While
Tetrahymena Brg1 does not contain a BRD, our AP-MS results identified a
BRD-containing SWI/SNFTt component, Ibd1 that associates with SWI/SNFTt during
growth but not development. AP-MS analysis of epitope-tagged Ibd1 revealed it
to be a subunit of several additional protein complexes, including putative
SWRTt, and SAGATt complexes as well as a putative H3K4-specific histone methyl
transferase complex. Recombinant Ibd1 recognizes acetyl-lysine marks on
histones correlated with active transcription. Consistent with our AP-MS and
histone array data suggesting a role in regulation of gene expression, ChIP-Seq
analysis of Ibd1 indicated that it primarily binds near promoters and within
gene bodies of highly expressed genes during growth.
Conclusions: Our results suggest that through recognizing specific histones
marks, Ibd1 targets active chromatin regions of highly expressed genes in
Tetrahymena where it subsequently might coordinate the recruitment of several
chromatin remodeling complexes to regulate the transcriptional landscape of
vegetatively growing Tetrahymena cells.
"
Partial Order on the set of Boolean Regulatory Functions,"  Logical models have been successfully used to describe regulatory and
signaling networks without requiring quantitative data. However, existing data
is insufficient to adequately define a unique model, rendering the
parametrization of a given model a difficult task.
Here, we focus on the characterization of the set of Boolean functions
compatible with a given regulatory structure, i.e. the set of all monotone
nondegenerate Boolean functions. We then propose an original set of rules to
locally explore the direct neighboring functions of any function in this set,
without explicitly generating the whole set. Also, we provide relationships
between the regulatory functions and their corresponding dynamics.
Finally, we illustrate the usefulness of this approach by revisiting
Probabilistic Boolean Networks with the model of T helper cell differentiation
from Mendoza & Xenarios.
"
Collision Selective Visual Neural Network Inspired by LGMD2 Neurons in Juvenile Locusts,"  For autonomous robots in dynamic environments mixed with human, it is vital
to detect impending collision quickly and robustly. The biological visual
systems evolved over millions of years may provide us efficient solutions for
collision detection in complex environments. In the cockpit of locusts, two
Lobula Giant Movement Detectors, i.e. LGMD1 and LGMD2, have been identified
which respond to looming objects rigorously with high firing rates. Compared to
LGMD1, LGMD2 matures early in the juvenile locusts with specific selectivity to
dark moving objects against bright background in depth while not responding to
light objects embedded in dark background - a similar situation which ground
vehicles and robots are facing with. However, little work has been done on
modeling LGMD2, let alone its potential in robotics and other vision-based
applications. In this article, we propose a novel way of modeling LGMD2 neuron,
with biased ON and OFF pathways splitting visual streams into parallel channels
encoding brightness increments and decrements separately to fulfill its
selectivity. Moreover, we apply a biophysical mechanism of spike frequency
adaptation to shape the looming selectivity in such a collision-detecting
neuron model. The proposed visual neural network has been tested with
systematic experiments, challenged against synthetic and real physical stimuli,
as well as image streams from the sensor of a miniature robot. The results
demonstrated this framework is able to detect looming dark objects embedded in
bright backgrounds selectively, which make it ideal for ground mobile
platforms. The robotic experiments also showed its robustness in collision
detection - it performed well for near range navigation in an arena with many
obstacles. Its enhanced collision selectivity to dark approaching objects
versus receding and translating ones has also been verified via systematic
experiments.
"
Network analyses of 4D genome datasets automate detection of community-scale gene structure and plasticity,"  Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
"
Caulking the Leakage Effect in MEEG Source Connectivity Analysis,"  Simplistic estimation of neural connectivity in MEEG sensor space is
impossible due to volume conduction. The only viable alternative is to carry
out connectivity estimation in source space. Among the neuroscience community
this is claimed to be impossible or misleading due to Leakage: linear mixing of
the reconstructed sources. To address this problematic we propose a novel
solution method that caulks the Leakage in MEEG source activity and
connectivity estimates: BC-VARETA. It is based on a joint estimation of source
activity and connectivity in the frequency domain representation of MEEG time
series. To achieve this, we go beyond current methods that assume a fixed
gaussian graphical model for source connectivity. In contrast we estimate this
graphical model in a Bayesian framework by placing priors on it, which allows
for highly optimized computations of the connectivity, via a new procedure
based on the local quadratic approximation under quite general prior models. A
further contribution of this paper is the rigorous definition of leakage via
the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic
distances over the cortical manifold. Both measures are extended for the first
time to quantify Connectivity Leakage by defining them on the cartesian product
of cortical manifolds. Using these measures, we show that BC-VARETA outperforms
most state of the art inverse solvers by several orders of magnitude.
"
Combinatorial views on persistent characters in phylogenetics,"  The so-called binary perfect phylogeny with persistent characters has
recently been thoroughly studied in computational biology as it is less
restrictive than the well known binary perfect phylogeny. Here, we focus on the
notion of (binary) persistent characters, i.e. characters that can be realized
on a phylogenetic tree by at most one $0 \rightarrow 1$ transition followed by
at most one $1 \rightarrow 0$ transition in the tree, and analyze these
characters under different aspects. First, we illustrate the connection between
persistent characters and Maximum Parsimony, where we characterize persistent
characters in terms of the first phase of the famous Fitch algorithm.
Afterwards we focus on the number of persistent characters for a given
phylogenetic tree. We show that this number solely depends on the balance of
the tree. To be precise, we develop a formula for counting the number of
persistent characters for a given phylogenetic tree based on an index of tree
balance, namely the Sackin index. Lastly, we consider the question of how many
(carefully chosen) binary characters together with their persistence status are
needed to uniquely determine a phylogenetic tree and provide an upper bound for
the number of characters needed.
"
FNS: an event-driven spiking neural network framework for efficient simulations of large-scale brain models,"  Limitations in processing capabilities and memory of today's computers make
spiking neuron-based (human) whole-brain simulations inevitably characterized
by a compromise between bio-plausibility and computational cost. It translates
into brain models composed of a reduced number of neurons and a simplified
neuron's mathematical model. Taking advantage of the sparse character of
brain-like computation, eventdriven technique allows us to carry out efficient
simulation of large-scale Spiking Neural Networks (SNN). The recent Leaky
Integrate-and-Fire with Latency (LIFL) spiking neuron model is event-driven
compatible and exhibits some realistic neuronal features, opening new horizons
in whole-brain modelling. In this paper we present FNS, a LIFL-based exact
event-driven spiking neural network framework implemented in Java and oriented
to wholebrain simulations. FNS combines spiking/synaptic whole-brain modelling
with the event-driven approach, allowing us to define heterogeneous modules and
multi-scale connectivity with delayed connections and plastic synapses,
providing fast simulations at the same time. A novel parallelization strategy
is also implemented in order to further speed up simulations. This paper
presents mathematical models, software implementation and simulation routines
on which FNS is based. Finally, a reduced brain network model (1400 neurons and
45000 synapses) is synthesized on the basis of real brain structural data, and
the resulting model activity is compared with associated brain functional
(source-space MEG) data. The conducted test shows a good matching between the
activity of model and that of the emulated subject, in outstanding simulation
times (about 20s for simulating 4s of activity with a normal PC). Dedicated
sections of stimuli editing and output synthesis allow the neuroscientist to
introduce and extract brain-like signals, respectively...
"
WheelCon: A wheel control-based gaming platform for studying human sensorimotor control,"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
"
Modeling Oral Multispecies Biofilm Recovery After Antibacterial Treatment,"  Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
"
The Ising distribution as a latent variable model,"  It is shown that the Ising distribution can be treated as a latent variable
model, where a set of N real-valued, correlated random variables are drawn and
used to generate N binary spins independently. This allows to approximate the
Ising distribution by a simpler model where the latent variables follow a
multivariate normal distribution, the so-called Cox distribution. The
approximation is formally related to an advanced mean field technique known as
adaptive TAP, and its domain of validity is similar. When valid, it allows a
principled replacement of the Ising distribution by a distribution much easier
to sample and manipulate.
"
Performance Analysis of MEC Approach for Haplotype Assembly,"  The Minimum Error Correction (MEC) approach is used as a metric for
reconstruction of haplotypes from NGS reads. In this paper, we show that the
MEC may encounter with imprecise reconstructed haplotypes for some NGS devices.
Specifically, using mathematical derivations, we evaluate this approach for the
SOLiD, Illumina, 454, Ion, Pacific BioSciences, Oxford Nanopore, and 10X
Genomics devices. Our results reveal that the MEC yields inexact haplotypes for
the Illumina MiniSeq, 454 GS Junior+, Ion PGM 314, and Oxford Nanopore MK 1
MinION.
"
The modularity of action and perception revisited using control theory and active inference,"  The assumption that action and perception can be investigated independently
is entrenched in theories, models and experimental approaches across the brain
and mind sciences. In cognitive science, this has been a central point of
contention between computationalist and 4Es (enactive, embodied, extended and
embedded) theories of cognition, with the former embracing the ""classical
sandwich"", modular, architecture of the mind and the latter actively denying
this separation can be made. In this work we suggest that the modular
independence of action and perception strongly resonates with the separation
principle of control theory and furthermore that this principle provides formal
criteria within which to evaluate the implications of the modularity of action
and perception. We will also see that real-time feedback with the environment,
often considered necessary for the definition of 4Es ideas, is not however a
sufficient condition to avoid the ""classical sandwich"". Finally, we argue that
an emerging framework in the cognitive and brain sciences, active inference,
extends ideas derived from control theory to the study of biological systems
while disposing of the separation principle, describing non-modular models of
behaviour strongly aligned with 4Es theories of cognition.
"
Is Information in the Brain Represented in Continuous or Discrete Form?,"  The question of continuous-versus-discrete information representation in the
brain is a fundamental yet unresolved physiological question. Historically,
most analyses assume a continuous representation without considering the
alternative possibility of a discrete representation. Our work explores the
plausibility of both representations, and answers the question from a
communications engineering perspective. Drawing on the well-established
Shannon's communications theory, we posit that information in the brain is
represented in a discrete form. Using a computer simulation, we show that
information cannot be communicated reliably between neurons using a continuous
representation, due to the presence of noise; neural information has to be in a
discrete form. In addition, we designed 3 (human) behavioral experiments on
probability estimation and analyzed the data using a novel discrete (quantized)
model of probability. Under a discrete model of probability, two distinct
probabilities (say, 0.57 and 0.58) are treated indifferently. We found that
data from all participants were better fit to discrete models than continuous
ones. Furthermore, we re-analyzed the data from a published (human) behavioral
study on intertemporal choice using a novel discrete (quantized) model of
intertemporal choice. Under such a model, two distinct time delays (say, 16
days and 17 days) are treated indifferently. We found corroborating results,
showing that data from all participants were better fit to discrete models than
continuous ones. In summary, all results reported here support our discrete
hypothesis of information representation in the brain, which signifies a major
demarcation from the current understanding of the brain's physiology.
"
Asymptotic and numerical analysis of a stochastic PDE model of volume transmission,"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
"
Scale-free networks are rare,"  A central claim in modern network science is that real-world networks are
typically ""scale free,"" meaning that the fraction of nodes with degree $k$
follows a power law, decaying like $k^{-\alpha}$, often with $2 < \alpha < 3$.
However, empirical evidence for this belief derives from a relatively small
number of real-world networks. We test the universality of scale-free structure
by applying state-of-the-art statistical tools to a large corpus of nearly 1000
network data sets drawn from social, biological, technological, and
informational sources. We fit the power-law model to each degree distribution,
test its statistical plausibility, and compare it via a likelihood ratio test
to alternative, non-scale-free models, e.g., the log-normal. Across domains, we
find that scale-free networks are rare, with only 4% exhibiting the
strongest-possible evidence of scale-free structure and 52% exhibiting the
weakest-possible evidence. Furthermore, evidence of scale-free structure is not
uniformly distributed across sources: social networks are at best weakly scale
free, while a handful of technological and biological networks can be called
strongly scale free. These results undermine the universality of scale-free
networks and reveal that real-world networks exhibit a rich structural
diversity that will likely require new ideas and mechanisms to explain.
"
Angiogenic Factors produced by Hypoxic Cells are a leading driver of Anastomoses in Sprouting Angiogenesis---a computational study,"  Angiogenesis - the growth of new blood vessels from a pre-existing
vasculature - is key in both physiological processes and on several
pathological scenarios such as cancer progression or diabetic retinopathy. For
the new vascular networks to be functional, it is required that the growing
sprouts merge either with an existing functional mature vessel or with another
growing sprout. This process is called anastomosis. We present a systematic 2D
and 3D computational study of vessel growth in a tissue to address the
capability of angiogenic factor gradients to drive anastomosis formation. We
consider that these growth factors are produced only by tissue cells in
hypoxia, i.e. until nearby vessels merge and become capable of carrying blood
and irrigating their vicinity. We demonstrate that this increased production of
angiogenic factors by hypoxic cells is able to promote vessel anastomoses
events in both 2D and 3D. The simulations also verify that the morphology of
these networks has an increased resilience toward variations in the endothelial
cell's proliferation and chemotactic response. The distribution of tissue
cell`s and the concentration of the growth factors they produce are the major
factors in determining the final morphology of the network.
"
Nonlinear learning and learning advantages in evolutionary games,"  The idea of incompetence as a learning or adaptation function was introduced
in the context of evolutionary games as a fixed parameter. However, live
organisms usually perform different nonlinear adaptation functions such as a
power law or exponential fitness growth. Here, we examine how the functional
form of the learning process may affect the social competition between
different behavioral types. Further, we extend our results for the evolutionary
games where fluctuations in the environment affect the behavioral adaptation of
competing species and demonstrate importance of the starting level of
incompetence for survival. Hence, we define a new concept of learning
advantages that becomes crucial when environments are constantly changing and
requiring rapid adaptation from species. This may lead to the evolutionarily
weak phase when even evolutionary stable populations become vulnerable to
invasions.
"
Proofs of life: molecular-biology reasoning simulates cell behaviors from first principles,"  We axiomatize the molecular-biology reasoning style, verify compliance of the
standard reference: Ptashne, A Genetic Switch, and present proof-theory-induced
technologies to predict phenotypes and life cycles from genotypes. The key is
to note that `reductionist discipline' entails constructive reasoning, i.e.,
that any argument for a compound property is constructed from more basic
arguments. Proof theory makes explicit the inner structure of the axiomatized
reasoning style and allows the permissible dynamics to be presented as a mode
of computation that can be executed and analyzed. Constructivity and
executability guarantee simulation when working over domain-specific languages.
Here, we exhibit phenotype properties for genotype reasons: a molecular-biology
argument is an open-system concurrent computation that results in compartment
changes and is performed among processes of physiology change as determined
from the molecular programming of given DNA. Life cycles are the possible
sequentializations of the processes. A main implication of our construction is
that technical correctness provides a complementary perspective on science that
is as fundamental there as it is for pure mathematics, provided mature
reductionism exists.
"
Fuzzy logic based approaches for gene regulatory network inference,"  The rapid advancement in high-throughput techniques has fueled the generation
of large volume of biological data rapidly with low cost. Some of these
techniques are microarray and next generation sequencing which provides genome
level insight of living cells. As a result, the size of most of the biological
databases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These
biological data are analyzed using computational techniques for knowledge
discovery - which is one of the objectives of bioinformatics research. Gene
regulatory network (GRN) is a gene-gene interaction network which plays pivotal
role in understanding gene regulation process and disease studies. From the
last couple of decades, the researchers are interested in developing
computational algorithms for GRN inference (GRNI) using high-throughput
experimental data. Several computational approaches have been applied for
inferring GRN from gene expression data including statistical techniques
(correlation coefficient), information theory (mutual information), regression
based approaches, probabilistic approaches (Bayesian networks, naive byes),
artificial neural networks, and fuzzy logic. The fuzzy logic, along with its
hybridization with other intelligent approach, is well studied in GRNI due to
its several advantages. In this paper, we present a consolidated review on
fuzzy logic and its hybrid approaches for GRNI developed during last two
decades.
"
Gradient-based Representational Similarity Analysis with Searchlight for Analyzing fMRI Data,"  Representational Similarity Analysis (RSA) aims to explore similarities
between neural activities of different stimuli. Classical RSA techniques employ
the inverse of the covariance matrix to explore a linear model between the
neural activities and task events. However, calculating the inverse of a
large-scale covariance matrix is time-consuming and can reduce the stability
and robustness of the final analysis. Notably, it becomes severe when the
number of samples is too large. For facing this shortcoming, this paper
proposes a novel RSA method called gradient-based RSA (GRSA). Moreover, the
proposed method is not restricted to a linear model. In fact, there is a
growing interest in finding more effective ways of using multi-subject and
whole-brain fMRI data. Searchlight technique can extend RSA from the localized
brain regions to the whole-brain regions with smaller memory footprint in each
process. Based on Searchlight, we propose a new method called Spatiotemporal
Searchlight GRSA (SSL-GRSA) that generalizes our ROI-based GRSA algorithm to
the whole-brain data. Further, our approach can handle some computational
challenges while dealing with large-scale, multi-subject fMRI data.
Experimental studies on multi-subject datasets confirm that both proposed
approaches achieve superior performance to other state-of-the-art RSA
algorithms.
"
Temporal Stable Community in Time-Varying Networks,"  Identifying community structure of a complex network provides insight to the
interdependence between the network topology and emergent collective behaviors
of networks, while detecting such invariant communities in a time-varying
network is more challenging. In this paper, we define the temporal stable
community and newly propose the concept of dynamic modularity to evaluate the
stable community structures in time-varying networks, which is robust against
small changes as verified by several empirical time-varying network datasets.
Besides, using the volatility features of temporal stable communities in
functional brain networks, we successfully differentiate the ADHD (Attention
Deficit Hyperactivity Disorder) patients and healthy controls efficiently.
"
Thermodynamic Mechanism of Life and Aging,"  Life is a complex biological phenomenon represented by numerous chemical,
physical and biological processes performed by a biothermodynamic
system/cell/organism. Both living organisms and inanimate objects are subject
to aging, a biological and physicochemical process characterized by changes in
biological and thermodynamic state. Thus, the same physical laws govern
processes in both animate and inanimate matter. All life processes lead to
change of an organism's state. The change of biological and thermodynamic state
of an organism in time underlies all of three kinds of aging (chronological,
biological and thermodynamic). Life and aging of an organism both start at the
moment of fertilization and continue through entire lifespan. Fertilization
represents formation of a new organism. The new organism represents a new
thermodynamic system. From the very beginning, it changes its state by changing
thermodynamic parameters. The change of thermodynamic parameters is observed as
aging and can be related to change in entropy. Entropy is thus the parameter
that is related to all others and describes aging in the best manner. In the
beginning, entropy change appears as a consequence of accumulation of matter
(growth). Later, decomposition and configurational changes dominate, as a
consequence of various chemical reactions (free radical, decomposition,
fragmentation, accumulation of lipofuscin-like substances...).
"
Markov modeling of peptide folding in the presence of protein crowders,"  We use Markov state models (MSMs) to analyze the dynamics of a
$\beta$-hairpin-forming peptide in Monte Carlo (MC) simulations with
interacting protein crowders, for two different types of crowder proteins
[bovine pancreatic trypsin inhibitor (BPTI) and GB1]. In these systems, at the
temperature used, the peptide can be folded or unfolded and bound or unbound to
crowder molecules. Four or five major free-energy minima can be identified. To
estimate the dominant MC relaxation times of the peptide, we build MSMs using a
range of different time resolutions or lag times. We show that stable
relaxation-time estimates can be obtained from the MSM eigenfunctions through
fits to autocorrelation data. The eigenfunctions remain sufficiently accurate
to permit stable relaxation-time estimation down to small lag times, at which
point simple estimates based on the corresponding eigenvalues have large
systematic uncertainties. The presence of the crowders have a stabilizing
effect on the peptide, especially with BPTI crowders, which can be attributed
to a reduced unfolding rate $k_\text{u}$, while the folding rate $k_\text{f}$
is left largely unchanged.
"
Deep Neural Network for Analysis of DNA Methylation Data,"  Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
"
"Reconciling cooperation, biodiversity and stability in complex ecological communities","  Empirical observations show that ecological communities can have a huge
number of coexisting species, also with few or limited number of resources.
These ecosystems are characterized by multiple type of interactions, in
particular displaying cooperative behaviors. However, standard modeling of
population dynamics based on Lotka-Volterra type of equations predicts that
ecosystem stability should decrease as the number of species in the community
increases and that cooperative systems are less stable than communities with
only competitive and/or exploitative interactions. Here we propose a stochastic
model of population dynamics, which includes exploitative interactions as well
as cooperative interactions induced by cross-feeding. The model is exactly
solved and we obtain results for relevant macro-ecological patterns, such as
species abundance distributions and correlation functions. In the large system
size limit, any number of species can coexist for a very general class of
interaction networks and stability increases as the number of species grows.
For pure mutualistic/commensalistic interactions we determine the topological
properties of the network that guarantee species coexistence. We also show that
the stationary state is globally stable and that inferring species interactions
through species abundance correlation analysis may be misleading. Our
theoretical approach thus show that appropriate models of cooperation naturally
leads to a solution of the long-standing question about complexity-stability
paradox and on how highly biodiverse communities can coexist.
"
Optimal segregation of proteins: phase transitions and symmetry breaking,"  Asymmetric segregation of key proteins at cell division -- be it a beneficial
or deleterious protein -- is ubiquitous in unicellular organisms and often
considered as an evolved trait to increase fitness in a stressed environment.
Here, we provide a general framework to describe the evolutionary origin of
this asymmetric segregation. We compute the population fitness as a function of
the protein segregation asymmetry $a$, and show that the value of $a$ which
optimizes the population growth manifests a phase transition between symmetric
and asymmetric partitioning phases. Surprisingly, the nature of phase
transition is different for the case of beneficial proteins as opposed to
proteins which decrease the single-cell growth rate. Our study elucidates the
optimization problem faced by evolution in the context of protein segregation,
and motivates further investigation of asymmetric protein segregation in
biological systems.
"
Model-based clustering of multi-tissue gene expression data,"  Recently, it has become feasible to generate large-scale, multi-tissue gene
expression data, where expression profiles are obtained from multiple tissues
or organs sampled from dozens to hundreds of individuals. When traditional
clustering methods are applied to this type of data, important information is
lost, because they either require all tissues to be analyzed independently,
ignoring dependencies and similarities between tissues, or to merge tissues in
a single, monolithic dataset, ignoring individual characteristics of tissues.
We developed a Bayesian model-based multi-tissue clustering algorithm, revamp,
which can incorporate prior information on physiological tissue similarity, and
which results in a set of clusters, each consisting of a core set of genes
conserved across tissues as well as differential sets of genes specific to one
or more subsets of tissues. Using data from seven vascular and metabolic
tissues from over 100 individuals in the STockholm Atherosclerosis Gene
Expression (STAGE) study, we demonstrate that multi-tissue clusters inferred by
revamp are more enriched for tissue-dependent protein-protein interactions
compared to alternative approaches. We further demonstrate that revamp results
in easily interpretable multi-tissue gene expression associations to key
coronary artery disease processes and clinical phenotypes in the STAGE
individuals. Revamp is implemented in the Lemon-Tree software, available at
this https URL
"
mGPfusion: Predicting protein stability changes with Gaussian process kernel learning and data fusion,"  Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins' properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein's stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
"
Use of Genome Information-Based Potentials to Characterize Human Adaptation,"  As a living information and communications system, the genome encodes
patterns in single nucleotide polymorphisms (SNPs) reflecting human adaption
that optimizes population survival in differing environments. This paper
mathematically models environmentally induced adaptive forces that quantify
changes in the distribution of SNP frequencies between populations. We make
direct connections between biophysical methods (e.g. minimizing genomic free
energy) and concepts in population genetics. Our unbiased computer program
scanned a large set of SNPs in the major histocompatibility complex region, and
flagged an altitude dependency on a SNP associated with response to oxygen
deprivation. The statistical power of our double-blind approach is demonstrated
in the flagging of mathematical functional correlations of SNP
information-based potentials in multiple populations with specific
environmental parameters. Furthermore, our approach provides insights for new
discoveries on the biology of common variants. This paper demonstrates the
power of biophysical modeling of population diversity for better understanding
genome-environment interactions in biological phenomenon.
"
Topological Sieving of Rings According to their Rigidity,"  We present a novel mechanism for resolving the mechanical rigidity of
nanoscopic circular polymers that flow in a complex environment. The emergence
of a regime of negative differential mobility induced by topological
interactions between the rings and the substrate is the key mechanism for
selective sieving of circular polymers with distinct flexibilities. A simple
model accurately describes the sieving process observed in molecular dynamics
simulations and yields experimentally verifiable analytical predictions, which
can be used as a reference guide for improving filtration procedures of
circular filaments. The topological sieving mechanism we propose ought to be
relevant also in probing the microscopic details of complex substrates.
"
Positive and Unlabeled Learning through Negative Selection and Imbalance-aware Classification,"  Motivated by applications in protein function prediction, we consider a
challenging supervised classification setting in which positive labels are
scarce and there are no explicit negative labels. The learning algorithm must
thus select which unlabeled examples to use as negative training points,
possibly ending up with an unbalanced learning problem. We address these issues
by proposing an algorithm that combines active learning (for selecting negative
examples) with imbalance-aware learning (for mitigating the label imbalance).
In our experiments we observe that these two techniques operate
synergistically, outperforming state-of-the-art methods on standard protein
function prediction benchmarks.
"
Evolutionary multiplayer games on graphs with edge diversity,"  Evolutionary game dynamics in structured populations has been extensively
explored in past decades. However, most previous studies assume that payoffs of
individuals are fully determined by the strategic behaviors of interacting
parties and social ties between them only serve as the indicator of the
existence of interactions. This assumption neglects important information
carried by inter-personal social ties such as genetic similarity, geographic
proximity, and social closeness, which may crucially affect the outcome of
interactions. To model these situations, we present a framework of evolutionary
multiplayer games on graphs with edge diversity, where different types of edges
describe diverse social ties. Strategic behaviors together with social ties
determine the resulting payoffs of interactants. Under weak selection, we
provide a general formula to predict the success of one behavior over the
other. We apply this formula to various examples which cannot be dealt with
using previous models, including the division of labor and relationship- or
edge-dependent games. We find that labor division facilitates collective
cooperation by decomposing a many-player game into several games of smaller
sizes. The evolutionary process based on relationship-dependent games can be
approximated by interactions under a transformed and unified game. Our work
stresses the importance of social ties and provides effective methods to reduce
the calculating complexity in analyzing the evolution of realistic systems.
"
Predicting B Cell Receptor Substitution Profiles Using Public Repertoire Data,"  B cells develop high affinity receptors during the course of affinity
maturation, a cyclic process of mutation and selection. At the end of affinity
maturation, a number of cells sharing the same ancestor (i.e. in the same
""clonal family"") are released from the germinal center, their amino acid
frequency profile reflects the allowed and disallowed substitutions at each
position. These clonal-family-specific frequency profiles, called ""substitution
profiles"", are useful for studying the course of affinity maturation as well as
for antibody engineering purposes. However, most often only a single sequence
is recovered from each clonal family in a sequencing experiment, making it
impossible to construct a clonal-family-specific substitution profile. Given
the public release of many high-quality large B cell receptor datasets, one may
ask whether it is possible to use such data in a prediction model for
clonal-family-specific substitution profiles. In this paper, we present the
method ""Substitution Profiles Using Related Families"" (SPURF), a penalized
tensor regression framework that integrates information from a rich assemblage
of datasets to predict the clonal-family-specific substitution profile for any
single input sequence. Using this framework, we show that substitution profiles
from similar clonal families can be leveraged together with simulated
substitution profiles and germline gene sequence information to improve
prediction. We fit this model on a large public dataset and validate the
robustness of our approach on an external dataset. Furthermore, we provide a
command-line tool in an open-source software package
(this https URL) implementing these ideas and providing easy
prediction using our pre-fit models.
"
Endemicity and prevalence of multipartite viruses under heterogeneous between-host transmission,"  Multipartite viruses replicate through a puzzling evolutionary strategy.
Their genome is segmented into two or more parts, and encapsidated in separate
particles that appear to propagate independently. Completing the replication
cycle, however, requires the full genome, so that a persistent infection of a
host requires the concurrent presence of several particles. This represents an
apparent evolutionary drawback of multipartitism, while its advantages remain
unclear. A transition from monopartite to multipartite viral forms has been
described in vitro under conditions of high multiplicity of infection,
suggesting that cooperation between defective mutants is a plausible
evolutionary pathway towards multipartitism. However, it is unknown how the
putative advantages that multipartitism might enjoy affect its epidemiology, or
if an explicit advantage is needed to explain its ecological persistence. To
disentangle which mechanisms might contribute to the rise and fixation of
multipartitism, we here investigate the interaction between viral spreading
dynamics and host population structure. We set up a compartmental model of the
spread of a virus in its different forms and explore its epidemiology using
both analytical and numerical techniques. We uncover that the impact of host
contact structure on spreading dynamics entails a rich phenomenology of
ecological relationships that includes cooperation, competition, and
commensality. Furthermore, we find out that multipartitism might rise to
fixation even in the absence of explicit microscopic advantages. Multipartitism
allows the virus to colonize environments that could not be invaded by the
monopartite form, while homogeneous contacts between hosts facilitate its
spread. We conjecture that there might have been an increase in the diversity
and prevalence of multipartite viral forms concomitantly with the expansion of
agricultural practices.
"
Can justice be fair when it is blind? How social network structures can promote or prevent the evolution of despotism,"  Hierarchy is an efficient way for a group to organize, but often goes along
with inequality that benefits leaders. To control despotic behaviour, followers
can assess leaders decisions by aggregating their own and their neighbours
experience, and in response challenge despotic leaders. But in hierarchical
social networks, this interactional justice can be limited by (i) the high
influence of a small clique who are treated better, and (ii) the low
connectedness of followers. Here we study how the connectedness of a social
network affects the co-evolution of despotism in leaders and tolerance to
despotism in followers. We simulate the evolution of a population of agents,
where the influence of an agent is its number of social links. Whether a leader
remains in power is controlled by the overall satisfaction of group members, as
determined by their joint assessment of the leaders behaviour. We demonstrate
that centralization of a social network around a highly influential clique
greatly increases the level of despotism. This is because the clique is more
satisfied, and their higher influence spreads their positive opinion of the
leader throughout the network. Finally, our results suggest that increasing the
connectedness of followers limits despotism while maintaining hierarchy.
"
GSAE: an autoencoder with embedded gene-set nodes for genomics functional characterization,"  Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
"
A geometric attractor mechanism for self-organization of entorhinal grid modules,"  Grid cells in the medial entorhinal cortex (mEC) respond when an animal
occupies a periodic lattice of ""grid fields"" in the environment. The grids are
organized in modules with spatial periods clustered around discrete values
separated by constant ratios reported in the range 1.3-1.8. We propose a
mechanism for dynamical self-organization in the mEC that can produce this
modular structure. In attractor network models of grid formation, the period of
a single module is set by the length scale of recurrent inhibition between
neurons. We show that grid cells will instead form a hierarchy of discrete
modules if a continuous increase in inhibition distance along the dorso-ventral
axis of the mEC is accompanied by excitatory interactions along this axis.
Moreover, constant scale ratios between successive modules arise through
geometric relationships between triangular grids, whose lattice constants are
separated by $\sqrt{3} \approx 1.7$, $\sqrt{7}/2 \approx 1.3$, or other ratios.
We discuss how the interactions required by our model might be tested
experimentally and realized by circuits in the mEC.
"
Socio-economic constraints to maximum human lifespan,"  The analysis of the demographic transition of the past century and a half,
using both empirical data and mathematical models, has rendered a wealth of
well-established facts, including the dramatic increases in life expectancy.
Despite these insights, such analyses have also occasionally triggered debates
which spill over many disciplines, from genetics, to biology, or demography.
Perhaps the hottest discussion is happening around the question of maximum
human lifespan, which --besides its fascinating historical and philosophical
interest-- poses urgent pragmatic warnings on a number of issues in public and
private decision-making. In this paper, we add to the controversy some results
which, based on purely statistical grounds, suggest that the maximum human
lifespan is not fixed, or has not reached yet a plateau. Quite the contrary,
analysis on reliable data for over 150 years in more than 20 industrialized
countries point at a sustained increase in the maximum age at death.
Furthermore, were this trend to continue, a limitless lifespan could be
achieved by 2102. Finally, we quantify the dependence of increases in the
maximum lifespan on socio-economic factors. Our analysis indicates that in some
countries the observed rising patterns can only be sustained by progressively
larger increases in GDP, setting the problem of longevity in a context of
diminishing returns.
"
A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks,"  An explosion of high-throughput DNA sequencing in the past decade has led to
a surge of interest in population-scale inference with whole-genome data.
Recent work in population genetics has centered on designing inference methods
for relatively simple model classes, and few scalable general-purpose inference
techniques exist for more realistic, complex models. To achieve this, two
inferential challenges need to be addressed: (1) population data are
exchangeable, calling for methods that efficiently exploit the symmetries of
the data, and (2) computing likelihoods is intractable as it requires
integrating over a set of correlated, extremely high-dimensional latent
variables. These challenges are traditionally tackled by likelihood-free
methods that use scientific simulators to generate datasets and reduce them to
hand-designed, permutation-invariant summary statistics, often leading to
inaccurate inference. In this work, we develop an exchangeable neural network
that performs summary statistic-free, likelihood-free inference. Our framework
can be applied in a black-box fashion across a variety of simulation-based
tasks, both within and outside biology. We demonstrate the power of our
approach on the recombination hotspot testing problem, outperforming the
state-of-the-art.
"
Multilayer flows in molecular networks identify biological modules in the human proteome,"  A variety of complex systems exhibit different types of relationships
simultaneously that can be modeled by multiplex networks. A typical problem is
to determine the community structure of such systems that, in general, depend
on one or more parameters to be tuned. In this study we propose one measure,
grounded on information theory, to find the optimal value of the relax rate
characterizing Multiplex Infomap, the generalization of the Infomap algorithm
to the realm of multilayer networks. We evaluate our methodology on synthetic
networks, to show that the most representative community structure can be
reliably identified when the most appropriate relax rate is used. Capitalizing
on these results, we use this measure to identify the most reliable meso-scale
functional organization in the human protein-protein interaction multiplex
network and compare the observed clusters against a collection of independently
annotated gene sets from the Molecular Signatures Database (MSigDB). Our
analysis reveals that modules obtained with the optimal value of the relax rate
are biologically significant and, remarkably, with higher functional content
than the ones obtained from the aggregate representation of the human proteome.
Our framework allows us to characterize the meso-scale structure of those
multilayer systems whose layers are not explicitly interconnected each other --
as in the case of edge-colored models -- the ones describing most biological
networks, from proteomes to connectomes.
"
Non-Archimedean Replicator Dynamics and Eigen's Paradox,"  We present a new non-Archimedean model of evolutionary dynamics, in which the
genomes are represented by p-adic numbers. In this model the genomes have a
variable length, not necessarily bounded, in contrast with the classical models
where the length is fixed. The time evolution of the concentration of a given
genome is controlled by a p-adic evolution equation. This equation depends on a
fitness function f and on mutation measure Q. By choosing a mutation measure of
Gibbs type, and by using a p-adic version of the Maynard Smith Ansatz, we show
the existence of threshold function M_{c}(f,Q), such that the long term
survival of a genome requires that its length grows faster than M_{c}(f,Q).
This implies that Eigen's paradox does not occur if the complexity of genomes
grows at the right pace. About twenty years ago, Scheuring and Poole, Jeffares,
Penny proposed a hypothesis to explain Eigen's paradox. Our mathematical model
shows that this biological hypothesis is feasible, but it requires p-adic
analysis instead of real analysis. More exactly, the Darwin-Eigen cycle
proposed by Poole et al. takes place if the length of the genomes exceeds
M_{c}(f,Q).
"
Holographic Neural Architectures,"  Representation learning is at the heart of what makes deep learning
effective. In this work, we introduce a new framework for representation
learning that we call ""Holographic Neural Architectures"" (HNAs). In the same
way that an observer can experience the 3D structure of a holographed object by
looking at its hologram from several angles, HNAs derive Holographic
Representations from the training set. These representations can then be
explored by moving along a continuous bounded single dimension. We show that
HNAs can be used to make generative networks, state-of-the-art regression
models and that they are inherently highly resistant to noise. Finally, we
argue that because of their denoising abilities and their capacity to
generalize well from very few examples, models based upon HNAs are particularly
well suited for biological applications where training examples are rare or
noisy.
"
Functional geometry of protein-protein interaction networks,"  Motivation: Protein-protein interactions (PPIs) are usually modelled as
networks. These networks have extensively been studied using graphlets, small
induced subgraphs capturing the local wiring patterns around nodes in networks.
They revealed that proteins involved in similar functions tend to be similarly
wired. However, such simple models can only represent pairwise relationships
and cannot fully capture the higher-order organization of protein interactions,
including protein complexes. Results: To model the multi-sale organization of
these complex biological systems, we utilize simplicial complexes from
computational geometry. The question is how to mine these new representations
of PPI networks to reveal additional biological information. To address this,
we define simplets, a generalization of graphlets to simplicial complexes. By
using simplets, we define a sensitive measure of similarity between simplicial
complex network representations that allows for clustering them according to
their data types better than clustering them by using other state-of-the-art
measures, e.g., spectral distance, or facet distribution distance. We model
human and baker's yeast PPI networks as simplicial complexes that capture PPIs
and protein complexes as simplices. On these models, we show that our newly
introduced simplet-based methods cluster proteins by function better than the
clustering methods that use the standard PPI networks, uncovering the new
underlying functional organization of the cell. We demonstrate the existence of
the functional geometry in the PPI data and the superiority of our
simplet-based methods to effectively mine for new biological information hidden
in the complexity of the higher order organization of PPI networks.
"
Spontaneous domain formation in disordered copolymers as a mechanism for chromosome structuring,"  Motivated by the problem of domain formation in chromosomes, we studied a
co--polymer model where only a subset of the monomers feel attractive
interactions. These monomers are displaced randomly from a regularly-spaced
pattern, thus introducing some quenched disorder in the system. Previous work
has shown that in the case of regularly-spaced interacting monomers this chain
can fold into structures characterized by multiple distinct domains of
consecutive segments. In each domain, attractive interactions are balanced by
the entropy cost of forming loops. We show by advanced replica-exchange
simulations that adding disorder in the position of the interacting monomers
further stabilizes these domains. The model suggests that the partitioning of
the chain into well-defined domains of consecutive monomers is a spontaneous
property of heteropolymers. In the case of chromosomes, evolution could have
acted on the spacing of interacting monomers to modulate in a simple way the
underlying domains for functional reasons.
"
Electrical transient laws in neuronal microdomains based on electro-diffusion,"  The current-voltage (I-V) conversion characterizes the physiology of cellular
microdomains and reflects cellular communication, excitability, and electrical
transduction. Yet deriving such I-V laws remains a major challenge in most
cellular microdomains due to their small sizes and the difficulty of accessing
voltage with a high nanometer precision. We present here novel analytical
relations derived for different numbers of ionic species inside a neuronal
micro/nano-domains, such as dendritic spines. When a steady-state current is
injected, we find a large deviation from the classical Ohm's law, showing that
the spine neck resistance is insuficent to characterize electrical properties.
For a constricted spine neck, modeled by a hyperboloid, we obtain a new I-V law
that illustrates the consequences of narrow passages on electrical conduction.
Finally, during a fast current transient, the local voltage is modulated by the
distance between activated voltage-gated channels. To conclude,
electro-diffusion laws can now be used to interpret voltage distribution in
neuronal microdomains.
"
Anticipating epileptic seizures through the analysis of EEG synchronization as a data classification problem,"  Epilepsy is a neurological disorder arising from anomalies of the electrical
activity in the brain, affecting about 0.5--0.8\% of the world population.
Several studies investigated the relationship between seizures and brainwave
synchronization patterns, pursuing the possibility of identifying interictal,
preictal, ictal and postictal states. In this work, we introduce a graph-based
model of the brain interactions developed to study synchronization patterns in
the electroencephalogram (EEG) signals. The aim is to develop a
patient-specific approach, also for a real-time use, for the prediction of
epileptic seizures' occurrences. Different synchronization measures of the EEG
signals and easily computable functions able to capture in real-time the
variations of EEG synchronization have been considered. Both standard and
ad-hoc classification algorithms have been developed and used. Results on scalp
EEG signals show that this simple and computationally viable processing is able
to highlight the changes in the synchronization corresponding to the preictal
state.
"
Elongation and shape changes in organisms with cell walls: a dialogue between experiments and models,"  The generation of anisotropic shapes occurs during morphogenesis of almost
all organisms. With the recent renewal of the interest in mechanical aspects of
morphogenesis, it has become clear that mechanics contributes to anisotropic
forms in a subtle interaction with various molecular actors. Here, we consider
plants, fungi, oomycetes, and bacteria, and we review the mechanisms by which
elongated shapes are generated and maintained. We focus on theoretical models
of the interplay between growth and mechanics, in relation with experimental
data, and discuss how models may help us improve our understanding of the
underlying biological mechanisms.
"
Replicator equation on networks with degree regular communities,"  The replicator equation is one of the fundamental tools to study evolutionary
dynamics in well-mixed populations. This paper contributes to the literature on
evolutionary graph theory, providing a version of the replicator equation for a
family of connected networks with communities, where nodes in the same
community have the same degree. This replicator equation is applied to the
study of different classes of games, exploring the impact of the graph
structure on the equilibria of the evolutionary dynamics.
"
Substrate inhibition imposes fitness penalty at high protein stability,"  Proteins are only moderately stable. It has long been debated whether this
narrow range of stabilities is solely a result of neutral drift towards lower
stability or purifying selection against excess stability is also at work - for
which no experimental evidence was found so far. Here we show that mutations
outside the active site in the essential E. coli enzyme adenylate kinase result
in stability-dependent increase in substrate inhibition by AMP, thereby
impairing overall enzyme activity at high stability. Such inhibition caused
substantial fitness defects not only in the presence of excess substrate but
also under physiological conditions. In the latter case, substrate inhibition
caused differential accumulation of AMP in the stationary phase for the
inhibition prone mutants. Further, we show that changes in flux through Adk
could accurately describe the variation in fitness effects. Taken together,
these data suggest that selection against substrate inhibition and hence excess
stability may have resulted in a narrow range of optimal stability observed for
modern proteins.
"
Gradient Hyperalignment for multi-subject fMRI data alignment,"  Multi-subject fMRI data analysis is an interesting and challenging problem in
human brain decoding studies. The inherent anatomical and functional
variability across subjects make it necessary to do both anatomical and
functional alignment before classification analysis. Besides, when it comes to
big data, time complexity becomes a problem that cannot be ignored. This paper
proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional
alignment method that is suitable for multi-subject fMRI datasets with large
amounts of samples and voxels. The advantage of Gradient-HA is that it can
solve independence and high dimension problems by using Independent Component
Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using
multi-classification tasks on big data demonstrates that Gradient-HA method has
less time complexity and better or comparable performance compared with other
state-of-the-art functional alignment methods.
"
Robustness of functional networks at criticality against structural defects,"  The robustness of dynamical properties of neuronal networks against
structural damages is a central problem in computational and experimental
neuroscience. Research has shown that the cortical network of a healthy brain
works near a critical state, and moreover, that functional neuronal networks
often have scale-free and small-world properties. In this work, we study how
the robustness of simple functional networks at criticality is affected by
structural defects. In particular, we consider a 2D Ising model at the critical
temperature and investigate how its functional network changes with the
increasing degree of structural defects. We show that the scale-free and
small-world properties of the functional network at criticality are robust
against large degrees of structural lesions while the system remains below the
percolation limit. Although the Ising model is only a conceptual description of
a two-state neuron, our research reveals fundamental robustness properties of
functional networks derived from classical statistical mechanics models.
"
Spatial interactions and oscillatory tragedies of the commons,"  A tragedy of the commons (TOC) occurs when individuals acting in their own
self-interest deplete commonly-held resources, leading to a worse outcome than
had they cooperated. Over time, the depletion of resources can change
incentives for subsequent actions. Here, we investigate long-term feedback
between game and environment across a continuum of incentives in an
individual-based framework. We identify payoff-dependent transition rules that
lead to oscillatory TOC-s in stochastic simulations and the mean field limit.
Further extending the stochastic model, we find that spatially explicit
interactions can lead to emergent, localized dynamics, including the
propagation of cooperative wave fronts and cluster formation of both social
context and resources. These dynamics suggest new mechanisms underlying how
TOCs arise and how they might be averted.
"
Optimal Evidence Accumulation on Social Networks,"  A fundamental question in biology is how organisms integrate sensory and
social evidence to make decisions. However, few models describe how both these
streams of information can be combined to optimize choices. Here we develop a
normative model for collective decision making in a network of agents
performing a two-alternative forced choice task. We assume that rational
(Bayesian) agents in this network make private measurements, and observe the
decisions of their neighbors until they accumulate sufficient evidence to make
an irreversible choice. As each agent communicates its decision to those
observing it, the flow of social information is described by a directed graph.
The decision-making process in this setting is intuitive, but can be complex.
We describe when and how the absence of a decision of a neighboring agent
communicates social information, and how an agent must marginalize over all
unobserved decisions. We also show how decision thresholds and network
connectivity affect group evidence accumulation, and describe the dynamics of
decision making in social cliques. Our model provides a bridge between the
abstractions used in the economics literature and the evidence accumulator
models used widely in neuroscience and psychology.
"
Temperature-dependent non-covalent protein-protein interactions explain normal and inverted solubility in a mutant of human gamma D-crystallin,"  Protein crystal production is a major bottleneck for the structural
characterisation of proteins. To advance beyond large-scale screening, rational
strategies for protein crystallization are crucial. Understanding how chemical
anisotropy (or patchiness) of the protein surface due to the variety of amino
acid side chains in contact with solvent, contributes to protein protein
contact formation in the crystal lattice is a major obstacle to predicting and
optimising crystallization. The relative scarcity of sophisticated theoretical
models that include sufficient detail to link collective behaviour, captured in
protein phase diagrams, and molecular level details, determined from
high-resolution structural information is a further barrier. Here we present
two crystals structures for the P23TR36S mutant of gamma D-crystallin, each
with opposite solubility behaviour, one melts when heated, the other when
cooled. When combined with the protein phase diagram and a tailored patchy
particle model we show that a single temperature dependent interaction is
sufficient to stabilise the inverted solubility crystal. This contact, at the
P23T substitution site, relates to a genetic cataract and reveals at a
molecular level, the origin of the lowered and retrograde solubility of the
protein. Our results show that the approach employed here may present an
alternative strategy for the rationalization of protein crystallization.
"
Fréchet ChemNet Distance: A metric for generative models for molecules in drug discovery,"  The new wave of successful generative models in machine learning has
increased the interest in deep learning driven de novo drug design. However,
assessing the performance of such generative models is notoriously difficult.
Metrics that are typically used to assess the performance of such generative
models are the percentage of chemically valid molecules or the similarity to
real molecules in terms of particular descriptors, such as the partition
coefficient (logP) or druglikeness. However, method comparison is difficult
because of the inconsistent use of evaluation metrics, the necessity for
multiple metrics, and the fact that some of these measures can easily be
tricked by simple rule-based systems. We propose a novel distance measure
between two sets of molecules, called Fréchet ChemNet distance (FCD), that
can be used as an evaluation metric for generative models. The FCD is similar
to a recently established performance metric for comparing image generation
methods, the Fréchet Inception Distance (FID). Whereas the FID uses one of
the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a
deep neural network called ChemNet, which was trained to predict drug
activities. Thus, the FCD metric takes into account chemically and biologically
relevant information about molecules, and also measures the diversity of the
set via the distribution of generated molecules. The FCD's advantage over
previous metrics is that it can detect if generated molecules are a) diverse
and have similar b) chemical and c) biological properties as real molecules. We
further provide an easy-to-use implementation that only requires the SMILES
representation of the generated molecules as input to calculate the FCD.
Implementations are available at: this https URL
"
TADPOLE Challenge: Prediction of Longitudinal Evolution in Alzheimer's Disease,"  The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)
Challenge compares the performance of algorithms at predicting future evolution
of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants
train their models and algorithms on historical data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI) study or any other datasets to which
they have access. Participants are then required to make monthly forecasts over
a period of 5 years from January 2018, of three key outcomes for ADNI-3
rollover participants: clinical diagnosis, Alzheimer's Disease Assessment Scale
Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. These
individual forecasts are later compared with the corresponding future
measurements in ADNI-3 (obtained after the TADPOLE submission deadline). The
first submission phase of TADPOLE was open for prize-eligible submissions
between 15 June and 15 November 2017. The submission system remains open via
the website: this https URL, although since 15 November
2017 submissions are not eligible for the first round of prizes. This paper
describes the design of the TADPOLE Challenge.
"
Parallel G-duplex and C-duplex DNA with Uninterrupted Spines of AgI-Mediated Base Pairs,"  Hydrogen bonding between nucleobases produces diverse DNA structural motifs,
including canonical duplexes, guanine (G) quadruplexes and cytosine (C)
i-motifs. Incorporating metal-mediated base pairs into nucleic acid structures
can introduce new functionalities and enhanced stabilities. Here we
demonstrate, using mass spectrometry (MS), ion mobility spectrometry (IMS) and
fluorescence resonance energy transfer (FRET), that parallel-stranded
structures consisting of up to 20 G-Ag(I)-G contiguous base pairs are formed
when natural DNA sequences are mixed with silver cations in aqueous solution.
FRET indicates that duplexes formed by poly(cytosine) strands with 20
contiguous C-Ag(I)-C base pairs are also parallel. Silver-mediated G-duplexes
form preferentially over G-quadruplexes, and the ability of Ag+ to convert
G-quadruplexes into silver-paired duplexes may provide a new route to
manipulating these biologically relevant structures. IMS indicates that
G-duplexes are linear and more rigid than B-DNA. DFT calculations were used to
propose structures compatible with the IMS experiments. Such inexpensive,
defect-free and soluble DNA-based nanowires open new directions in the design
of novel metal-mediated DNA nanotechnology.
"
A single coordinate framework for optic flow and binocular disparity,"  Optic flow is two dimensional, but no special qualities are attached to one
or other of these dimensions. For binocular disparity, on the other hand, the
terms 'horizontal' and 'vertical' disparities are commonly used. This is odd,
since binocular disparity and optic flow describe essentially the same thing.
The difference is that, generally, people tend to fixate relatively close to
the direction of heading as they move, meaning that fixation is close to the
optic flow epipole, whereas, for binocular vision, fixation is close to the
head-centric midline, i.e. approximately 90 degrees from the binocular epipole.
For fixating animals, some separations of flow may lead to simple algorithms
for the judgement of surface structure and the control of action. We consider
the following canonical flow patterns that sum to produce overall flow: (i)
'towards' flow, the component of translational flow produced by approaching (or
retreating from) the fixated object, which produces pure radial flow on the
retina; (ii) 'sideways' flow, the remaining component of translational flow,
which is produced by translation of the optic centre orthogonal to the
cyclopean line of sight and (iii) 'vergence' flow, rotational flow produced by
a counter-rotation of the eye in order to maintain fixation. A general flow
pattern could also include (iv) 'cyclovergence' flow, produced by rotation of
one eye relative to the other about the line of sight. We consider some
practical advantages of dividing up flow in this way when an observer fixates
as they move. As in some previous treatments, we suggest that there are certain
tasks for which it is sensible to consider 'towards' flow as one component and
'sideways' + 'vergence' flow as another.
"
Gain control with A-type potassium current: IA as a switch between divisive and subtractive inhibition,"  Neurons process information by transforming barrages of synaptic inputs into
spiking activity. Synaptic inhibition suppresses the output firing activity of
a neuron, and is commonly classified as having a subtractive or divisive effect
on a neuron's output firing activity. Subtractive inhibition can narrow the
range of inputs that evoke spiking activity by eliminating responses to
non-preferred inputs. Divisive inhibition is a form of gain control: it
modifies firing rates while preserving the range of inputs that evoke firing
activity. Since these two ""modes"" of inhibition have distinct impacts on neural
coding, it is important to understand the biophysical mechanisms that
distinguish these response profiles.
We use simulations and mathematical analysis of a neuron model to find the
specific conditions for which inhibitory inputs have subtractive or divisive
effects. We identify a novel role for the A-type Potassium current (IA). In our
model, this fast-activating, slowly- inactivating outward current acts as a
switch between subtractive and divisive inhibition. If IA is strong (large
maximal conductance) and fast (activates on a time-scale similar to spike
initiation), then inhibition has a subtractive effect on neural firing. In
contrast, if IA is weak or insufficiently fast-activating, then inhibition has
a divisive effect on neural firing. We explain these findings using dynamical
systems methods to define how a spike threshold condition depends on synaptic
inputs and IA.
Our findings suggest that neurons can ""self-regulate"" the gain control
effects of inhibition via combinations of synaptic plasticity and/or modulation
of the conductance and kinetics of A-type Potassium channels. This novel role
for IA would add flexibility to neurons and networks, and may relate to recent
observations of divisive inhibitory effects on neurons in the nucleus of the
solitary tract.
"
"MEG-Derived Functional Tractography, Results for Normal and Concussed Cohorts","  Measures of neuroelectric activity from each of 18 automatically identified
white matter tracts were extracted from resting MEG recordings from a
normative, n=588, and a chronic TBI, traumatic brain injury, n=63, cohort, 60
of whose TBIs were mild. Activity in the TBI cohort was significantly reduced
compared with the norms for ten of the tracts, p < 10-6 for each. Significantly
reduced activity (p < 10-3) was seen in more than one tract in seven mTBI
individuals and one member of the normative cohort.
"
In-Silico Proportional-Integral Moment Control of Stochastic Reaction Networks with Applications to Gene Expression (with Dimerization),"  The problem of controlling the mean and the variance of a species of interest
in a simple gene expression is addressed. It is shown that the protein mean
level can be globally and robustly tracked to any desired value using a simple
PI controller that satisfies certain sufficient conditions. Controlling both
the mean and variance however requires an additional control input, e.g. the
mRNA degradation rate, and local robust tracking of mean and variance is proved
to be achievable using multivariable PI control, provided that the reference
point satisfies necessary conditions imposed by the system. Even more
importantly, it is shown that there exist PI controllers that locally, robustly
and simultaneously stabilize all the equilibrium points inside the admissible
region. The results are then extended to the mean control of a gene expression
with protein dimerization. It is shown that the moment closure problem can be
circumvented without invoking any moment closure technique. Local stabilization
and convergence of the average dimer population to any desired reference value
is ensured using a pure integral control law. Explicit bounds on the controller
gain are provided and shown to be valid for any reference value. As a
byproduct, an explicit upper-bound of the variance of the monomer species,
acting on the system as unknown input due to the moment openness, is obtained.
The results are illustrated by simulation.
"
Experimental Tests of Spirituality,"  We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth.
"
From Neuronal Models to Neuronal Dynamics and Image Processing,"  This paper is an introduction to the membrane potential equation for neurons.
Its properties are described, as well as sample applications. Networks of these
equations can be used for modeling neuronal systems, which also process images
and video sequences, respectively. Specifically, (i) a dynamic retina is
proposed (based on a reaction-diffusion system), which predicts afterimages and
simple visual illusions, (ii) a system for texture segregation (texture
elements are understood as even-symmetric contrast features), and (iii) a
network for detecting object approaches (inspired by the locust visual system).
"
"AMPA, NMDA and GABAA receptor mediated network burst dynamics in cortical cultures in vitro","  In this work we study the excitatory AMPA, and NMDA, and inhibitory GABAA
receptor mediated dynamical changes in neuronal networks of neonatal rat cortex
in vitro. Extracellular network-wide activity was recorded with 59 planar
electrodes simultaneously under different pharmacological conditions. We
analyzed the changes of overall network activity and network-wide burst
frequency between baseline and AMPA receptor (AMPA-R) or NMDA receptor (NMDA-R)
driven activity, as well as between the latter states and disinhibited
activity. Additionally, spatiotemporal structures of pharmacologically modified
bursts and recruitment of electrodes during the network bursts were studied.
Our results show that AMPA-R and NMDA-R receptors have clearly distinct roles
in network dynamics. AMPA-Rs are in greater charge to initiate network wide
bursts. Therefore NMDA-Rs maintain the already initiated activity. GABAA
receptors (GABAA-Rs) inhibit AMPA-R driven network activity more strongly than
NMDA-R driven activity during the bursts.
"
The PomXYZ Proteins Self-Organize on the Bacterial Nucleoid to Stimulate Cell Division,"  Cell division site positioning is precisely regulated to generate correctly
sized and shaped daughters. We uncover a novel strategy to position the FtsZ
cytokinetic ring at midcell in the social bacterium Myxococcus xanthus. PomX,
PomY and the nucleoid-binding ParA/MinD ATPase PomZ self-assemble forming a
large nucleoid-associated complex that localizes at the division site before
FtsZ to directly guide and stimulate division. PomXYZ localization is generated
through self-organized biased random motion on the nucleoid towards midcell and
constrained motion at midcell. Experiments and theory show that PomXYZ motion
is produced by diffusive PomZ fluxes on the nucleoid into the complex. Flux
differences scale with the intracellular asymmetry of the complex and are
converted into a local PomZ concentration gradient across the complex with
translocation towards the higher PomZ concentration. At midcell, fluxes
equalize resulting in constrained motion. Flux-based mechanisms may represent a
general paradigm for positioning of macromolecular structures in bacteria.
"
Beta-rhythm oscillations and synchronization transition in network models of Izhikevich neurons: effect of topology and synaptic type,"  Despite their significant functional roles, beta-band oscillations are least
understood. Synchronization in neuronal networks have attracted much attention
in recent years with the main focus on transition type. Whether one obtains
explosive transition or a continuous transition is an important feature of the
neuronal network which can depend on network structure as well as synaptic
types. In this study we consider the effect of synaptic interaction (electrical
and chemical) as well as structural connectivity on synchronization transition
in network models of Izhikevich neurons which spike regularly with beta
rhythms. We find a wide range of behavior including continuous transition,
explosive transition, as well as lack of global order. The stronger electrical
synapses are more conducive to synchronization and can even lead to explosive
synchronization. The key network element which determines the order of
transition is found to be the clustering coefficient and not the small world
effect, or the existence of hubs in a network. These results are in contrast to
previous results which use phase oscillator models such as the Kuramoto model.
Furthermore, we show that the patterns of synchronization changes when one goes
to the gamma band. We attribute such a change to the change in the refractory
period of Izhikevich neurons which changes significantly with frequency.
"
Towards a Science of Mind,"  The ancient mind/body problem continues to be one of deepest mysteries of
science and of the human spirit. Despite major advances in many fields, there
is still no plausible link between subjective experience (qualia) and its
realization in the body. This paper outlines some of the elements of a rigorous
science of mind (SoM) - key ideas include scientific realism of mind, agnostic
mysterianism, careful attention to language, and a focus on concrete
(touchstone) questions and results.
"
A metric model for the functional architecture of the visual cortex,"  The purpose of this work is to construct a model for the functional
architecture of the primary visual cortex (V1), based on a structure of metric
measure space induced by the underlying organization of receptive profiles
(RPs) of visual cells. In order to account for the horizontal connectivity of
V1 in such a context, a diffusion process compatible with the geometry of the
space is defined following the classical approach of K.-T. Sturm. The
construction of our distance function does neither require any group
parameterization of the family of RPs, nor involve any differential structure.
As such, it adapts to non-parameterized sets of RPs, possibly obtained through
numerical procedures; it also allows to model the lateral connectivity arising
from non-differential metrics such as the one induced on a pinwheel surface by
a family of filters of vanishing scale. On the other hand, when applied to the
classical framework of Gabor filters, this construction yields a distance
approximating the sub-Riemannian structure proposed as a model for V1 by G.
Citti and A. Sarti [J Math Imaging Vis 24: 307 (2006)], thus showing itself to
be consistent with existing cortex models.
"
Discovering the effect of nonlocal payoff calculation on the stabilty of ESS: Spatial patterns of Hawk-Dove game in metapopulations,"  The classical idea of evolutionarily stable strategy (ESS) modeling animal
behavior does not involve any spatial dependence. We considered a spatial
Hawk-Dove game played by animals in a patchy environment with wrap around
boundaries. We posit that each site contains the same number of individuals. An
evolution equation for analyzing the stability of the ESS is found as the mean
dynamics of the classical frequency dependent Moran process coupled via
migration and nonlocal payoff calculation in 1D and 2D habitats. The linear
stability analysis of the model is performed and conditions to observe spatial
patterns are investigated. For the nearest neighbor interactions (including von
Neumann and Moore neighborhoods in 2D) we concluded that it is possible to
destabilize the ESS of the game and observe pattern formation when the
dispersal rate is small enough. We numerically investigate the spatial patterns
arising from the replicator equations coupled via nearest neighbor payoff
calculation and dispersal.
"
Bioinformatics and Medicine in the Era of Deep Learning,"  Many of the current scientific advances in the life sciences have their
origin in the intensive use of data for knowledge discovery. In no area this is
so clear as in bioinformatics, led by technological breakthroughs in data
acquisition technologies. It has been argued that bioinformatics could quickly
become the field of research generating the largest data repositories, beating
other data-intensive areas such as high-energy physics or astroinformatics.
Over the last decade, deep learning has become a disruptive advance in machine
learning, giving new live to the long-standing connectionist paradigm in
artificial intelligence. Deep learning methods are ideally suited to
large-scale data and, therefore, they should be ideally suited to knowledge
discovery in bioinformatics and biomedicine at large. In this brief paper, we
review key aspects of the application of deep learning in bioinformatics and
medicine, drawing from the themes covered by the contributions to an ESANN 2018
special session devoted to this topic.
"
An integration of fast alignment and maximum-likelihood methods for electron subtomogram averaging and classification,"  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging
technique that visualizes subcellular organization of single cells at
submolecular resolution and in near-native state. CECT captures large numbers
of macromolecular complexes of highly diverse structures and abundances.
However, the structural complexity and imaging limits complicate the systematic
de novo structural recovery and recognition of these macromolecular complexes.
Efficient and accurate reference-free subtomogram averaging and classification
represent the most critical tasks for such analysis. Existing subtomogram
alignment based methods are prone to the missing wedge effects and low
signal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based
methods rely on integration operations, which are in principle computationally
infeasible for accurate calculation.
Results: Built on existing works, we propose an integrated method, Fast
Alignment Maximum Likelihood method (FAML), which uses fast subtomogram
alignment to sample sub-optimal rigid transformations. The transformations are
then used to approximate integrals for maximum-likelihood update of subtomogram
averages through expectation-maximization algorithm. Our tests on simulated and
experimental subtomograms showed that, compared to our previously developed
fast alignment method (FA), FAML is significantly more robust to noise and
missing wedge effects with moderate increases of computation cost.Besides, FAML
performs well with significantly fewer input subtomograms when the FA method
fails. Therefore, FAML can serve as a key component for improved construction
of initial structural models from macromolecules captured by CECT.
"
Phase-type distributions in population genetics,"  Probability modelling for DNA sequence evolution is well established and
provides a rich framework for understanding genetic variation between samples
of individuals from one or more populations. We show that both classical and
more recent models for coalescence (with or without recombination) can be
described in terms of the so-called phase-type theory, where complicated and
tedious calculations are circumvented by the use of matrices. The application
of phase-type theory consists of describing the stochastic model as a Markov
model by appropriately setting up a state space and calculating the
corresponding intensity and reward matrices. Formulae of interest are then
expressed in terms of these aforementioned matrices. We illustrate this by a
few examples calculating the mean, variance and even higher order moments of
the site frequency spectrum in the multiple merger coalescent models, and by
analysing the mean and variance for the number of segregating sites for
multiple samples in the two-locus ancestral recombination graph. We believe
that phase-type theory has great potential as a tool for analysing probability
models in population genetics. The compact matrix notation is useful for
clarification of current models, in particular their formal manipulation
(calculation), but also for further development or extensions.
"
