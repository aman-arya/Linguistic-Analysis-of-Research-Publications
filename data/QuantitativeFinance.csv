Title,Abstract
Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs,"  In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
"
High Dimensional Estimation and Multi-Factor Models,"  This paper re-investigates the estimation of multiple factor models relaxing
the convention that the number of factors is small and using a new approach for
identifying factors. We first obtain the collection of all possible factors and
then provide a simultaneous test, security by security, of which factors are
significant. Since the collection of risk factors is large and highly
correlated, high-dimension methods (including the LASSO and prototype
clustering) have to be used. The multi-factor model is shown to have a
significantly better fit than the Fama-French 5-factor model. Robustness tests
are also provided.
"
An Expanded Local Variance Gamma model,"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
"
Psychological model of the investor and manager behavior in risk,"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
"
Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency,"  This paper considers a time-inconsistent stopping problem in which the
inconsistency arises from non-constant time preference rates. We show that the
smooth pasting principle, the main approach that has been used to construct
explicit solutions for conventional time-consistent optimal stopping problems,
may fail under time-inconsistency. Specifically, we prove that the smooth
pasting principle solves a time-inconsistent problem within the intra-personal
game theoretic framework if and only if a certain inequality on the model
primitives is satisfied. We show that the violation of this inequality can
happen even for very simple non-exponential discount functions. Moreover, we
demonstrate that the stopping problem does not admit any intra-personal
equilibrium whenever the smooth pasting principle fails. The ""negative"" results
in this paper caution blindly extending the classical approaches for
time-consistent stopping problems to their time-inconsistent counterparts.
"
Cryptoasset Factor Models,"  We propose factor models for the cross-section of daily cryptoasset returns
and provide source code for data downloads, computing risk factors and
backtesting them out-of-sample. In ""cryptoassets"" we include all
cryptocurrencies and a host of various other digital assets (coins and tokens)
for which exchange market data is available. Based on our empirical analysis,
we identify the leading factor that appears to strongly contribute into daily
cryptoasset returns. Our results suggest that cross-sectional statistical
arbitrage trading may be possible for cryptoassets subject to efficient
executions and shorting.
"
Exploring the predictability of range-based volatility estimators using RNNs,"  We investigate the predictability of several range-based stock volatility
estimators, and compare them to the standard close-to-close estimator which is
most commonly acknowledged as the volatility. The patterns of volatility
changes are analyzed using LSTM recurrent neural networks, which are a state of
the art method of sequence learning. We implement the analysis on all current
constituents of the Dow Jones Industrial Average index, and report averaged
evaluation results. We find that changes in the values of range-based
estimators are more predictable than that of the estimator using daily closing
values only.
"
"Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers","  Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
"
Affine Rough Models,"  The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
"
A Game of Tax Evasion: evidences from an agent-based model,"  This paper presents a simple agent-based model of an economic system,
populated by agents playing different games according to their different view
about social cohesion and tax payment. After a first set of simulations,
correctly replicating results of existing literature, a wider analysis is
presented in order to study the effects of a dynamic-adaptation rule, in which
citizens may possibly decide to modify their individual tax compliance
according to individual criteria, such as, the strength of their ethical
commitment, the satisfaction gained by consumption of the public good and the
perceived opinion of neighbors. Results show the presence of thresholds levels
in the composition of society - between taxpayers and evaders - which explain
the extent of damages deriving from tax evasion.
"
Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis,"  Interbank markets are often characterised in terms of a core-periphery
network structure, with a highly interconnected core of banks holding the
market together, and a periphery of banks connected mostly to the core but not
internally. This paradigm has recently been challenged for short time scales,
where interbank markets seem better characterised by a bipartite structure with
more core-periphery connections than inside the core. Using a novel
core-periphery detection method on the eMID interbank market, we enrich this
picture by showing that the network is actually characterised by multiple
core-periphery pairs. Moreover, a transition from core-periphery to bipartite
structures occurs by shortening the temporal scale of data aggregation. We
further show how the global financial crisis transformed the market, in terms
of composition, multiplicity and internal organisation of core-periphery pairs.
By unveiling such a fine-grained organisation and transformation of the
interbank market, our method can find important applications in the
understanding of how distress can propagate over financial networks.
"
On the complexity of solving a decision problem with flow-depending costs: the case of the IJsselmeer dikes,"  We consider a fundamental integer programming (IP) model for cost-benefit
analysis flood protection through dike building in the Netherlands, due to
Verweij and Zwaneveld.
Experimental analysis with data for the Ijsselmeer lead to integral optimal
solution of the linear programming relaxation of the IP model.
This naturally led to the question of integrality of the polytope associated
with the IP model.
In this paper we first give a negative answer to this question by
establishing non-integrality of the polytope.
Second, we establish natural conditions that guarantee the linear programming
relaxation of the IP model to be integral.
We then test the most recent data on flood probabilities, damage and
investment costs of the IJsselmeer for these conditions.
Third, we show that the IP model can be solved in polynomial time when the
number of dike segments, or the number of feasible barrier heights, are
constant.
"
Mining Illegal Insider Trading of Stocks: A Proactive Approach,"  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
"
Mean-variance portfolio selection under partial information with drift uncertainty,"  This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
"
Generalizing Geometric Brownian Motion,"  To convert standard Brownian motion $Z$ into a positive process, Geometric
Brownian motion (GBM) $e^{\beta Z_t}, \beta >0$ is widely used. We generalize
this positive process by introducing an asymmetry parameter $ \alpha \geq 0$
which describes the instantaneous volatility whenever the process reaches a new
low. For our new process, $\beta$ is the instantaneous volatility as prices
become arbitrarily high. Our generalization preserves the positivity, constant
proportional drift, and tractability of GBM, while expressing the instantaneous
volatility as a randomly weighted $L^2$ mean of $\alpha$ and $\beta$. The
running minimum and relative drawup of this process are also analytically
tractable. Letting $\alpha = \beta$, our positive process reduces to Geometric
Brownian motion. By adding a jump to default to the new process, we introduce a
non-negative martingale with the same tractabilities. Assuming a security's
dynamics are driven by these processes in risk neutral measure, we price
several derivatives including vanilla, barrier and lookback options.
"
Pricing options and computing implied volatilities using neural networks,"  This paper proposes a data-driven approach, by means of an Artificial Neural
Network (ANN), to value financial options and to calculate implied volatilities
with the aim of accelerating the corresponding numerical methods. With ANNs
being universal function approximators, this method trains an optimized ANN on
a data set generated by a sophisticated financial model, and runs the trained
ANN as an agent of the original solver in a fast and efficient way. We test
this approach on three different types of solvers, including the analytic
solution for the Black-Scholes equation, the COS method for the Heston
stochastic volatility model and Brent's iterative root-finding method for the
calculation of implied volatilities. The numerical results show that the ANN
solver can reduce the computing time significantly.
"
Social Network based Short-Term Stock Trading System,"  This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.
"
Identification of Conduit Countries and Community Structures in the Withholding Tax Networks,"  Due to economic globalization, each country's economic law, including tax
laws and tax treaties, has been forced to work as a single network. However,
each jurisdiction (country or region) has not made its economic law under the
assumption that its law functions as an element of one network, so it has
brought unexpected results. We thought that the results are exactly
international tax avoidance. To contribute to the solution of international tax
avoidance, we tried to investigate which part of the network is vulnerable.
Specifically, focusing on treaty shopping, which is one of international tax
avoidance methods, we attempt to identified which jurisdiction are likely to be
used for treaty shopping from tax liabilities and the relationship between
jurisdictions which are likely to be used for treaty shopping and others. For
that purpose, based on withholding tax rates imposed on dividends, interest,
and royalties by jurisdictions, we produced weighted multiple directed graphs,
computed the centralities and detected the communities. As a result, we
clarified the jurisdictions that are likely to be used for treaty shopping and
pointed out that there are community structures. The results of this study
suggested that fewer jurisdictions need to introduce more regulations for
prevention of treaty abuse worldwide.
"
International crop trade networks: The impact of shocks and cascades,"  Analyzing available FAO data from 176 countries over 21 years, we observe an
increase of complexity in the international trade of maize, rice, soy, and
wheat. A larger number of countries play a role as producers or intermediaries,
either for trade or food processing. In consequence, we find that the trade
networks become more prone to failure cascades caused by exogenous shocks. In
our model, countries compensate for demand deficits by imposing export
restrictions. To capture these, we construct higher-order trade dependency
networks for the different crops and years. These networks reveal hidden
dependencies between countries and allow to discuss policy implications.
"
Dynamical regularities of US equities opening and closing auctions,"  We first investigate the evolution of opening and closing auctions volumes of
US equities along the years. We then report dynamical properties of pre-auction
periods: the indicative match price is strongly mean-reverting because the
imbalance is; the final auction price reacts to a single auction order
placement or cancellation in markedly different ways in the opening and closing
auctions when computed conditionally on imbalance improving or worsening
events; the indicative price reverts towards the mid price of the regular limit
order book but is not especially bound to the spread.
"
Quantum Structures in Human Decision-making: Towards Quantum Expected Utility,"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
"
Transition probability of Brownian motion in the octant and its application to default modeling,"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
"
The Wisdom of a Kalman Crowd,"  The Kalman Filter has been called one of the greatest inventions in
statistics during the 20th century. Its purpose is to measure the state of a
system by processing the noisy data received from different electronic sensors.
In comparison, a useful resource for managers in their effort to make the right
decisions is the wisdom of crowds. This phenomenon allows managers to combine
judgments by different employees to get estimates that are often more accurate
and reliable than estimates, which managers produce alone. Since harnessing the
collective intelligence of employees, and filtering signals from multiple noisy
sensors appear related, we looked at the possibility of using the Kalman Filter
on estimates by people. Our predictions suggest, and our findings based on the
Survey of Professional Forecasters reveal, that the Kalman Filter can help
managers solve their decision-making problems by giving them stronger signals
before they choose. Indeed, when used on a subset of forecasters identified by
the Contribution Weighted Model, the Kalman Filter beat that rule clearly,
across all the forecasting horizons in the survey.
"
Status maximization as a source of fairness in a networked dictator game,"  Human behavioural patterns exhibit selfish or competitive, as well as
selfless or altruistic tendencies, both of which have demonstrable effects on
human social and economic activity. In behavioural economics, such effects have
traditionally been illustrated experimentally via simple games like the
dictator and ultimatum games. Experiments with these games suggest that, beyond
rational economic thinking, human decision-making processes are influenced by
social preferences, such as an inclination to fairness. In this study we
suggest that the apparent gap between competitive and altruistic human
tendencies can be bridged by assuming that people are primarily maximising
their status, i.e., a utility function different from simple profit
maximisation. To this end we analyse a simple agent-based model, where
individuals play the repeated dictator game in a social network they can
modify. As model parameters we consider the living costs and the rate at which
agents forget infractions by others. We find that individual strategies used in
the game vary greatly, from selfish to selfless, and that both of the above
parameters determine when individuals form complex and cohesive social
networks.
"
Affine forward variance models,"  We introduce the class of affine forward variance (AFV) models of which both
the conventional Heston model and the rough Heston model are special cases. We
show that AFV models can be characterized by the affine form of their cumulant
generating function, which can be obtained as solution of a convolution Riccati
equation. We further introduce the class of affine forward order flow intensity
(AFI) models, which are structurally similar to AFV models, but driven by jump
processes, and which include Hawkes-type models. We show that the cumulant
generating function of an AFI model satisfies a generalized convolution Riccati
equation and that a high-frequency limit of AFI models converges in
distribution to the AFV model.
"
"The role of industry, occupation, and location specific knowledge in the survival of new firms","  How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.
"
The Broad Consequences of Narrow Banking,"  We investigate the macroeconomic consequences of narrow banking in the
context of stock-flow consistent models. We begin with an extension of the
Goodwin-Keen model incorporating time deposits, government bills, cash, and
central bank reserves to the base model with loans and demand deposits and use
it to describe a fractional reserve banking system. We then characterize narrow
banking by a full reserve requirement on demand deposits and describe the
resulting separation between the payment system and lending functions of the
resulting banking sector. By way of numerical examples, we explore the
properties of fractional and full reserve versions of the model and compare
their asymptotic properties. We find that narrow banking does not lead to any
loss in economic growth when the models converge to a finite equilibrium, while
allowing for more direct monitoring and prevention of financial breakdowns in
the case of explosive asymptotic behaviour.
"
Spatial risk measures and rate of spatial diversification,"  An accurate assessment of the risk of extreme environmental events is of
great importance for populations, authorities and the banking/insurance
industry. Koch (2017) introduced a notion of spatial risk measure and a
corresponding set of axioms which are well suited to analyze the risk due to
events having a spatial extent, precisely such as environmental phenomena. The
axiom of asymptotic spatial homogeneity is of particular interest since it
allows one to quantify the rate of spatial diversification when the region
under consideration becomes large. In this paper, we first investigate the
general concepts of spatial risk measures and corresponding axioms further. We
also explain the usefulness of this theory for the actuarial practice. Second,
in the case of a general cost field, we especially give sufficient conditions
such that spatial risk measures associated with expectation, variance,
Value-at-Risk as well as expected shortfall and induced by this cost field
satisfy the axioms of asymptotic spatial homogeneity of order 0, -2, -1 and -1,
respectively. Last but not least, in the case where the cost field is a
function of a max-stable random field, we mainly provide conditions on both the
function and the max-stable field ensuring the latter properties. Max-stable
random fields are relevant when assessing the risk of extreme events since they
appear as a natural extension of multivariate extreme-value theory to the level
of random fields. Overall, this paper improves our understanding of spatial
risk measures as well as of their properties with respect to the space variable
and generalizes many results obtained in Koch (2017).
"
The Rank Effect,"  We decompose returns for portfolios of bottom-ranked, lower-priced assets
relative to the market into rank crossovers and changes in the relative price
of those bottom-ranked assets. This decomposition is general and consistent
with virtually any asset pricing model. Crossovers measure changes in rank and
are smoothly increasing over time, while return fluctuations are driven by
volatile relative price changes. Our results imply that in a closed,
dividend-free market in which the relative price of bottom-ranked assets is
approximately constant, a portfolio of those bottom-ranked assets will
outperform the market portfolio over time. We show that bottom-ranked relative
commodity futures prices have increased only slightly, and confirm the
existence of substantial excess returns predicted by our theory. If these
excess returns did not exist, then top-ranked relative prices would have had to
be much higher in 2018 than those actually observed -- this would imply a
radically different commodity price distribution.
"
Smart TWAP trading in continuous-time equilibria,"  This paper presents a continuous-time equilibrium model of TWAP trading and
liquidity provision in a market with multiple strategic investors with
heterogeneous intraday trading targets. We solve the model in closed-form and
show there are infinitely many equilibria. We compare the competitive
equilibrium with different non-price-taking equilibria. In addition, we show
intraday TWAP benchmarking reduces market liquidity relative to just terminal
trading targets alone. The model is computationally tractable, and we provide a
number of numerical illustrations. An extension to stochastic VWAP targets is
also provided.
"
Extended Reduced-Form Framework for Non-Life Insurance,"  In this paper we propose a general framework for modeling an insurance
claims' information flow in continuous time, by generalizing the reduced-form
framework for credit risk and life insurance. In particular, we assume a
nontrivial dependence structure between the reference filtration and the
insurance internal filtration. We apply these results for pricing non-life
insurance liabilities in hybrid financial and insurance markets, while taking
into account the role of inflation under the benchmark approach. This framework
offers at the same time a general and flexible structure, and explicit and
treatable pricing formula.
"
Estimation of Covariance Matrices for Portfolio Optimization using Gaussian Processes,"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
"
Algorithmic Trading with Fitted Q Iteration and Heston Model,"  We present the use of the fitted Q iteration in algorithmic trading. We show
that the fitted Q iteration helps alleviate the dimension problem that the
basic Q-learning algorithm faces in application to trading. Furthermore, we
introduce a procedure including model fitting and data simulation to enrich
training data as the lack of data is often a problem in realistic application.
We experiment our method on both simulated environment that permits arbitrage
opportunity and real-world environment by using prices of 450 stocks. In the
former environment, the method performs well, implying that our method works in
theory. To perform well in the real-world environment, the agents trained might
require more training (iteration) and more meaningful variables with predictive
value.
"
Aggressive Economic Incentives and Physical Activity: The Role of Choice and Technology Decision Aids,"  Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.
"
Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law and the LPPLS Model,"  We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
"
Aggregating multiple types of complex data in stock market prediction: A model-independent framework,"  The increasing richness in volume, and especially types of data in the
financial domain provides unprecedented opportunities to understand the stock
market more comprehensively and makes the price prediction more accurate than
before. However, they also bring challenges to classic statistic approaches
since those models might be constrained to a certain type of data. Aiming at
aggregating differently sourced information and offering type-free capability
to existing models, a framework for predicting stock market of scenarios with
mixed data, including scalar data, compositional data (pie-like) and functional
data (curve-like), is established. The presented framework is
model-independent, as it serves like an interface to multiple types of data and
can be combined with various prediction models. And it is proved to be
effective through numerical simulations. Regarding to price prediction, we
incorporate the trading volume (scalar data), intraday return series
(functional data), and investors' emotions from social media (compositional
data) through the framework to competently forecast whether the market goes up
or down at opening in the next day. The strong explanatory power of the
framework is further demonstrated. Specifically, it is found that the intraday
returns impact the following opening prices differently between bearish market
and bullish market. And it is not at the beginning of the bearish market but
the subsequent period in which the investors' ""fear"" comes to be indicative.
The framework would help extend existing prediction models easily to scenarios
with multiple types of data and shed light on a more systemic understanding of
the stock market.
"
Exploring the Interconnectedness of Cryptocurrencies using Correlation Networks,"  Correlation networks were used to detect characteristics which, although
fixed over time, have an important influence on the evolution of prices over
time. Potentially important features were identified using the websites and
whitepapers of cryptocurrencies with the largest userbases. These were assessed
using two datasets to enhance robustness: one with fourteen cryptocurrencies
beginning from 9 November 2017, and a subset with nine cryptocurrencies
starting 9 September 2016, both ending 6 March 2018. Separately analysing the
subset of cryptocurrencies raised the number of data points from 115 to 537,
and improved robustness to changes in relationships over time. Excluding USD
Tether, the results showed a positive association between different
cryptocurrencies that was statistically significant. Robust, strong positive
associations were observed for six cryptocurrencies where one was a fork of the
other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the
existence of a group of cryptocurrencies particularly associated with Cardano,
and a separate group correlated with Ethereum. The data was not consistent with
a token's functionality or creation mechanism being the dominant determinants
of the evolution of prices over time but did suggest that factors other than
speculation contributed to the price.
"
Limits to Arbitrage in Markets with Stochastic Settlement Latency,"  Distributed ledger technologies rely on consensus protocols confronting
traders with random waiting times until the transfer of ownership is
accomplished. This time-consuming settlement process exposes arbitrageurs to
price risk and imposes limits to arbitrage. We derive theoretical arbitrage
boundaries under general assumptions and show that they increase with expected
latency, latency uncertainty, spot volatility, and risk aversion. Using
high-frequency data from the Bitcoin network, we estimate arbitrage boundaries
due to settlement latency of on average 124 basis points, covering 88 percent
of the observed cross-exchange price differences. Settlement through
decentralized systems thus induces non-trivial frictions affecting market
efficiency and price formation.
"
Efficient Pricing of Barrier Options on High Volatility Assets using Subset Simulation,"  Barrier options are one of the most widely traded exotic options on stock
exchanges. In this paper, we develop a new stochastic simulation method for
pricing barrier options and estimating the corresponding execution
probabilities. We show that the proposed method always outperforms the standard
Monte Carlo approach and becomes substantially more efficient when the
underlying asset has high volatility, while it performs better than multilevel
Monte Carlo for special cases of barrier options and underlying assets. These
theoretical findings are confirmed by numerous simulation results.
"
"Risk-neutral valuation under differential funding costs, defaults and collateralization","  We develop a unified valuation theory that incorporates credit risk
(defaults), collateralization and funding costs, by expanding the replication
approach to a generality that has not yet been studied previously and reaching
valuation when replication is not assumed. This unifying theoretical framework
clarifies the relationship between the two valuation approaches: the adjusted
cash flows approach pioneered for example by Brigo, Pallavicini and co-authors
([12, 13, 34]) and the classic replication approach illustrated for example by
Bielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this
work cover most previous papers where the authors studied specific replication
models.
"
Dynamic Clearing and Contagion in Financial Networks,"  In this paper we will consider a generalized extension of the Eisenberg-Noe
model of financial contagion to allow for time dynamics in both discrete and
continuous time. Derivation and interpretation of the financial implications
will be provided. Emphasis will be placed on the continuous-time framework and
its formulation as a differential equation driven by the operating cash flows.
Mathematical results on existence and uniqueness of firm wealths under the
discrete and continuous-time models will be provided. Finally, the financial
implications of time dynamics will be considered. The focus will be on how the
dynamic clearing solutions differ from those of the static Eisenberg-Noe model.
"
"Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk Forecasting, Incorporating Realized Measures","  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression
model of Taylor (2017) is extended via incorporating a realized measure, to
drive the tail risk dynamics, as a potentially more efficient driver than daily
returns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte
Carlo method are employed for estimation, whose properties are assessed and
compared via a simulation study; results favour the Bayesian approach, which is
subsequently employed in a forecasting study of seven market indices and two
individual assets. The proposed models are compared to a range of parametric,
non-parametric and semi-parametric models, including GARCH, Realized-GARCH and
the joint VaR and ES quantile regression models in Taylor (2017). The
comparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected
Shortfall forecasts, over a long forecast sample period that includes the
global financial crisis in 2007-2008. The results favor the proposed models
incorporating a realized measure, especially when employing the sub-sampled
Realized Variance and the sub-sampled Realized Range.
"
On smile properties of volatility derivatives and exotic products: understanding the VIX skew,"  We develop a method to study the implied volatility for exotic options and
volatility derivatives with European payoffs such as VIX options. Our approach,
based on Malliavin calculus techniques, allows us to describe the properties of
the at-the-money implied volatility (ATMI) in terms of the Malliavin
derivatives of the underlying process. More precisely, we study the short-time
behaviour of the ATMI level and skew. As an application, we describe the
short-term behavior of the ATMI of VIX and realized variance options in terms
of the Hurst parameter of the model, and most importantly we describe the class
of volatility processes that generate a positive skew for the VIX implied
volatility. In addition, we find that our ATMI asymptotic formulae perform very
well even for large maturities. Several numerical examples are provided to
support our theoretical results.
"
Deep Learning for Forecasting Stock Returns in the Cross-Section,"  Many studies have been undertaken by using machine learning techniques,
including neural networks, to predict stock returns. Recently, a method known
as deep learning, which achieves high performance mainly in image recognition
and speech recognition, has attracted attention in the machine learning field.
This paper implements deep learning to predict one-month-ahead stock returns in
the cross-section in the Japanese stock market and investigates the performance
of the method. Our results show that deep neural networks generally outperform
shallow neural networks, and the best networks also outperform representative
machine learning models. These results indicate that deep learning shows
promise as a skillful machine learning method to predict stock returns in the
cross-section.
"
Assessing the state of e-Readiness for Small and Medium Companies in Mexico: a Proposed Taxonomy and Adoption Model,"  Emerging economies frequently show a large component of their Gross Domestic
Product to be dependant on the economic activity of small and medium
enterprises. Nevertheless, e-business solutions are more likely designed for
large companies. SMEs seem to follow a classical family-based management, used
to traditional activities, rather than seeking new ways of adding value to
their business strategy. Thus, a large portion of a nations economy may be at
disadvantage for competition. This paper aims at assessing the state of
e-business readiness of Mexican SMEs based on already published e-business
evolution models and by means of a survey research design. Data is being
collected in three cities with differing sizes and infrastructure conditions.
Statistical results are expected to be presented. A second part of this
research aims at applying classical adoption models to suggest potential causal
relationships, as well as more suitable recommendations for development.
"
Portfolio Construction Matters,"  The role of portfolio construction in the implementation of equity market
neutral factors is often underestimated. Taking the classical momentum strategy
as an example, we show that one can significantly improve the main strategy's
features by properly taking care of this key step. More precisely, an optimized
portfolio construction algorithm allows one to significantly improve the Sharpe
Ratio, reduce sector exposures and volatility fluctuations, and mitigate the
strategy's skewness and tail correlation with the market. These results are
supported by long-term, world-wide simulations and will be shown to be
universal. Our findings are quite general and hold true for a number of other
""equity factors"". Finally, we discuss the details of a more realistic set-up
where we also deal with transaction costs.
"
DeepTriangle: A Deep Learning Approach to Loss Reserving,"  We propose a novel approach for loss reserving based on deep neural networks.
The approach allows for jointly modeling of paid losses and claims outstanding,
and incorporation of heterogenous inputs. We validate the models on loss
reserving data across lines of business, and show that they attain or exceed
the predictive accuracy of existing stochastic methods. The models require
minimal feature engineering and expert input, and can be automated to produce
forecasts at a high frequency.
"
Consistent Inter-Model Specification for Time-Homogeneous SPX Stochastic Volatility and VIX Market Models,"  This paper shows how to recover stochastic volatility models (SVMs) from
market models for the VIX futures term structure. Market models have more
flexibility for fitting of curves than do SVMs, and therefore they are
better-suited for pricing VIX futures and derivatives. But the VIX itself is a
derivative of the S&P500 (SPX) and it is common practice to price SPX
derivatives using an SVM. Hence, a consistent model for both SPX and VIX
derivatives would be one where the SVM is obtained by inverting the market
model. This paper's main result is a method for the recovery of a stochastic
volatility function as the output of an inverse problem, with the inputs given
by a VIX futures market model. Analysis will show that some conditions need to
be met in order for there to not be any inter-model arbitrage or mis-priced
derivatives. Given these conditions the inverse problem can be solved. Several
models are analyzed and explored numerically to gain a better understanding of
the theory and its limitations.
"
Characterization of catastrophic instabilities: Market crashes as paradigm,"  Catastrophic events, though rare, do occur and when they occur, they have
devastating effects. It is, therefore, of utmost importance to understand the
complexity of the underlying dynamics and signatures of catastrophic events,
such as market crashes. For deeper understanding, we choose the US and Japanese
markets from 1985 onward, and study the evolution of the cross-correlation
structures of stock return matrices and their eigenspectra over different short
time-intervals or ""epochs"". A slight non-linear distortion is applied to the
correlation matrix computed for any epoch, leading to the emerging spectrum of
eigenvalues. The statistical properties of the emerging spectrum display: (i)
the shape of the emerging spectrum reflects the market instability, (ii) the
smallest eigenvalue may be able to statistically distinguish the nature of a
market turbulence or crisis -- internal instability or external shock, and
(iii) the time-lagged smallest eigenvalue has a statistically significant
correlation with the mean market cross-correlation. The smallest eigenvalue
seems to indicate that the financial market has become more turbulent in a
similar way as the mean does. Yet we show features of the smallest eigenvalue
of the emerging spectrum that distinguish different types of market
instabilities related to internal or external causes. Based on the paradigmatic
character of financial time series for other complex systems, the capacity of
the emerging spectrum to understand the nature of instability may be a new
feature, which can be broadly applied.
"
A closed formula for illiquid corporate bonds and an application to the European market,"  We deduce a simple closed formula for illiquid corporate coupon bond prices
when liquid bonds with similar characteristics (e.g. maturity) are present in
the market for the same issuer. The key model parameter is the
time-to-liquidate a position, i.e. the time that an experienced bond trader
takes to liquidate a given position on a corporate coupon bond.
The option approach we propose for pricing bonds' illiquidity is reminiscent
of the celebrated work of Longstaff (1995) on the non-marketability of some
non-dividend-paying shares in IPOs. This approach describes a quite common
situation in the fixed income market: it is rather usual to find issuers that,
besides liquid benchmark bonds, issue some other bonds that either are placed
to a small number of investors in private placements or have a limited issue
size.
The model considers interest rate and credit spread term structures and their
dynamics. We show that illiquid bonds present an additional liquidity spread
that depends on the time-to-liquidate aside from credit and interest rate
parameters. We provide a detailed application for two issuers in the European
market.
"
The Stretch to Stray on Time: Resonant Length of Random Walks in a Transient,"  First-passage times in random walks have a vast number of diverse
applications in physics, chemistry, biology, and finance. In general,
environmental conditions for a stochastic process are not constant on the time
scale of the average first-passage time, or control might be applied to reduce
noise. We investigate moments of the first-passage time distribution under a
transient describing relaxation of environmental conditions. We solve the
Laplace-transformed (generalized) master equation analytically using a novel
method that is applicable to general state schemes. The first-passage time from
one end to the other of a linear chain of states is our application for the
solutions. The dependence of its average on the relaxation rate obeys a power
law for slow transients. The exponent $\nu$ depends on the chain length $N$
like $\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the
noise of first-passage times expressed as the coefficient of variation (CV),
even if the average first-passage time is much longer than the transient. The
CV has a pronounced minimum for some lengths, which we call resonant lengths.
These results also suggest a simple and efficient noise control strategy, and
are closely related to the timing of repetitive excitations, coherence
resonance and information transmission by noisy excitable systems. A resonant
number of steps from the inhibited state to the excitation threshold and slow
recovery from negative feedback provide optimal timing noise reduction and
information transmission.
"
Explaining Parochialism: A Causal Account for Political Polarization in Changing Economic Environments,"  Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
"
General multilevel Monte Carlo methods for pricing discretely monitored Asian options,"  We describe general multilevel Monte Carlo methods that estimate the price of
an Asian option monitored at $m$ fixed dates. Our approach yields unbiased
estimators with standard deviation $O(\epsilon)$ in $O(m + (1/\epsilon)^{2})$
expected time for a variety of processes including the Black-Scholes model,
Merton's jump-diffusion model, the Square-Root diffusion model, Kou's double
exponential jump-diffusion model, the variance gamma and NIG exponential Levy
processes and, via the Milstein scheme, processes driven by scalar stochastic
differential equations. Using the Euler scheme, our approach estimates the
Asian option price with root mean square error $O(\epsilon)$ in
$O(m+(\ln(\epsilon)/\epsilon)^{2})$ expected time for processes driven by
multidimensional stochastic differential equations. Numerical experiments
confirm that our approach outperforms the conventional Monte Carlo method by a
factor of order $m$.
"
Instantaneous Arbitrage and the CAPM,"  This paper studies the concept of instantaneous arbitrage in continuous time
and its relation to the instantaneous CAPM. Absence of instantaneous arbitrage
is equivalent to the existence of a trading strategy which satisfies the CAPM
beta pricing relation in place of the market. Thus the difference between the
arbitrage argument and the CAPM argument in Black and Scholes (1973) is this:
the arbitrage argument assumes that there exists some portfolio satisfying the
capm equation, whereas the CAPM argument assumes, in addition, that this
portfolio is the market portfolio.
"
A six-factor asset pricing model,"  The present study introduce the human capital component to the Fama and
French five-factor model proposing an equilibrium six-factor asset pricing
model. The study employs an aggregate of four sets of portfolios mimicking size
and industry with varying dimensions. The first set consists of three set of
six portfolios each sorted on size to B/M, size to investment, and size to
momentum. The second set comprises of five index portfolios, third, a four-set
of twenty-five portfolios each sorted on size to B/M, size to investment, size
to profitability, and size to momentum, and the final set constitute thirty
industry portfolios. To estimate the parameters of six-factor asset pricing
model for the four sets of variant portfolios, we use OLS and Generalized
method of moments based robust instrumental variables technique (IVGMM). The
results obtained from the relevance, endogeneity, overidentifying restrictions,
and the Hausman's specification, tests indicate that the parameter estimates of
the six-factor model using IVGMM are robust and performs better than the OLS
approach. The human capital component shares equally the predictive power
alongside the factors in the framework in explaining the variations in return
on portfolios. Furthermore, we assess the t-ratio of the human capital
component of each IVGMM estimates of the six-factor asset pricing model for the
four sets of variant portfolios. The t-ratio of the human capital of the
eighty-three IVGMM estimates are more than 3.00 with reference to the standard
proposed by Harvey et al. (2016). This indicates the empirical success of the
six-factor asset-pricing model in explaining the variation in asset returns.
"
Particle-without-Particle: a practical pseudospectral collocation method for linear partial differential equations with distributional sources,"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
"
Capital Regulation under Price Impacts and Dynamic Financial Contagion,"  We construct a continuous time model for price-mediated contagion
precipitated by a common exogenous stress to the trading book of all firms in
the financial system. In this setting, firms are constrained so as to satisfy a
risk-weight based capital ratio requirement. We use this model to find
analytical bounds on the risk-weights for an asset as a function of the market
liquidity. Under these appropriate risk-weights, we find existence and
uniqueness for the joint system of firm behavior and the asset price. We
further consider an analytical bound on the firm liquidations, which allows us
to construct exact formulas for stress testing the financial system with
deterministic or random stresses. Numerical case studies are provided to
demonstrate various implications of this model and analytical bounds.
"
Spreading of an infectious disease between different locations,"  The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.
"
Improving Stock Movement Prediction with Adversarial Training,"  This paper contributes a new machine learning solution for stock movement
prediction, which aims to predict whether the price of a stock will be up or
down in the near future. The key novelty is that we propose to employ
adversarial training to improve the generalization of a recurrent neural
network model. The rationality of adversarial training here is that the input
features to stock prediction are typically based on stock price, which is
essentially a stochastic variable and continuously changed with time by nature.
As such, normal training with stationary price-based features (e.g. the closing
price) can easily overfit the data, being insufficient to obtain reliable
models. To address this problem, we propose to add perturbations to simulate
the stochasticity of continuous price variable, and train the model to work
well under small yet intentional perturbations. Extensive experiments on two
real-world stock data show that our method outperforms the state-of-the-art
solution with 3.11% relative improvements on average w.r.t. accuracy, verifying
the usefulness of adversarial training for stock prediction task. Codes will be
made available upon acceptance.
"
Scaling Limits for Super--replication with Transient Price Impact,"  We prove limit theorems for the super-replication cost of European options in
a Binomial model with transient price impact. We show that if the time step
goes to zero and the effective resilience between consecutive trading times
remains constant then the limit of the super--replication prices coincide with
the scaling limit for temporary price impact with a modified market depth.
"
Fast swaption pricing in Gaussian term structure models,"  We propose a fast and accurate numerical method for pricing European
swaptions in multi-factor Gaussian term structure models. Our method can be
used to accelerate the calibration of such models to the volatility surface.
The pricing of an interest rate option in such a model involves evaluating a
multi-dimensional integral of the payoff of the claim on a domain where the
payoff is positive. In our method, we approximate the exercise boundary of the
state space by a hyperplane tangent to the maximum probability point on the
boundary and simplify the multi-dimensional integration into an analytical
form. The maximum probability point can be determined using the gradient
descent method. We demonstrate that our method is superior to previous methods
by comparing the results to the price obtained by numerical integration.
"
Opinion Dynamics via Search Engines (and other Algorithmic Gatekeepers),"  Ranking algorithms are the information gatekeepers of the Internet era. We
develop a stylized model to study the effects of ranking algorithms on opinion
dynamics. We consider a search engine that uses an algorithm based on
popularity and on personalization. We find that popularity-based rankings
generate an advantage of the fewer effect: fewer websites reporting a given
signal attract relatively more traffic overall. This highlights a novel,
ranking-driven channel that explains the diffusion of misinformation, as
websites reporting incorrect information may attract an amplified amount of
traffic precisely because they are few. Furthermore, when individuals provide
sufficiently positive feedback to the ranking algorithm, popularity-based
rankings tend to aggregate information while personalization acts in the
opposite direction.
"
Optimal proportional reinsurance and investment for stochastic factor models,"  In this work we investigate the optimal proportional reinsurance-investment
strategy of an insurance company which wishes to maximize the expected
exponential utility of its terminal wealth in a finite time horizon. Our goal
is to extend the classical Cramer-Lundberg model introducing a stochastic
factor which affects the intensity of the claims arrival process, described by
a Cox process, as well as the insurance and reinsurance premia. Using the
classical stochastic control approach based on the Hamilton-Jacobi-Bellman
equation we characterize the optimal strategy and provide a verification result
for the value function via classical solutions of two backward partial
differential equations. Existence and uniqueness of these solutions are
discussed. Results under various premium calculation principles are illustrated
and a new premium calculation rule is proposed in order to get more realistic
strategies and to better fit our stochastic factor model. Finally, numerical
simulations are performed to obtain sensitivity analyses.
"
Endogeneous Dynamics of Intraday Liquidity,"  In this paper we investigate the endogenous information contained in four
liquidity variables at a five minutes time scale on equity markets around the
world: the traded volume, the bid-ask spread, the volatility and the volume at
first limits of the orderbook. In the spirit of Granger causality, we measure
the level of information by the level of accuracy of linear autoregressive
models. This empirical study is carried out on a dataset of more than 300
stocks from four different markets (US, UK, Japan and Hong Kong) from a period
of over five years. We discuss the obtained performances of autoregressive (AR)
models on stationarized versions of the variables, focusing on explaining the
observed differences between stocks.
Since empirical studies are often conducted at this time scale, we believe it
is of paramount importance to document endogenous dynamics in a simple
framework with no addition of supplemental information. Our study can hence be
used as a benchmark to identify exogenous effects. On the other hand, most
optimal trading frameworks (like the celebrated Almgren and Chriss one), focus
on computing an optimal trading speed at a frequency close to the one we
consider. Such frameworks very often take i.i.d. assumptions on liquidity
variables; this paper document the auto-correlations emerging from real data,
opening the door to new developments in optimal trading.
"
Trends in the Diffusion of Misinformation on Social Media,"  We measure trends in the diffusion of misinformation on Facebook and Twitter
between January 2015 and July 2018. We focus on stories from 570 sites that
have been identified as producers of false stories. Interactions with these
sites on both Facebook and Twitter rose steadily through the end of 2016.
Interactions then fell sharply on Facebook while they continued to rise on
Twitter, with the ratio of Facebook engagements to Twitter shares falling by
approximately 60 percent. We see no similar pattern for other news, business,
or culture sites, where interactions have been relatively stable over time and
have followed similar trends on the two platforms both before and after the
election.
"
Evaluation of equity-based debt obligations,"  We consider a class of participation rights, i.e. obligations issued by a
company to investors who are interested in performance-based compensation.
Albeit having desirable economic properties equity-based debt obligations
(EbDO) pose challenges in accounting and contract pricing. We formulate and
solve the associated mathematical problem in a discrete time, as well as a
continuous time setting. In the latter case the problem is reduced to a
forward-backward stochastic differential equation (FBSDE) and solved using the
method of decoupling fields.
"
Option Pricing Models Driven by the Space-Time Fractional Diffusion: Series Representation and Applications,"  In this paper, we focus on option pricing models based on space-time
fractional diffusion. We briefly revise recent results which show that the
option price can be represented in the terms of rapidly converging
double-series and apply these results to the data from real markets. We focus
on estimation of model parameters from the market data and estimation of
implied volatility within the space-time fractional option pricing models.
"
Unravelling Airbnb Predicting Price for New Listing,"  This paper analyzes Airbnb listings in the city of San Francisco to better
understand how different attributes such as bedrooms, location, house type
amongst others can be used to accurately predict the price of a new listing
that optimal in terms of the host's profitability yet affordable to their
guests. This model is intended to be helpful to the internal pricing tools that
Airbnb provides to its hosts. Furthermore, additional analysis is performed to
ascertain the likelihood of a listings availability for potential guests to
consider while making a booking. The analysis begins with exploring and
examining the data to make necessary transformations that can be conducive for
a better understanding of the problem at large while helping us make
hypothesis. Moving further, machine learning models are built that are
intuitive to use to validate the hypothesis on pricing and availability and run
experiments in that context to arrive at a viable solution. The paper then
concludes with a discussion on the business implications, associated risks and
future scope.
"
Optimal portfolio selection in an It-Markov additive market,"  We study a portfolio selection problem in a continuous-time It-Markov
additive market with prices of financial assets described by Markov additive
processes which combine Lvy processes and regime switching models. Thus the
model takes into account two sources of risk: the jump diffusion risk and the
regime switching risk. For this reason the market is incomplete. We complete
the market by enlarging it with the use of a set of Markovian jump securities,
Markovian power-jump securities and impulse regime switching securities.
Moreover, we give conditions under which the market is
asymptotic-arbitrage-free. We solve the portfolio selection problem in the
It-Markov additive market for the power utility and the logarithmic utility.
"
Optimal hedging under fast-varying stochastic volatility,"  In a market with a rough or Markovian mean-reverting stochastic volatility
there is no perfect hedge. Here it is shown how various delta-type hedging
strategies perform and can be evaluated in such markets. A precise
characterization of the hedging cost, the replication cost caused by the
volatility fluctuations, is presented in an asymptotic regime of rapid mean
reversion for the volatility fluctuations. The optimal dynamic asset based
hedging strategy in the considered regime is identified as the so-called
`practitioners' delta hedging scheme. It is moreover shown that the
performances of the delta-type hedging schemes are essentially independent of
the regularity of the volatility paths in the considered regime and that the
hedging costs are related to a vega risk martingale whose magnitude is
proportional to a new market risk parameter.
"
Computation of optimal transport and related hedging problems via penalization and neural networks,"  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
"
The Italian Pension Gap: a Stochastic Optimal Control Approach,"  We study the gap between the state pension provided by the Italian pension
system pre-Dini reform and post-Dini reform. The goal is to fill the gap
between the old and the new pension by joining a defined contribution pension
scheme and adopting an optimal investment strategy that is target-based. We
find that it is possible to cover, at least partially, this gap with the
additional income of the pension scheme, especially in the presence of late
retirement and in the presence of stagnant career. Workers with dynamic career
and workers who retire early are those who are most penalised by the reform.
Results are intuitive and in line with previous studies on the subject.
"
"Health Care Expenditures, Financial Stability, and Participation in the Supplemental Nutrition Assistance Program (SNAP)","  This paper examines the association between household healthcare expenses and
participation in the Supplemental Nutrition Assistance Program (SNAP) when
moderated by factors associated with financial stability of households. Using a
large longitudinal panel encompassing eight years, this study finds that an
inter-temporal increase in out-of-pocket medical expenses increased the
likelihood of household SNAP participation in the current period. Financially
stable households with precautionary financial assets to cover at least 6
months worth of household expenses were significantly less likely to
participate in SNAP. The low income households who recently experienced an
increase in out of pocket medical expenses but had adequate precautionary
savings were less likely than similar households who did not have precautionary
savings to participate in SNAP. Implications for economists, policy makers, and
household finance professionals are discussed.
"
Technological Parasitism,"  Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.
"
Asynchronous stochastic price pump,"  We propose a model for equity trading in a population of agents where each
agent acts to achieve his or her target stock-to-bond ratio, and, as a feedback
mechanism, follows a market adaptive strategy. In this model only a fraction of
agents participates in buying and selling stock during a trading period, while
the rest of the group accepts the newly set price. Using numerical simulations
we show that the stochastic process settles on a stationary regime for the
returns. The mean return can be greater or less than the return on the bond and
it is determined by the parameters of the adaptive mechanism. When the number
of interacting agents is fixed, the distribution of the returns follows the
log-normal density. In this case, we give an analytic formula for the mean rate
of return in terms of the rate of change of agents' risk levels and confirm the
formula by numerical simulations. However, when the number of interacting
agents per period is random, the distribution of returns can significantly
deviate from the log-normal, especially as the variance of the distribution for
the number of interacting agents increases.
"
A path integral based model for stocks and order dynamics,"  We introduce a model for the short-term dynamics of financial assets based on
an application to finance of quantum gauge theory, developing ideas of Ilinski.
We present a numerical algorithm for the computation of the probability
distribution of prices and compare the results with APPLE stocks prices and the
S&P500 index.
"
New face of multifractality: Multi-branched left-sidedness and phase transitions in multifractality of interevent times,"  We develop an extended multifractal analysis based on the Legendre-Fenchel
transform rather than the routinely used Legendre transform. We apply this
analysis to studying time series consisting of inter-event times. As a result,
we discern the non-monotonic behavior of the generalized Hurst exponent - the
fundamental exponent studied by us - and hence a multi-branched left-sided
spectrum of dimensions. This kind of multifractality is a direct result of the
non-monotonic behavior of the generalized Hurst exponent and is not caused by
non-analytic behavior as has been previously suggested. We examine the main
thermodynamic consequences of the existence of this type of multifractality
related to the thermal stable, metastable, and unstable phases within a
hierarchy of fluctuations, and also to the first and second order phase
transitions between them.
"
The CCI30 Index,"  We describe the design of the CCI30 cryptocurrency index.
"
Quantifying the Model Risk Inherent in the Calibration and Recalibration of Option Pricing Models,"  We focus on two particular aspects of model risk: the inability of a chosen
model to fit observed market prices at a given point in time (calibration
error) and the model risk due to recalibration of model parameters (in
contradiction to the model assumptions). In this context, we follow the
approach of Glasserman and Xu (2014) and use relative entropy as a pre-metric
in order to quantify these two sources of model risk in a common framework, and
consider the trade-offs between them when choosing a model and the frequency
with which to recalibrate to the market. We illustrate this approach applied to
the models of Black and Scholes (1973) and Heston (1993), using option data for
Apple (AAPL) and Google (GOOG). We find that recalibrating a model more
frequently simply shifts model risk from one type to another, without any
substantial reduction of aggregate model risk. Furthermore, moving to a more
complicated stochastic model is seen to be counterproductive if one requires a
high degree of robustness, for example as quantified by a 99 percent quantile
of aggregate model risk.
"
Model Risk Measurement under Wasserstein Distance,"  The paper proposes a new approach to model risk measurement based on the
Wasserstein distance between two probability measures. It formulates the
theoretical motivation resulting from the interpretation of fictitious
adversary of robust risk management. The proposed approach accounts for all
alternative models and incorporates the economic reality of the fictitious
adversary. It provides practically feasible results that overcome the
restriction and the integrability issue imposed by the nominal model. The
Wasserstein approach suits for all types of model risk problems, ranging from
the single-asset hedging risk problem to the multi-asset allocation problem.
The robust capital allocation line, accounting for the correlation risk, is not
achievable with other non-parametric approaches.
"
Optimal VWAP execution under transient price impact,"  We solve the problem of optimal liquidation with volume weighted average
price (VWAP) benchmark when the market impact is linear and transient. Our
setting is indeed more general as it considers the case when the trading
interval is not necessarily coincident with the benchmark interval:
Implementation Shortfall and Target Close execution are shown to be particular
cases of our setting. We find explicit solutions in continuous and discrete
time considering risk averse investors having a CARA utility function. Finally,
we show that, contrary to what is observed for Implementation Shortfall, the
optimal VWAP solution contains both buy and sell trades also when the decay
kernel is convex.
"
Portfolio Optimization under Fast Mean-reverting and Rough Fractional Stochastic Environment,"  Fractional stochastic volatility models have been widely used to capture the
non-Markovian structure revealed from financial time series of realized
volatility. On the other hand, empirical studies have identified scales in
stock price volatility: both fast-time scale on the order of days and
slow-scale on the order of months. So, it is natural to study the portfolio
optimization problem under the effects of dependence behavior which we will
model by fractional Brownian motions with Hurst index $H$, and in the fast or
slow regimes characterized by small parameters $\eps$ or $\delta$. For the
slowly varying volatility with $H \in (0,1)$, it was shown that the first order
correction to the problem value contains two terms of order $\delta^H$, one
random component and one deterministic function of state processes, while for
the fast varying case with $H > \half$, the same form holds at order
$\eps^{1-H}$. This paper is dedicated to the remaining case of a fast-varying
rough environment ($H < \half$) which exhibits a different behavior. We show
that, in the expansion, only one deterministic term of order $\sqrt{\eps}$
appears in the first order correction.
"
A Game of Martingales,"  We consider a two player dynamic game played over $T \leq \infty$ periods. In
each period each player chooses any probability distribution with support on
$[0,1]$ with a given mean, where the mean is the realized value of the draw
from the previous period. The player with the highest realization in the period
achieves a payoff of $1$, and the other player, $0$; and each player seeks to
maximize the discounted sum of his per-period payoffs over the whole time
horizon. We solve for the unique subgame perfect equilibrium of this game, and
establish properties of the equilibrium strategies and payoffs in the limit.
The solution and comparative statics thereof provide insight about
intertemporal choice with status concerns. In particular we find that patient
players take fewer risks.
"
Investor Reaction to Financial Disclosures Across Topics: An Application of Latent Dirichlet Allocation,"  This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.
"
Calibration for Weak Variance-Alpha-Gamma Processes,"  The weak variance-alpha-gamma process is a multivariate Lvy process
constructed by weakly subordinating Brownian motion, possibly with correlated
components with an alpha-gamma subordinator. It generalises the
variance-alpha-gamma process of Semeraro constructed by traditional
subordination. We compare three calibration methods for the weak
variance-alpha-gamma process, method of moments, maximum likelihood estimation
(MLE) and digital moment estimation (DME). We derive a condition for Fourier
invertibility needed to apply MLE and show in our simulations that MLE produces
a better fit when this condition holds, while DME produces a better fit when it
is violated. We also find that the weak variance-alpha-gamma process exhibits a
wider range of dependence and produces a significantly better fit than the
variance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME
produces the best fit in this situation.
"
Measuring Systematic Risk with Neural Network Factor Model,"  In this paper, we measure systematic risk with a new nonparametric factor
model, the neural network factor model. The suitable factors for systematic
risk can be naturally found by inserting daily returns on a wide range of
assets into the bottleneck network. The network-based model does not stick to a
probabilistic structure unlike parametric factor models, and it does not need
feature engineering because it selects notable features by itself. In addition,
we compare performance between our model and the existing models using 20-year
data of S&P 100 components. Although the new model can not outperform the best
ones among the parametric factor models due to limitations of the variational
inference, the estimation method used for this study, it is still noteworthy in
that it achieves the performance as best the comparable models could without
any prior knowledge.
"
Dimensional Analysis in Economics: A Study of the Neoclassical Economic Growth Model,"  The fundamental purpose of the present research article is to introduce the
basic principles of Dimensional Analysis in the context of the neoclassical
economic theory, in order to apply such principles to the fundamental relations
that underlay most models of economic growth. In particular, basic instruments
from Dimensional Analysis are used to evaluate the analytical consistency of
the Neoclassical economic growth model. The analysis shows that an adjustment
to the model is required in such a way that the principle of dimensional
homogeneity is satisfied.
"
"Cross-Sectional Variation of Intraday Liquidity, Cross-Impact, and their Effect on Portfolio Execution","  The composition of natural liquidity has been changing over time. An analysis
of intraday volumes for the S&P500 constituent stocks illustrates that (i)
volume surprises, i.e., deviations from their respective forecasts, are
correlated across stocks, and (ii) this correlation increases during the last
few hours of the trading session. These observations could be attributed, in
part, to the prevalence of portfolio trading activity that is implicit in the
growth of ETF, passive and systematic investment strategies; and, to the
increased trading intensity of such strategies towards the end of the trading
session, e.g., due to execution of mutual fund inflows/outflows that are
benchmarked to the closing price on each day. In this paper, we investigate the
consequences of such portfolio liquidity on price impact and portfolio
execution. We derive a linear cross-asset market impact from a stylized model
that explicitly captures the fact that a certain fraction of natural liquidity
providers only trade portfolios of stocks whenever they choose to execute. We
find that due to cross-impact and its intraday variation, it is optimal for a
risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a
coupled manner, as opposed to a separable VWAP-like execution that is often
assumed. The optimal schedule couples the execution of the various orders so as
to be able to take advantage of increased portfolio liquidity towards the end
of the day. A worst case analysis shows that the potential cost reduction from
this optimized execution schedule over the separable approach can be as high as
6% for plausible model parameters. Finally, we discuss how to estimate
cross-sectional price impact if one had a dataset of realized portfolio
transaction records that exploits the low-rank structure of its coefficient
matrix suggested by our analysis.
"
How production networks amplify economic growth,"  Technological improvement is the most important cause of long-term economic
growth, but the factors that drive it are still not fully understood. In
standard growth models technology is treated in the aggregate, and a main goal
has been to understand how growth depends on factors such as knowledge
production. But an economy can also be viewed as a network, in which producers
purchase goods, convert them to new goods, and sell them to households or other
producers. Here we develop a simple theory that shows how the network
properties of an economy can amplify the effects of technological improvements
as they propagate along chains of production. A key property of an industry is
its output multiplier, which can be understood as the average number of
production steps required to make a good. The model predicts that the output
multiplier of an industry predicts future changes in prices, and that the
average output multiplier of a country predicts future economic growth. We test
these predictions using data from the World Input Output Database and find
results in good agreement with the model. The results show how purely
structural properties of an economy, that have nothing to do with innovation or
human creativity, can exert an important influence on long-term growth.
"
Quantification of market efficiency based on informational-entropy,"  Since the 1960s, the question whether markets are efficient or not is
controversially discussed. One reason for the difficulty to overcome the
controversy is the lack of a universal, but also precise, quantitative
definition of efficiency that is able to graduate between different states of
efficiency. The main purpose of this article is to fill this gap by developing
a measure for the efficiency of markets that fulfill all the stated
requirements. It is shown that the new definition of efficiency, based on
informational-entropy, is equivalent to the two most used definitions of
efficiency from Fama and Jensen. The new measure therefore enables steps to
settle the dispute over the state of efficiency in markets. Moreover, it is
shown that inefficiency in a market can either arise from the possibility to
use information to predict an event with higher than chance level, or can
emerge from wrong pricing/ quotes that do not reflect the right probabilities
of possible events. Finally, the calculation of efficiency is demonstrated on a
simple game (of coin tossing), to show how one could exactly quantify the
efficiency in any market-like system, if all probabilities are known.
"
Corruption-free scheme of entering into contract: mathematical model,"  The main purpose of this paper is to formalize the modelling process,
analysis and mathematical definition of corruption when entering into a
contract between principal agent and producers. The formulation of the problem
and the definition of concepts for the general case are considered. For
definiteness, all calculations and formulas are given for the case of three
producers, one principal agent and one intermediary. Economic analysis of
corruption allowed building a mathematical model of interaction between agents.
Financial resources distribution problem in a contract with a corrupted
intermediary is considered.Then proposed conditions for corruption emergence
and its possible consequences. Optimal non-corruption schemes of financial
resources distribution in a contract are formed, when principal agent's choice
is limited first only by asymmetrical information and then also by external
influences.Numerical examples suggesting optimal corruption-free agents'
behaviour are presented.
"
Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler,"  In this work, we propose a model for estimating volatility from financial
time series, extending the non-Gaussian family of space-state models with exact
marginal likelihood proposed by Gamerman, Santos and Franco (2013). On the
literature there are models focused on estimating financial assets risk,
however, most of them rely on MCMC methods based on Metropolis algorithms,
since full conditional posterior distributions are not known. We present an
alternative model capable of estimating the volatility, in an automatic way,
since all full conditional posterior distributions are known, and it is
possible to obtain an exact sample of parameters via Gibbs Sampler. The
incorporation of jumps in returns allows the model to capture speculative
movements of the data, so that their influence does not propagate to
volatility. We evaluate the performance of the algorithm using synthetic and
real data time series.
Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,
Dynamic linear models.
"
Asymptotics for Small Nonlinear Price Impact: a PDE Approach to the Multidimensional Case,"  We provide an asymptotic expansion of the value function of a
multidimensional utility maximization problem from consumption with small
non-linear price impact. In our model cross-impacts between assets are allowed.
In the limit for small price impact, we determine the asymptotic expansion of
the value function around its frictionless version. The leading order
correction is characterized by a nonlinear second order PDE related to an
ergodic control problem and a linear parabolic PDE. We illustrate our result on
a multivariate geometric Brownian motion price model.
"
Optimal investment-consumption problem post-retirement with a minimum guarantee,"  We study the optimal investment-consumption problem for a member of defined
contribution plan during the decumulation phase. For a fixed annuitization
time, to achieve higher final annuity, we consider a variable consumption rate.
Moreover, to eliminate the ruin possibilities and having a minimum guarantee
for the final annuity, we consider a safety level for the wealth process which
consequently yields a Hamilton-Jacobi-Bellman (HJB) equation on a bounded
domain. We apply the policy iteration method to find approximations of solution
of the HJB equation. Finally, we give the simulation results for the optimal
investment-consumption strategies, optimal wealth process and the final annuity
for different ranges of admissible consumptions. Furthermore, by calculating
the present market value of the future cash flows before and after the
annuitization, we compare the results for different consumption policies.
"
Using Stock Prices as Ground Truth in Sentiment Analysis to Generate Profitable Trading Signals,"  The increasing availability of ""big"" (large volume) social media data has
motivated a great deal of research in applying sentiment analysis to predict
the movement of prices within financial markets. Previous work in this field
investigates how the true sentiment of text (i.e. positive or negative
opinions) can be used for financial predictions, based on the assumption that
sentiments expressed online are representative of the true market sentiment.
Here we consider the converse idea, that using the stock price as the
ground-truth in the system may be a better indication of sentiment. Tweets are
labelled as Buy or Sell dependent on whether the stock price discussed rose or
fell over the following hour, and from this, stock-specific dictionaries are
built for individual companies. A Bayesian classifier is used to generate stock
predictions, which are input to an automated trading algorithm. Placing 468
trades over a 1 month period yields a return rate of 5.18%, which annualises to
approximately 83% per annum. This approach performs significantly better than
random chance and outperforms two baseline sentiment analysis methods tested.
"
Dealing with the Dimensionality Curse in Dynamic Pricing Competition: Using Frequent Repricing to Compensate Imperfect Market Anticipations,"  Most sales applications are characterized by competition and limited demand
information. For successful pricing strategies, frequent price adjustments as
well as anticipation of market dynamics are crucial. Both effects are
challenging as competitive markets are complex and computations of optimized
pricing adjustments can be time-consuming. We analyze stochastic dynamic
pricing models under oligopoly competition for the sale of perishable goods. To
circumvent the curse of dimensionality, we propose a heuristic approach to
efficiently compute price adjustments. To demonstrate our strategy's
applicability even if the number of competitors is large and their strategies
are unknown, we consider different competitive settings in which competitors
frequently and strategically adjust their prices. For all settings, we verify
that our heuristic strategy yields promising results. We compare the
performance of our heuristic against upper bounds, which are obtained by
optimal strategies that take advantage of perfect price anticipations. We find
that price adjustment frequencies can have a larger impact on expected profits
than price anticipations. Finally, our approach has been applied on Amazon for
the sale of used books. We have used a seller's historical market data to
calibrate our model. Sales results show that our data-driven strategy
outperforms the rule-based strategy of an experienced seller by a profit
increase of more than 20%.
"
Investigating the configurations in cross-shareholding: a joint copula-entropy approach,"  --- the companies populating a Stock market, along with their connections,
can be effectively modeled through a directed network, where the nodes
represent the companies, and the links indicate the ownership. This paper deals
with this theme and discusses the concentration of a market. A
cross-shareholding matrix is considered, along with two key factors: the node
out-degree distribution which represents the diversification of investments in
terms of the number of involved companies, and the node in-degree distribution
which reports the integration of a company due to the sales of its own shares
to other companies. While diversification is widely explored in the literature,
integration is most present in literature on contagions. This paper captures
such quantities of interest in the two frameworks and studies the stochastic
dependence of diversification and integration through a copula approach. We
adopt entropies as measures for assessing the concentration in the market. The
main question is to assess the dependence structure leading to a better
description of the data or to market polarization (minimal entropy) or market
fairness (maximal entropy). In so doing, we derive information on the way in
which the in- and out-degrees should be connected in order to shape the market.
The question is of interest to regulators bodies, as witnessed by specific
alert threshold published on the US mergers guidelines for limiting the
possibility of acquisitions and the prevalence of a single company on the
market. Indeed, all countries and the EU have also rules or guidelines in order
to limit concentrations, in a country or across borders, respectively. The
calibration of copulas and model parameters on the basis of real data serves as
an illustrative application of the theoretical proposal.
"
A Hilbert Space of Stationary Ergodic Processes,"  Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about ""angles"" between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
"
Conditional Optimal Stopping: A Time-Inconsistent Optimization,"  Inspired by recent work of P.-L. Lions on conditional optimal control, we
introduce a problem of optimal stopping under bounded rationality: the
objective is the expected payoff at the time of stopping, conditioned on
another event. For instance, an agent may care only about states where she is
still alive at the time of stopping, or a company may condition on not being
bankrupt. We observe that conditional optimization is time-inconsistent due to
the dynamic change of the conditioning probability and develop an equilibrium
approach in the spirit of R. H. Strotz' work for sophisticated agents in
discrete time. Equilibria are found to be essentially unique in the case of a
finite time horizon whereas an infinite horizon gives rise to non-uniqueness
and other interesting phenomena. We also introduce a theory which generalizes
the classical Snell envelope approach for optimal stopping by considering a
pair of processes with Snell-type properties.
"
Multi-channel discourse as an indicator for Bitcoin price and volume movements,"  This research aims to identify how Bitcoin-related news publications and
online discourse are expressed in Bitcoin exchange movements of price and
volume. Being inherently digital, all Bitcoin-related fundamental data (from
exchanges, as well as transactional data directly from the blockchain) is
available online, something that is not true for traditional businesses or
currencies traded on exchanges. This makes Bitcoin an interesting subject for
such research, as it enables the mapping of sentiment to fundamental events
that might otherwise be inaccessible. Furthermore, Bitcoin discussion largely
takes place on online forums and chat channels. In stock trading, the value of
sentiment data in trading decisions has been demonstrated numerous times [1]
[2] [3], and this research aims to determine whether there is value in such
data for Bitcoin trading models. To achieve this, data over the year 2015 has
been collected from Bitcointalk.org, (the biggest Bitcoin forum in post
volume), established news sources such as Bloomberg and the Wall Street
Journal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and
bitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we
find weak to moderate correlations between forum, news, and Reddit sentiment
and movements in price and volume from 1 to 5 days after the sentiment was
expressed. A Granger causality test confirms the predictive causality of the
sentiment on the daily percentage price and volume movements, and at the same
time underscores the predictive causality of market movements on sentiment
expressions in online communities
"
Dynamics of observables in rank-based models and performance of functionally generated portfolios,"  In the seminal work [9], several macroscopic market observables have been
introduced, in an attempt to find characteristics capturing the diversity of a
financial market. Despite the crucial importance of such observables for
investment decisions, a concise mathematical description of their dynamics has
been missing. We fill this gap in the setting of rank-based models and expect
our ideas to extend to other models of large financial markets as well. The
results are then used to study the performance of multiplicatively and
additively functionally generated portfolios, in particular, over short-term
and medium-term horizons.
"
Credit Risk Meets Random Matrices: Coping with Non-Stationary Asset Correlations,"  We review recent progress in modeling credit risk for correlated assets. We
start from the Merton model which default events and losses are derived from
the asset values at maturity. To estimate the time development of the asset
values, the stock prices are used whose correlations have a strong impact on
the loss distribution, particularly on its tails. These correlations are
non-stationary which also influences the tails. We account for the asset
fluctuations by averaging over an ensemble of random matrices that models the
truly existing set of measured correlation matrices. As a most welcome side
effect, this approach drastically reduces the parameter dependence of the loss
distribution, allowing us to obtain very explicit results which show
quantitatively that the heavy tails prevail over diversification benefits even
for small correlations. We calibrate our random matrix model with market data
and show how it is capable of grasping different market situations.
Furthermore, we present numerical simulations for concurrent portfolio risks,
i.e., for the joint probability densities of losses for two portfolios. For the
convenience of the reader, we give an introduction to the Wishart random matrix
model.
"
"Growth, Industrial Externality, Prospect Dynamics and Well-being on Markets","  Functions or 'functionnings' enable to give a structure to any economic
activity whether they are used to describe a good or a service that is
exchanged on a market or they constitute the capability of an agent to provide
the labor market with specific work and skills. That structure encompasses the
basic law of supply and demand and the conditions of growth within a
transaction and of the inflation control. Functional requirements can be
followed from the design of a product to the delivery of a solution to a
customer needs with different levels of externalities while value is created
integrating organizational and technical constraints whereas a budget is
allocated to the various entities of the firm involved in the production.
Entering the market through that structure leads to designing basic equations
of its dynamics and to finding canonical solutions out of particular
equilibria. This approach enables to tackle behavioral foundations of Prospect
Theory within a generalization of its probability weighting function turned
into an operator which applies to Western, Educated, Industrialized, Rich, and
Democratic societies as well as to the poorest ones. The nature of reality and
well-being appears then as closely related to the relative satisfaction reached
on the market, as it can be conceived by an agent, according to business
cycles. This reality being the result of the complementary systems that govern
human mind as structured by rational psychologists.
"
On the optimal investment-consumption and life insurance selection problem with an external stochastic factor,"  In this paper, we study a stochastic optimal control problem with stochastic
volatility. We prove the sufficient and necessary maximum principle for the
proposed problem. Then we apply the results to solve an investment, consumption
and life insurance problem with stochastic volatility, that is, we consider a
wage earner investing in one risk-free asset and one risky asset described by a
jump-diffusion process and has to decide concerning consumption and life
insurance purchase. We assume that the life insurance for the wage earner is
bought from a market composed of $M>1$ life insurance companies offering
pairwise distinct life insurance contracts. The goal is to maximize the
expected utilities derived from the consumption, the legacy in the case of a
premature death and the investor's terminal wealth.
"
Incremental Sharpe and other performance ratios,"  We present a new methodology of computing incremental contribution for
performance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling
ratios. Using Euler's homogeneous function theorem, we are able to decompose
these performance ratios as a linear combination of individual modified
performance ratios. This allows understanding the drivers of these performance
ratios as well as deriving a condition for a new asset to provide incremental
performance for the portfolio. We provide various numerical examples of this
performance ratio decomposition.
"
"Exploring the nuances in the relationship ""culture-strategy"" for the business world","  The current article explores interesting, significant and recently identified
nuances in the relationship ""culture-strategy"". The shared views of leading
scholars at the University of National and World Economy in relation with the
essence, direction, structure, role and hierarchy of ""culture-strategy""
relation are defined as a starting point of the analysis. The research emphasis
is directed on recent developments in interpreting the observed realizations of
the aforementioned link among the community of international scholars and
consultants, publishing in selected electronic scientific databases. In this
way a contemporary notion of the nature of ""culture-strategy"" relationship for
the entities from the world of business is outlined.
"
"The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and Option Portfolios","  The QLBS model is a discrete-time option hedging and pricing model that is
based on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines
the famous Q-Learning method for RL with the Black-Scholes (-Merton) model's
idea of reducing the problem of option pricing and hedging to the problem of
optimal rebalancing of a dynamic replicating portfolio for the option, which is
made of a stock and cash. Here we expand on several NuQLear (Numerical
Q-Learning) topics with the QLBS model. First, we investigate the performance
of Fitted Q Iteration for a RL (data-driven) solution to the model, and
benchmark it versus a DP (model-based) solution, as well as versus the BSM
model. Second, we develop an Inverse Reinforcement Learning (IRL) setting for
the model, where we only observe prices and actions (re-hedges) taken by a
trader, but not rewards. Third, we outline how the QLBS model can be used for
pricing portfolios of options, rather than a single option in isolation, thus
providing its own, data-driven and model independent solution to the (in)famous
volatility smile problem of the Black-Scholes model.
"
Towards equation of state for a market: A thermodynamical paradigm of economics,"  Foundations of equilibrium thermodynamics are the equation of state (EoS) and
four postulated laws of thermodynamics. We use equilibrium thermodynamics
paradigms in constructing the EoS for microeconomics system that is a market.
This speculation is hoped to be first step towards whole pictures of
thermodynamical paradigm of economics.
"
Portfolio Optimization in Fractional and Rough Heston Models,"  We consider a fractional version of the Heston volatility model which is
inspired by [16]. Within this model we treat portfolio optimization problems
for power utility functions. Using a suitable representation of the fractional
part, followed by a reasonable approximation we show that it is possible to
cast the problem into the classical stochastic control framework. This approach
is generic for fractional processes. We derive explicit solutions and obtain as
a by-product the Laplace transform of the integrated volatility. In order to
get rid of some undesirable features we introduce a new model for the rough
path scenario which is based on the Marchaud fractional derivative. We provide
a numerical study to underline our results.
"
Infinitesimal perturbation analysis for risk measures based on the Smith max-stable random field,"  When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
"
The sum of log-normal variates in geometric Brownian motion,"  Geometric Brownian motion (GBM) is a key model for representing
self-reproducing entities. Self-reproduction may be considered the definition
of life [5], and the dynamics it induces are of interest to those concerned
with living systems from biology to economics. Trajectories of GBM are
distributed according to the well-known log-normal density, broadening with
time. However, in many applications, what's of interest is not a single
trajectory but the sum, or average, of several trajectories. The distribution
of these objects is more complicated. Here we show two different ways of
finding their typical trajectories. We make use of an intriguing connection to
spin glasses: the expected free energy of the random energy model is an average
of log-normal variates. We make the mapping to GBM explicit and find that the
free energy result gives qualitatively correct behavior for GBM trajectories.
We then also compute the typical sum of lognormal variates using Ito calculus.
This alternative route is in close quantitative agreement with numerical work.
"
A time change strategy to model reporting delay dynamics in claims reserving,"  This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
"
Asset Price Bubbles: An Option-based Indicator,"  We construct a statistical indicator for the detection of short-term asset
price bubbles based on the information content of bid and ask market quotes for
plain vanilla put and call options. Our construction makes use of the
martingale theory of asset price bubbles and the fact that such scenarios where
the price for an asset exceeds its fundamental value can in principle be
detected by analysis of the asymptotic behavior of the implied volatility
surface. For extrapolating this implied volatility, we choose the SABR model,
mainly because of its decent fit to real option market quotes for a broad range
of maturities and its ease of calibration. As main theoretical result, we show
that under lognormal SABR dynamics, we can compute a simple yet powerful
closed-form martingale defect indicator by solving an ill-posed inverse
calibration problem. In order to cope with the ill-posedness and to quantify
the uncertainty which is inherent to such an indicator, we adopt a Bayesian
statistical parameter estimation perspective. We probe the resulting posterior
densities with a combination of optimization and adaptive Markov chain Monte
Carlo methods, thus providing a full-blown uncertainty estimation of all the
underlying parameters and the martingale defect indicator. Finally, we provide
real-market tests of the proposed option-based indicator with focus on tech
stocks due to increasing concerns about a tech bubble 2.0.
"
What are the most important factors that influence the changes in London Real Estate Prices? How to quantify them?,"  In recent years, real estate industry has captured government and public
attention around the world. The factors influencing the prices of real estate
are diversified and complex. However, due to the limitations and one-sidedness
of their respective views, they did not provide enough theoretical basis for
the fluctuation of house price and its influential factors. The purpose of this
paper is to build a housing price model to make the scientific and objective
analysis of London's real estate market trends from the year 1996 to 2016 and
proposes some countermeasures to reasonably control house prices. Specifically,
the paper analyzes eight factors which affect the house prices from two
aspects: housing supply and demand and find out the factor which is of vital
importance to the increase of housing price per square meter. The problem of a
high level of multicollinearity between them is solved by using principal
components analysis.
"
Derivatives pricing using signature payoffs,"  We introduce signature payoffs, a family of path-dependent derivatives that
are given in terms of the signature of the price path of the underlying asset.
We show that these derivatives are dense in the space of continuous payoffs, a
result that is exploited to quickly price arbitrary continuous payoffs. This
approach to pricing derivatives is then tested with European options, American
options, Asian options, lookback options and variance swaps. As we show,
signature payoffs can be used to price these derivatives with very high
accuracy.
"
Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of Critical Metal Companies,"  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more ""critical,"" and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different ""advisors."" creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
"
Discovering Bayesian Market Views for Intelligent Asset Allocation,"  Along with the advance of opinion mining techniques, public mood has been
found to be a key element for stock market prediction. However, how market
participants' behavior is affected by public mood has been rarely discussed.
Consequently, there has been little progress in leveraging public mood for the
asset allocation problem, which is preferred in a trusted and interpretable
way. In order to address the issue of incorporating public mood analyzed from
social media, we propose to formalize public mood into market views, because
market views can be integrated into the modern portfolio theory. In our
framework, the optimal market views will maximize returns in each period with a
Bayesian asset allocation model. We train two neural models to generate the
market views, and benchmark the model performance on other popular asset
allocation strategies. Our experimental results suggest that the formalization
of market views significantly increases the profitability (5% to 10% annually)
of the simulated portfolio at a given risk level.
"
Systems of ergodic BSDE arising in regime switching forward performance processes,"  We introduce and solve a new type of quadratic backward stochastic
differential equation systems defined in an infinite time horizon, called
\emph{ergodic BSDE systems}. Such systems arise naturally as candidate
solutions to characterize forward performance processes and their associated
optimal trading strategies in a regime switching market. In addition, we
develop a connection between the solution of the ergodic BSDE system and the
long-term growth rate of classical utility maximization problems, and use the
ergodic BSDE system to study the large time behavior of PDE systems with
quadratic growth Hamiltonians.
"
On approximations of Value at Risk and Expected Shortfall involving kurtosis,"  We derive new approximations for the Value at Risk and the Expected Shortfall
at high levels of loss distributions with positive skewness and excess
kurtosis, and we describe their precisions for notable ones such as for
exponential, Pareto type I, lognormal and compound (Poisson) distributions. Our
approximations are motivated by extensions of the so-called Normal Power
Approximation, used for approximating the cumulative distribution function of a
random variable, incorporating not only the skewness but the kurtosis of the
random variable in question as well. We show the performance of our
approximations in numerical examples and we also give comparisons with some
known ones in the literature.
"
"The Network of U.S. Mutual Fund Investments: Diversification, Similarity and Fragility throughout the Global Financial Crisis","  Network theory proved recently to be useful in the quantification of many
properties of financial systems. The analysis of the structure of investment
portfolios is a major application since their eventual correlation and overlap
impact the actual risk diversification by individual investors. We investigate
the bipartite network of US mutual fund portfolios and their assets. We follow
its evolution during the Global Financial Crisis and analyse the interplay
between diversification, as understood in classical portfolio theory, and
similarity of the investments of different funds. We show that, on average,
portfolios have become more diversified and less similar during the crisis.
However, we also find that large overlap is far more likely than expected from
models of random allocation of investments. This indicates the existence of
strong correlations between fund portfolio strategies. We introduce a
simplified model of propagation of financial shocks, that we exploit to show
that a systemic risk component origins from the similarity of portfolios. The
network is still vulnerable after crisis because of this effect, despite the
increase in the diversification of portfolios. Our results indicate that
diversification may even increase systemic risk when funds diversify in the
same way. Diversification and similarity can play antagonistic roles and the
trade-off between the two should be taken into account to properly assess
systemic risk.
"
A Community Microgrid Architecture with an Internal Local Market,"  This work fits in the context of community microgrids, where members of a
community can exchange energy and services among themselves, without going
through the usual channels of the public electricity grid. We introduce and
analyze a framework to operate a community microgrid, and to share the
resulting revenues and costs among its members. A market-oriented pricing of
energy exchanges within the community is obtained by implementing an internal
local market based on the marginal pricing scheme. The market aims at
maximizing the social welfare of the community, thanks to the more efficient
allocation of resources, the reduction of the peak power to be paid, and the
increased amount of reserve, achieved at an aggregate level. A community
microgrid operator, acting as a benevolent planner, redistributes revenues and
costs among the members, in such a way that the solution achieved by each
member within the community is not worse than the solution it would achieve by
acting individually. In this way, each member is incentivized to participate in
the community on a voluntary basis. The overall framework is formulated in the
form of a bilevel model, where the lower level problem clears the market, while
the upper level problem plays the role of the community microgrid operator.
Numerical results obtained on a real test case implemented in Belgium show
significant cost savings on a yearly scale for the community members, as
compared to the case when they act individually.
"
Strong and Weak Equilibria for Time-Inconsistent Stochastic Control in Continuous Time,"  A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
"
Trading algorithms with learning in latent alpha models,"  Alpha signals for statistical arbitrage strategies are often driven by latent
factors. This paper analyses how to optimally trade with latent factors that
cause prices to jump and diffuse. Moreover, we account for the effect of the
trader's actions on quoted prices and the prices they receive from trading.
Under fairly general assumptions, we demonstrate how the trader can learn the
posterior distribution over the latent states, and explicitly solve the latent
optimal trading problem. We provide a verification theorem, and a methodology
for calibrating the model by deriving a variation of the
expectation-maximization algorithm. To illustrate the efficacy of the optimal
strategy, we demonstrate its performance through simulations and compare it to
strategies which ignore learning in the latent factors. We also provide
calibration results for a particular model using Intel Corporation stock as an
example.
"
Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation,"  This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.
"
PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency issuance protocol,"  In this paper, we will describe a concept of a cryptocurrency issuance
protocol which supports digital currencies in a Proof-of-Work (< PoW >) like
manner. However, the methods assume alternative utilization of assets used for
cryptocurrency creation (rather than purchasing electricity necessary for <
mining >).
"
Zero-Inflated Autoregressive Conditional Duration Model for Discrete Trade Durations with Excessive Zeros,"  In finance, durations between successive transactions are usually modeled by
the autoregressive conditional duration model based on a continuous
distribution omitting frequent zero values. Zero durations can be caused by
either split transactions or independent transactions. We propose a discrete
model allowing for excessive zero values based on the zero-inflated negative
binomial distribution with score dynamics. We establish the invertibility of
the score filter. Additionally, we derive sufficient conditions for the
consistency and asymptotic normality of the maximum likelihood of the model
parameters. In an empirical study of DJIA stocks, we find that split
transactions cause on average 63% of zero values. Furthermore, the loss of
decimal places in the proposed model is less severe than incorrect treatment of
zero values in continuous models.
"
A Numerical Study of Carr and Lee's Correlation Immunization Strategy for Volatility Derivatives,"  In their seminal work `Robust Replication of Volatility Derivatives,' Carr
and Lee show how to robustly price and replicate a variety of claims written on
the quadratic variation of a risky asset under the assumption that the asset's
volatility process is independent of the Brownian motion that drives the
asset's price. Additionally, they propose a correlation immunization method
that minimizes the pricing and hedging error that results when the correlation
between the risky asset's price and volatility is nonzero. In this paper, we
perform a number of Monte Carlo experiments to test the effectiveness of Carr
and Lee's immunization strategy. Our results indicate that the correlation
immunization method is an effective means of reducing pricing and hedging
errors that result from nonzero correlation.
"
Stochastic comparisons of the largest claim amounts from two sets of interdependent heterogeneous portfolios,"  Let $ X_{\lambda_1},\ldots,X_{\lambda_n}$ be dependent non-negative random
variables and $Y_i=I_{p_i} X_{\lambda_i}$, $i=1,\ldots,n$, where
$I_{p_1},\ldots,I_{p_n}$ are independent Bernoulli random variables independent
of $X_{\lambda_i}$'s, with ${\rm E}[I_{p_i}]=p_i$, $i=1,\ldots,n$. In actuarial
sciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In
this paper, we compare the largest claim amounts of two sets of interdependent
portfolios, in the sense of usual stochastic order, when the variables in one
set have the parameters $\lambda_1,\ldots,\lambda_n$ and $p_1,\ldots,p_n$ and
the variables in the other set have the parameters
$\lambda^{*}_1,\ldots,\lambda^{*}_n$ and $p^*_1,\ldots,p^*_n$. For
illustration, we apply the results to some important models in actuary.
"
Statistical estimation of superhedging prices,"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
"
Will a Large Economy Be Stable?,"  We study networks of firms with Leontief production functions. Relying on
results from Random Matrix Theory, we argue that such networks generically
become unstable when their size increases, or when the heterogeneity in
productivities/connectivities becomes too strong. At marginal stability and for
large heterogeneities, we find that the distribution of firm sizes develops a
power-law tail, as observed empirically. Crises can be triggered by small
idiosyncratic shocks, which lead to ""avalanches"" of defaults characterized by a
power-law distribution of total output losses. We conjecture that evolutionary
and behavioural forces conspire to keep the economy close to marginal
stability. This scenario would naturally explain the well-known ""small shocks,
large business cycles"" puzzle, as anticipated long ago by Bak, Chen, Scheinkman
and Woodford.
"
A Markov Chain Model for the Cure Rate of Non-Performing Loans,"  A Markov-chain model is developed for the purpose estimation of the cure rate
of non-performing loans. The technique is performed collectively, on portfolios
and it can be applicable in the process of calculation of credit impairment. It
is efficient in terms of data manipulation costs which makes it accessible even
to smaller financial institutions. In addition, several other applications to
portfolio optimization are suggested.
"
Kinetic Theory for Finance Brownian Motion from Microscopic Dynamics,"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
"
Trading Strategies Generated by Path-dependent Functionals of Market Weights,"  Almost twenty years ago, E.R. Fernholz introduced portfolio generating
functions which can be used to construct a variety of portfolios, solely in the
terms of the individual companies' market weights. I. Karatzas and J. Ruf
recently developed another methodology for the functional construction of
portfolios, which leads to very simple conditions for strong relative arbitrage
with respect to the market. In this paper, both of these notions of functional
portfolio generation are generalized in a pathwise, probability-free setting;
portfolio generating functions are substituted by path-dependent functionals,
which involve the current market weights, as well as additional
bounded-variation functions of past and present market weights. This
generalization leads to a wider class of functionally-generated portfolios than
was heretofore possible, and yields improved conditions for outperforming the
market portfolio over suitable time-horizons.
"
Geert Hofstede et al's set of national cultural dimensions - popularity and criticisms,"  This article outlines different stages in development of the national culture
model, created by Geert Hofstede and his affiliates. This paper reveals and
synthesizes the contemporary review of the application spheres of this
framework. Numerous applications of the dimensions set are used as a source of
identifying significant critiques, concerning different aspects in model's
operation. These critiques are classified and their underlying reasons are also
outlined by means of a fishbone diagram.
"
A Machine Learning Framework for Stock Selection,"  This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
"
Distributions of Historic Market Data -- Implied and Realized Volatility,"  We undertake a systematic comparison between implied volatility, as
represented by VIX (new methodology) and VXO (old methodology), and realized
volatility. We compare visually and statistically distributions of realized and
implied variance (volatility squared) and study the distribution of their
ratio. We find that the ratio is best fitted by heavy-tailed -- lognormal and
fat-tailed (power-law) -- distributions, depending on whether preceding or
concurrent month of realized variance is used. We do not find substantial
difference in accuracy between VIX and VXO. Additionally, we study the variance
of theoretical realized variance for Heston and multiplicative models of
stochastic volatility and compare those with realized variance obtained from
historic market data.
"
Academic Engagement and Commercialization in an Institutional Transition Environment: Evidence from Shanghai Maritime University,"  Does academic engagement accelerate or crowd out the commercialization of
university knowledge? Research on this topic seldom considers the impact of the
institutional environment, especially when a formal institution for encouraging
the commercial activities of scholars has not yet been established. This study
investigates this question in the context of China, which is in the
institutional transition stage. Based on a survey of scholars from Shanghai
Maritime University, we demonstrate that academic engagement has a positive
impact on commercialization and that this impact is greater for risk-averse
scholars than for other risk-seeking scholars. Our results suggest that in an
institutional transition environment, the government should consider
encouraging academic engagement to stimulate the commercialization activities
of conservative scholars.
"
Multi-agent Economics and the Emergence of Critical Markets,"  The dual crises of the sub-prime mortgage crisis and the global financial
crisis has prompted a call for explanations of non-equilibrium market dynamics.
Recently a promising approach has been the use of agent based models (ABMs) to
simulate aggregate market dynamics. A key aspect of these models is the
endogenous emergence of critical transitions between equilibria, i.e. market
collapses, caused by multiple equilibria and changing market parameters.
Several research themes have developed microeconomic based models that include
multiple equilibria: social decision theory (Brock and Durlauf), quantal
response models (McKelvey and Palfrey), and strategic complementarities
(Goldstein). A gap that needs to be filled in the literature is a unified
analysis of the relationship between these models and how aggregate criticality
emerges from the individual agent level. This article reviews the agent-based
foundations of markets starting with the individual agent perspective of
McFadden and the aggregate perspective of catastrophe theory emphasising
connections between the different approaches. It is shown that changes in the
uncertainty agents have in the value of their interactions with one another,
even if these changes are one-sided, plays a central role in systemic market
risks such as market instability and the twin crises effect. These interactions
can endogenously cause crises that are an emergent phenomena of markets.
"
"Private Information, Credit Risk and Graph Structure in P2P Lending Networks","  This research investigated the potential for improving Peer-to-Peer (P2P)
credit scoring by using ""private information"" about communications and travels
of borrowers. We found that P2P borrowers' ego networks exhibit scale-free
behavior driven by underlying preferential attachment mechanisms that connect
borrowers in a fashion that can be used to predict loan profitability. The
projection of these private networks onto networks of mobile phone
communication and geographical locations from mobile phone GPS potentially give
loan providers access to private information through graph and location metrics
which we used to predict loan profitability. Graph topology was found to be an
important predictor of loan profitability, explaining over 5.5% of variability.
Networks of borrower location information explain an additional 19% of the
profitability. Machine learning algorithms were applied to the data set
previously analyzed to develop the predictive model and resulted in a 4%
reduction in mean squared error.
"
Quantum Blockchain using entanglement in time,"  A conceptual design for a quantum blockchain is proposed. Our method involves
encoding the blockchain into a temporal GHZ (Greenberger-Horne-Zeilinger) state
of photons that do not simultaneously coexist. It is shown that the
entanglement in time, as opposed to an entanglement in space, provides the
crucial quantum advantage. All the subcomponents of this system have already
been shown to be experimentally realized. Perhaps more shockingly, our encoding
procedure can be interpreted as non-classically influencing the past; hence
this decentralized quantum blockchain can be viewed as a quantum networked time
machine.
"
Assessing the effect of advertising expenditures upon sales: a Bayesian structural time series model,"  We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager's views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.
"
Effects of a Price limit Change on Market Stability at the Intraday Horizon in the Korean Stock Market,"  This paper investigates the effects of a price limit change on the volatility
of the Korean stock market's (KRX) intraday stock price process. Based on the
most recent transaction data from the KRX, which experienced a change in the
price limit on June 15, 2015, we examine the change in realized variance after
the price limit change to investigate the overall effects of the change on the
intraday market volatility. We then analyze the effects in more detail by
applying the discrete Fourier transform (DFT) to the data set. We find evidence
that the market becomes more volatile in the intraday horizon because of the
increase in the amplitudes of the low-frequency components of the price
processes after the price limit change. Therefore, liquidity providers are in a
worse situation than they were prior to the change.
"
Herding behavior in cryptocurrency markets,"  There are no solid arguments to sustain that digital currencies are the
future of online payments or the disruptive technology that some of its former
participants declared when used to face critiques. This paper aims to solve the
cryptocurrency puzzle from a behavioral finance perspective by finding the
parallelism between biases present in financial markets that could be applied
to cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are
driven by herding, hence this study test herding behavior under asymmetric and
symmetric conditions and the existence of different herding regimes by
employing the Markov-Switching approach.
"
Disentangling and Assessing Uncertainties in Multiperiod Corporate Default Risk Predictions,"  Measuring the corporate default risk is broadly important in economics and
finance. Quantitative methods have been developed to predictively assess future
corporate default probabilities. However, as a more difficult yet crucial
problem, evaluating the uncertainties associated with the default predictions
remains little explored. In this paper, we attempt to fill this blank by
developing a procedure for quantifying the level of associated uncertainties
upon carefully disentangling multiple contributing sources. Our framework
effectively incorporates broad information from historical default data,
corporates' financial records, and macroeconomic conditions by a)
characterizing the default mechanism, and b) capturing the future dynamics of
various features contributing to the default mechanism. Our procedure overcomes
the major challenges in this large scale statistical inference problem and
makes it practically feasible by using parsimonious models, innovative methods,
and modern computational facilities. By predicting the marketwide total number
of defaults and assessing the associated uncertainties, our method can also be
applied for evaluating the aggregated market credit risk level. Upon analyzing
a US market data set, we demonstrate that the level of uncertainties associated
with default risk assessments is indeed substantial. More informatively, we
also find that the level of uncertainties associated with the default risk
predictions is correlated with the level of default risks, indicating potential
for new scopes in practical applications including improving the accuracy of
default risk assessments.
"
Inferring short-term volatility indicators from Bitcoin blockchain,"  In this paper, we study the possibility of inferring early warning indicators
(EWIs) for periods of extreme bitcoin price volatility using features obtained
from Bitcoin daily transaction graphs. We infer the low-dimensional
representations of transaction graphs in the time period from 2012 to 2017
using Bitcoin blockchain, and demonstrate how these representations can be used
to predict extreme price volatility events. Our EWI, which is obtained with a
non-negative decomposition, contains more predictive information than those
obtained with singular value decomposition or scalar value of the total Bitcoin
transaction volume.
"
A sparse grid approach to balance sheet risk measurement,"  In this work, we present a numerical method based on a sparse grid
approximation to compute the loss distribution of the balance sheet of a
financial or an insurance company. We first describe, in a stylised way, the
assets and liabilities dynamics that are used for the numerical estimation of
the balance sheet distribution. For the pricing and hedging model, we chose a
classical Black & Scholes model with a stochastic interest rate following a
Hull & White model. The risk management model describing the evolution of the
parameters of the pricing and hedging model is a Gaussian model. The new
numerical method is compared with the traditional nested simulation approach.
We review the convergence of both methods to estimate the risk indicators under
consideration. Finally, we provide numerical results showing that the sparse
grid approach is extremely competitive for models with moderate dimension.
"
Option market (in)efficiency and implied volatility dynamics after return jumps,"  In informationally efficient financial markets, option prices and this
implied volatility should immediately be adjusted to new information that
arrives along with a jump in underlying's return, whereas gradual changes in
implied volatility would indicate market inefficiency. Using minute-by-minute
data on S&P 500 index options, we provide evidence regarding delayed and
gradual movements in implied volatility after the arrival of return jumps.
These movements are directed and persistent, especially in the case of negative
return jumps. Our results are significant when the implied volatilities are
extracted from at-the-money options and out-of-the-money puts, while the
implied volatility obtained from out-of-the-money calls converges to its new
level immediately rather than gradually. Thus, our analysis reveals that the
implied volatility smile is adjusted to jumps in underlying's return
asymmetrically. Finally, it would be possible to have statistical arbitrage in
zero-transaction-cost option markets, but under actual option price spreads,
our results do not imply abnormal option returns.
"
Thought Viruses and Asset Prices,"  We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with ""investment ideas."" Once an investment idea ""goes
viral,"" equilibrium prices exhibit the typical ""fever peak,"" which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.
"
Defining and estimating stochastic rate change in a dynamic general insurance portfolio,"  Rate change calculations in the literature involve deterministic methods that
measure the change in premium for a given policy. The definition of rate change
as a statistical parameter is proposed to address the stochastic nature of the
premium charged for a policy. It promotes the idea that rate change is a
property of an asymptotic population to be estimated, not just a property to
measure or monitor in the sample of observed policies that are written. Various
models and techniques are given for estimating this stochastic rate change and
quantifying the uncertainty in the estimates. The use of matched sampling is
emphasized for rate change estimation, as it adjusts for changes in policy
characteristics by directly searching for similar policies across policy years.
This avoids any of the assumptions and recipes that are required to re-rate
policies in years where they were not written, as is common with deterministic
methods. Such procedures can be subjective or implausible if the structure of
rating algorithms change or there are complex and heterogeneous exposure bases
and coverages. The methods discussed are applied to a motor premium database.
The application includes the use of a genetic algorithm with parallel
computations to automatically optimize the matched sampling.
"
"Contemporary facets of business successes among leading companies, operating in Bulgaria","  The current article unveils and analyzes some important factors, influencing
diversity in strategic decision-making approaches in local companies.
Researcher's attention is oriented to survey important characteristics of the
strategic moves, undertaken by leading companies in Bulgaria.
"
Integrating electricity markets: Impacts of increasing trade on prices and emissions in the western United States,"  This paper analyzes the market impacts of expanding California's centralized
electricity market across the western United States and provides the first
statistical assessment of this issue. Using market data from 2015-2018, I
estimate the short-term effects of increasing regional electricity trade
between California and neighboring states on prices, emissions, and generation.
Consistent with economic theory, I find negative price impacts from regional
trade, with each 1 gigawatt-hour (GWh) increase in California electricity
imports associated with an average 0.15 dollar decrease in CAISO price. The
price effect yields significant consumer savings well in excess of
implementation costs required to set up a regional market. I find a short-term
decrease in California carbon dioxide emissions associated with trading that is
partially offset by increased emissions in neighboring regions. Specifically,
each 1 GWh increase in regional trade is associated with a net 70-ton average
decrease in CO2 emissions across the western U.S. A small amount of increased
SO2 and NOx emissions are also observed in neighboring states associated with
increased exports to California. This implies a small portion (less than 10
percent) of electricity exports to California are supplied by coal generation.
This study identifies substantial short-term monetary benefits from market
regionalization for California consumers. It also shows that California's cap
and trade program is relatively effective in limiting the carbon content of
imported electricity, even absent a regional cap on CO2. The conclusions
suggest efforts to reduce trade barriers should move forward in parallel with
strong greenhouse gas policies that cap emissions levels across the market
region.
"
Continuity of Utility Maximization under Weak Convergence,"  In this paper we find sufficient conditions for the continuity of the value
of the utility maximization problem from terminal wealth with respect to the
convergence in distribution of the underlying processes. We provide several
examples which illustrate that without these conditions, we cannot generally
expect continuity to hold. Finally, we apply our results to the computation of
the minimum shortfall in the Heston model by building an appropriate lattice
approximation.
"
Static vs Adaptive Strategies for Optimal Execution with Signals,"  We consider an optimal execution problem in which a trader is looking at a
short-term price predictive signal while trading. In the case where the trader
is creating an instantaneous market impact, we show that transactions costs
resulting from the optimal adaptive strategy are substantially lower than the
corresponding costs of the optimal static strategy. Later, we investigate the
case where the trader is creating transient market impact. We show that
strategies in which the trader is observing the signal a number of times during
the trading period, can dramatically reduce the transaction costs and improve
the performance of the optimal static strategy. These results answer a question
which was raised by Brigo and Piat [6], by analyzing two cases where adaptive
strategies can improve the performance of the execution.
"
Simulation Methods for Stochastic Storage Problems: A Statistical Learning Perspective,"  We consider solution of stochastic storage problems through regression Monte
Carlo (RMC) methods. Taking a statistical learning perspective, we develop the
dynamic emulation algorithm (DEA) that unifies the different existing
approaches in a single modular template. We then investigate the two central
aspects of regression architecture and experimental design that constitute DEA.
For the regression piece, we discuss various non-parametric approaches, in
particular introducing the use of Gaussian process regression in the context of
stochastic storage. For simulation design, we compare the performance of
traditional design (grid discretization), against space-filling, and several
adaptive alternatives. The overall DEA template is illustrated with multiple
examples drawing from natural gas storage valuation and optimal control of
back-up generator in a microgrid.
"
Lee-Carter method for forecasting mortality for Peruvian Population,"  In this article, we have modeled mortality rates of Peruvian female and male
populations during the period of 1950-2017 using the Lee-Carter (LC) model. The
stochastic mortality model was introduced by Lee and Carter (1992) and has been
used by many authors for fitting and forecasting the human mortality rates. The
Singular Value Decomposition (SVD) approach is used for estimation of the
parameters of the LC model. Utilizing the best fitted auto regressive
integrated moving average (ARIMA) model we forecast the values of the time
dependent parameter of the LC model for the next thirty years. The forecasted
values of life expectancy at different age group with $95\%$ confidence
intervals are also reported for the next thirty years. In this research we use
the data, obtained from the Peruvian National Institute of Statistics (INEI).
"
A Stochastic Control Approach to Managed Futures Portfolios,"  We study a stochastic control approach to managed futures portfolios.
Building on the Schwartz 97 stochastic convenience yield model for commodity
prices, we formulate a utility maximization problem for dynamically trading a
single-maturity futures or multiple futures contracts over a finite horizon. By
analyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the
investor's utility maximization problem explicitly and derive the optimal
dynamic trading strategies in closed form. We provide numerical examples and
illustrate the optimal trading strategies using WTI crude oil futures data.
"
Generalised Lyapunov Functions and Functionally Generated Trading Strategies,"  This paper investigates the dependence of functional portfolio generation,
introduced by Fernholz (1999), on an extra finite variation process. The
framework of Karatzas and Ruf (2017) is used to formulate conditions on trading
strategies to be strong arbitrage relative to the market over sufficiently
large time horizons. A mollification argument and Komlos theorem yield a
general class of potential arbitrage strategies. These theoretical results are
complemented by several empirical examples using data from the S&P 500 stocks.
"
"A dynamic network model with persistent links and node-specific latent variables, with an application to the interbank market","  We propose a dynamic network model where two mechanisms control the
probability of a link between two nodes: (i) the existence or absence of this
link in the past, and (ii) node-specific latent variables (dynamic fitnesses)
describing the propensity of each node to create links. Assuming a Markov
dynamics for both mechanisms, we propose an Expectation-Maximization algorithm
for model estimation and inference of the latent variables. The estimated
parameters and fitnesses can be used to forecast the presence of a link in the
future. We apply our methodology to the e-MID interbank network for which the
two linkage mechanisms are associated with two different trading behaviors in
the process of network formation, namely preferential trading and trading
driven by node-specific characteristics. The empirical results allow to
recognise preferential lending in the interbank market and indicate how a
method that does not account for time-varying network topologies tends to
overestimate preferential linkage.
"
Time consistency for scalar multivariate risk measures,"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
"
Stock Market Visualization,"  We provide complete source code for a front-end GUI and its back-end
counterpart for a stock market visualization tool. It is built based on the
""functional visualization"" concept we discuss, whereby functionality is not
sacrificed for fancy graphics. The GUI, among other things, displays a
color-coded signal (computed by the back-end code) based on how ""out-of-whack""
each stock is trading compared with its peers (""mean-reversion""), and the most
sizable changes in the signal (""momentum""). The GUI also allows to efficiently
filter/tier stocks by various parameters (e.g., sector, exchange, signal,
liquidity, market cap) and functionally display them. The tool can be run as a
web-based or local application.
"
Extended opportunity cost model to find near equilibrium electricity prices under non-convexities,"  This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.
"
An Efficient Approach for Removing Look-ahead Bias in the Least Square Monte Carlo Algorithm: Leave-One-Out,"  The least square Monte Carlo (LSM) algorithm proposed by Longstaff and
Schwartz [2001] is the most widely used method for pricing options with early
exercise features. The LSM estimator contains look-ahead bias, and the
conventional technique of removing it necessitates an independent set of
simulations. This study proposes a new approach for efficiently eliminating
look-ahead bias by using the leave-one-out method, a well-known
cross-validation technique for machine learning applications. The leave-one-out
LSM (LOOLSM) method is illustrated with examples, including multi-asset options
whose LSM price is biased high. The asymptotic behavior of look-ahead bias is
also discussed with the LOOLSM approach.
"
Bayesian mean-variance analysis: Optimal portfolio selection under parameter uncertainty,"  The paper solves the problem of optimal portfolio choice when the parameters
of the asset returns distribution, like the mean vector and the covariance
matrix are unknown and have to be estimated by using historical data of the
asset returns. The new approach employs the Bayesian posterior predictive
distribution which is the distribution of the future realization of the asset
returns given the observable sample. The parameters of the posterior predictive
distributions are functions of the observed data values and, consequently, the
solution of the optimization problem is expressed in terms of data only and
does not depend on unknown quantities. In contrast, the optimization problem of
the traditional approach is based on unknown quantities which are estimated in
the second step leading to a suboptimal solution. We also derive a very useful
stochastic representation of the posterior predictive distribution whose
application leads not only to the solution of the considered optimization
problem, but provides the posterior predictive distribution of the optimal
portfolio return used to construct a prediction interval. A Bayesian efficient
frontier, a set of optimal portfolios obtained by employing the posterior
predictive distribution, is constructed as well. Theoretically and using real
data we show that the Bayesian efficient frontier outperforms the sample
efficient frontier, a common estimator of the set of optimal portfolios known
to be overoptimistic.
"
On the degree of incompleteness of an incomplete financial market,"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
"
Closed-form approximations in derivatives pricing: The Kristensen-Mele approach,"  Kristensen and Mele (2011) developed a new approach to obtain closed-form
approximations to continuous-time derivatives pricing models. The approach uses
a power series expansion of the pricing bias between an intractable model and
some known auxiliary model. Since the resulting approximation formula has
closed-form it is straightforward to obtain approximations of greeks. In this
thesis I will introduce Kristensen and Mele's methods and apply it to a variety
of stochastic volatility models of European style options as well as a model
for commodity futures. The focus of this thesis is the effect of different
model choices and different model parameter values on the numerical stability
of Kristensen and Mele's approximation.
"
Gaussian Approximation of a Risk Model with Stationary Hawkes Arrivals of Claims,"  We consider a classical risk process with arrival of claims following a
stationary Hawkes process. We study the asymptotic regime when the premium rate
and the baseline intensity of the claims arrival process are large, and claim
size is small. The main goal of this article is to establish a diffusion
approximation by verifying a functional central limit theorem of this model and
to compute both the finite-time and infinite-time horizon ruin probabilities.
Numerical results will also be given.
"
Erratum: Higher Order Elicitability and Osband's Principle,"  This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and
comments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel
(2016).
"
A novel improved fuzzy support vector machine based stock price trend forecast model,"  Application of fuzzy support vector machine in stock price forecast. Support
vector machine is a new type of machine learning method proposed in 1990s. It
can deal with classification and regression problems very successfully. Due to
the excellent learning performance of support vector machine, the technology
has become a hot research topic in the field of machine learning, and it has
been successfully applied in many fields. However, as a new technology, there
are many limitations to support vector machines. There is a large amount of
fuzzy information in the objective world. If the training of support vector
machine contains noise and fuzzy information, the performance of the support
vector machine will become very weak and powerless. As the complexity of many
factors influence the stock price prediction, the prediction results of
traditional support vector machine cannot meet people with precision, this
study improved the traditional support vector machine fuzzy prediction
algorithm is proposed to improve the new model precision. NASDAQ Stock Market,
Standard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy
support vector machine (NA-FSVM) is the proposed methodology.
"
"Practical volume computation of structured convex bodies, and an application to modeling portfolio dependencies and financial crises","  We examine volume computation of general-dimensional polytopes and more
general convex bodies, defined as the intersection of a simplex by a family of
parallel hyperplanes, and another family of parallel hyperplanes or a family of
concentric ellipsoids. Such convex bodies appear in modeling and predicting
financial crises. The impact of crises on the economy (labor, income, etc.)
makes its detection of prime interest. Certain features of dependencies in the
markets clearly identify times of turmoil. We describe the relationship between
asset characteristics by means of a copula; each characteristic is either a
linear or quadratic form of the portfolio components, hence the copula can be
constructed by computing volumes of convex bodies. We design and implement
practical algorithms in the exact and approximate setting, we experimentally
juxtapose them and study the tradeoff of exactness and accuracy for speed. We
analyze the following methods in order of increasing generality: rejection
sampling relying on uniformly sampling the simplex, which is the fastest
approach, but inaccurate for small volumes; exact formulae based on the
computation of integrals of probability distribution functions; an optimized
Lawrence sign decomposition method, since the polytopes at hand are shown to be
simple; Markov chain Monte Carlo algorithms using random walks based on the
hit-and-run paradigm generalized to nonlinear convex bodies and relying on new
methods for computing a ball enclosed; the latter is experimentally extended to
non-convex bodies with very encouraging results. Our C++ software, based on
CGAL and Eigen and available on github, is shown to be very effective in up to
100 dimensions. Our results offer novel, effective means of computing portfolio
dependencies and an indicator of financial crises, which is shown to correctly
identify past crises.
"
Spatial risk measures induced by powers of max-stable random fields,"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
"
"Token Economics in Energy Systems: Concept, Functionality and Applications","  Traditional centralized energy systems have the disadvantages of difficult
management and insufficient incentives. Blockchain is an emerging technology,
which can be utilized in energy systems to enhance their management and
control. Integrating token economy and blockchain technology, token economic
systems in energy possess the characteristics of strong incentives and low
cost, facilitating integrating renewable energy and demand side management, and
providing guarantees for improving energy efficiency and reducing emission.
This article describes the concept and functionality of token economics, and
then analyzes the feasibility of applying token economics in the energy
systems, and finally discuss the applications of token economics with an
example in integrated energy systems.
"
The cooling-off effect of price limits in the Chinese stock markets,"  In this paper, we investigate the cooling-off effect (opposite to the magnet
effect) from two aspects. Firstly, from the viewpoint of dynamics, we study the
existence of the cooling-off effect by following the dynamical evolution of
some financial variables over a period of time before the stock price hits its
limit. Secondly, from the probability perspective, we investigate, with the
logit model, the existence of the cooling-off effect through analyzing the
high-frequency data of all A-share common stocks traded on the Shanghai Stock
Exchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the
trading period from the opening phase prior to the moment that the stock price
hits its limits. A comparison is made of the properties between up-limit hits
and down-limit hits, and the possible difference will also be compared between
bullish and bearish market state by dividing the whole period into three
alternating bullish periods and three bearish periods. We find that the
cooling-off effect emerges for both up-limit hits and down-limit hits, and the
cooling-off effect of the down-limit hits is stronger than that of the up-limit
hits. The difference of the cooling-off effect between bullish period and
bearish period is quite modest. Moreover, we examine the sub-optimal orders
effect, and infer that the professional individual investors and institutional
investors play a positive role in the cooling-off effects. All these findings
indicate that the price limit trading rule exerts a positive effect on
maintaining the stability of the Chinese stock markets.
"
On the quasi-sure superhedging duality with frictions,"  We prove the superhedging duality for a discrete-time financial market with
proportional transaction costs under portfolio constraints and model
uncertainty. Frictions are modeled through solvency cones as in the original
model of [Kabanov, Y., Hedging and liquidation under transaction costs in
currency markets. Fin. Stoch., 3(2):237-248, 1999] adapted to the quasi-sure
setup of [Bouchard, B. and Nutz, M., Arbitrage and duality in nondominated
discrete-time models. Ann. Appl. Probab., 25(2):823-859, 2015]. Our results
hold under the condition of No Strict Arbitrage and under the efficient
friction hypothesis.
"
Reducing Estimation Risk in Mean-Variance Portfolios with Machine Learning,"  In portfolio analysis, the traditional approach of replacing population
moments with sample counterparts may lead to suboptimal portfolio choices. I
show that optimal portfolio weights can be estimated using a machine learning
(ML) framework, where the outcome to be predicted is a constant and the vector
of explanatory variables is the asset returns. It follows that ML specifically
targets estimation risk when estimating portfolio weights, and that
""off-the-shelf"" ML algorithms can be used to estimate the optimal portfolio in
the presence of parameter uncertainty. The framework nests the traditional
approach and recently proposed shrinkage approaches as special cases. By
relying on results from the ML literature, I derive new insights for existing
approaches and propose new estimation methods. Based on simulation studies and
several datasets, I find that ML significantly reduces estimation risk compared
to both the traditional approach and the equal weight strategy.
"
"Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual Framework for Large-Scale Collaboration and Transparent Investigation in Financial Markets","  Modern investigation in economics and in other sciences requires the ability
to store, share, and replicate results and methods of experiments that are
often multidisciplinary and yield a massive amount of data. Given the
increasing complexity and growing interaction across diverse bodies of
knowledge it is becoming imperative to define a platform to properly support
collaborative research and track origin, accuracy and use of data. This paper
starts by defining a set of methods leveraging scientific principles and
advocating the importance of those methods in multidisciplinary, computer
intensive fields like computational finance. The next part of this paper
defines a class of systems called scientific support systems, vis-a-vis usages
in other research fields such as bioinformatics, physics and engineering. We
outline a basic set of fundamental concepts, and list our goals and motivation
for leveraging such systems to enable large-scale investigation, ""crowd powered
science"", in economics. The core of this paper provides an outline of FRACTI in
five steps. First we present definitions related to scientific support systems
intrinsic to finance and describe common characteristics of financial use
cases. The second step concentrates on what can be exchanged through the
definition of shareable entities called contributions. The third step is the
description of a classification system for building blocks of the conceptual
framework, called facets. The fourth step introduces the meta-model that will
enable provenance tracking and representation of data fragments and simulation.
Finally we describe intended cases of use to highlight main strengths of
FRACTI: application of the scientific method for investigation in computational
finance, large-scale collaboration and simulation.
"
Discrete Time Dynamic Programming with Recursive Preferences: Optimality and Applications,"  This paper provides an alternative approach to the theory of dynamic
programming, designed to accommodate the kinds of recursive preference
specifications that have become popular in economic and financial analysis,
while still supporting traditional additively separable rewards. The approach
exploits the theory of monotone convex operators, which turns out to be well
suited to dynamic maximization. The intuition is that convexity is preserved
under maximization, so convexity properties found in preferences extend
naturally to the Bellman operator.
"
Retirement spending and biological age,"  We solve a lifecycle model in which the consumer's chronological age does not
move in lockstep with calendar time. Instead, biological age increases at a
stochastic non-linear rate in time like a broken clock that might occasionally
move backwards. In other words, biological age could actually decline. Our
paper is inspired by the growing body of medical literature that has identified
biomarkers which indicate how people age at different rates. This offers better
estimates of expected remaining lifetime and future mortality rates. It isn't
farfetched to argue that in the not-too-distant future personal age will be
more closely associated with biological vs. calendar age. Thus, after
introducing our stochastic mortality model we derive optimal consumption rates
in a classic Yaari (1965) framework adjusted to our proper clock time. In
addition to the normative implications of having access to biological age, our
positive objective is to partially explain the cross-sectional heterogeneity in
retirement spending rates at any given chronological age. In sum, we argue that
neither biological nor chronological age alone is a sufficient statistic for
making economic decisions. Rather, both ages are required to behave rationally.
"
Adapting the CVA model to Leland's framework,"  We consider the framework proposed by Burgard and Kjaer (2011) that derives
the PDE which governs the price of an option including bilateral counterparty
risk and funding. We extend this work by relaxing the assumption of absence of
transaction costs in the hedging portfolio by proposing a cost proportional to
the amount of assets traded and the traded price. After deriving the nonlinear
PDE, we prove the existence of a solution for the corresponding
initial-boundary value problem. Moreover, we develop a numerical scheme that
allows to find the solution of the PDE by setting different values for each
parameter of the model. To understand the impact of each variable within the
model, we analyze the Greeks of the option and the sensitivity of the price to
changes in all the risk factors.
"
"Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy","  We present a simple model of a non-equilibrium self-organizing market where
asset prices are partially driven by investment decisions of a bounded-rational
agent. The agent acts in a stochastic market environment driven by various
exogenous ""alpha"" signals, agent's own actions (via market impact), and noise.
Unlike traditional agent-based models, our agent aggregates all traders in the
market, rather than being a representative agent. Therefore, it can be
identified with a bounded-rational component of the market itself, providing a
particular implementation of an Invisible Hand market mechanism. In such
setting, market dynamics are modeled as a fictitious self-play of such
bounded-rational market-agent in its adversarial stochastic environment. As
rewards obtained by such self-playing market agent are not observed from market
data, we formulate and solve a simple model of such market dynamics based on a
neuroscience-inspired Bounded Rational Information Theoretic Inverse
Reinforcement Learning (BRIT-IRL). This results in effective asset price
dynamics with a non-linear mean reversion - which in our model is generated
dynamically, rather than being postulated. We argue that our model can be used
in a similar way to the Black-Litterman model. In particular, it represents, in
a simple modeling framework, market views of common predictive signals, market
impacts and implied optimal dynamic portfolio allocations, and can be used to
assess values of private signals. Moreover, it allows one to quantify a
""market-implied"" optimal investment strategy, along with a measure of market
rationality. Our approach is numerically light, and can be implemented using
standard off-the-shelf software such as TensorFlow.
"
Multivariate stable distributions and their applications for modelling cryptocurrency-returns,"  In this paper we extend the known methodology for fitting stable
distributions to the multivariate case and apply the suggested method to the
modelling of daily cryptocurrency-return data. The investigated time period is
cut into 10 non-overlapping sections, thus the changes can also be observed. We
apply bootstrap tests for checking the models and compare our approach to the
more traditional extreme-value and copula models.
"
Voting power of political parties in the Senate of Chile during the whole binomial system period: 1990-2017,"  The binomial system is an electoral system unique in the world. It was used
to elect the senators and deputies of Chile during 27 years, from the return of
democracy in 1990 until 2017. In this paper we study the real voting power of
the different political parties in the Senate of Chile during the whole
binomial period. We not only consider the different legislative periods, but
also any party changes between one period and the next. The real voting power
is measured by considering power indices from cooperative game theory, which
are based on the capability of the political parties to form winning
coalitions. With this approach, we can do an analysis that goes beyond the
simple count of parliamentary seats.
"
The risk of contagion spreading and its optimal control in the economy,"  The global crisis of 2008 provoked a heightened interest among scientists to
study the phenomenon, its propagation and negative consequences. The process of
modelling the spread of a virus is commonly used in epidemiology. Conceptually,
the spread of a disease among a population is similar to the contagion process
in economy. This similarity allows considering the contagion in the world
financial system using the same mathematical model of infection spread that is
often used in epidemiology. Our research focuses on the dynamic behaviour of
contagion spreading in the global financial network. The effect of infection by
a systemic spread of risks in the network of national banking systems of
countries is tested. An optimal control problem is then formulated to simulate
a control that may avoid significant financial losses. The results show that
the proposed approach describes well the reality of the world economy, and
emphasizes the importance of international relations between countries on the
financial stability.
"
Financial density forecasts: A comprehensive comparison of risk-neutral and historical schemes,"  We investigate the forecasting ability of the most commonly used benchmarks
in financial economics. We approach the usual caveats of probabilistic
forecasts studies -small samples, limited models and non-holistic validations-
by performing a comprehensive comparison of 15 predictive schemes during a time
period of over 21 years. All densities are evaluated in terms of their
statistical consistency, local accuracy and forecasting errors. Using a new
composite indicator, the Integrated Forecast Score (IFS), we show that
risk-neutral densities outperform historical-based predictions in terms of
information content. We find that the Variance Gamma model generates the
highest out-of-sample likelihood of observed prices and the lowest predictive
errors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts
across the entire density range. In contrast, lognormal densities, the Heston
model or the Breeden-Litzenberger formula yield biased predictions and are
rejected in statistical tests.
"
Complexity of products: the effect of data regularisation,"  Among several developments, the field of Economic Complexity (EC) has notably
seen the introduction of two new techniques. One is the Bootstrapped Selective
Predictability Scheme (SPSb), which can provide quantitative forecasts of the
Gross Domestic Product of countries. The other, Hidden Markov Model (HMM)
regularisation, denoises the datasets typically employed in the literature. We
contribute to EC along three different directions. First, we prove the
convergence of the SPSb algorithm to a well-known statistical learning
technique known as Nadaraya-Watson Kernel regression. The latter has
significantly lower time complexity, produces deterministic results, and it is
interchangeable with SPSb for the purpose of making predictions. Second, we
study the effects of HMM regularization on the Product Complexity and logPRODY
metrics, for which a model of time evolution has been recently proposed. We
find confirmation for the original interpretation of the logPRODY model as
describing the change in the global market structure of products with new
insights allowing a new interpretation of the Complexity measure, for which we
propose a modification. Third, we explore new effects of regularisation on the
data. We find that it reduces noise, and observe for the first time that it
increases nestedness in the export network adjacency matrix.
"
Advertising and Brand Attitudes: Evidence from 575 Brands over Five Years,"  Little is known about how different types of advertising affect brand
attitudes. We investigate the relationships between three brand attitude
variables (perceived quality, perceived value and recent satisfaction) and
three types of advertising (national traditional, local traditional and
digital). The data represent ten million brand attitude surveys and $264
billion spent on ads by 575 regular advertisers over a five-year period,
approximately 37% of all ad spend measured between 2008 and 2012. Inclusion of
brand/quarter fixed effects and industry/week fixed effects brings parameter
estimates closer to expectations without major reductions in estimation
precision. The findings indicate that (i) national traditional ads increase
perceived quality, perceived value, and recent satisfaction; (ii) local
traditional ads increase perceived quality and perceived value; (iii) digital
ads increase perceived value; and (iv) competitor ad effects are generally
negative.
"
Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks,"  We conduct an extensive empirical study on short-term electricity price
forecasting (EPF) to address the long-standing question if the optimal model
structure for EPF is univariate or multivariate. We provide evidence that
despite a minor edge in predictive performance overall, the multivariate
modeling framework does not uniformly outperform the univariate one across all
12 considered datasets, seasons of the year or hours of the day, and at times
is outperformed by the latter. This is an indication that combining advanced
structures or the corresponding forecasts from both modeling approaches can
bring a further improvement in forecasting accuracy. We show that this indeed
can be the case, even for a simple averaging scheme involving only two models.
Finally, we also analyze variable selection for the best performing
high-dimensional lasso-type models, thus provide guidelines to structuring
better performing forecasting model designs.
"
Markov cubature rules for polynomial processes,"  We study discretizations of polynomial processes using finite state Markov
processes satisfying suitable moment matching conditions. The states of these
Markov processes together with their transition probabilities can be
interpreted as Markov cubature rules. The polynomial property allows us to
study such rules using algebraic techniques. Markov cubature rules aid the
tractability of path-dependent tasks such as American option pricing in models
where the underlying factors are polynomial processes.
"
Optimal continuous-time ALM for insurers: a martingale approach,"  We study a continuous-time asset-allocation problem for a firm in the
insurance industry that backs up the liabilities raised by the insurance
contracts with the underwriting profits and the income resulting from investing
in the financial market. Using the martingale approach and convex duality
techniques we characterize strategies that maximize expected utility from
consumption and final wealth under CRRA preferences. We present numerical
results for some distributions of claims/liabilities with policy limits.
"
Critical factors and enablers of food quality and safety compliance risk management in the Vietnamese seafood supply chain,"  Recently, along with the emergence of food scandals, food supply chains have
to face with ever-increasing pressure from compliance with food quality and
safety regulations and standards. This paper aims to explore critical factors
of compliance risk in food supply chain with an illustrated case in Vietnamese
seafood industry. To this end, this study takes advantage of both primary and
secondary data sources through a comprehensive literature research of
industrial and scientific papers, combined with expert interview. Findings
showed that there are three main critical factor groups influencing on
compliance risk including challenges originating from Vietnamese food supply
chain itself, characteristics of regulation and standards, and business
environment. Furthermore, author proposed enablers to eliminate compliance
risks to food supply chain managers as well as recommendations to government
and other influencers and supporters.
"
Arbitrage-Free Interpolation in Models of Market Observable Interest Rates,"  Models which postulate lognormal dynamics for interest rates which are
compounded according to market conventions, such as forward LIBOR or forward
swap rates, can be constructed initially in a discrete tenor framework.
Interpolating interest rates between maturities in the discrete tenor structure
is equivalent to extending the model to continuous tenor. The present paper
sets forth an alternative way of performing this extension; one which preserves
the Markovian properties of the discrete tenor models and guarantees the
positivity of all interpolated rates.
"
Pricing of debt and equity in a financial network with comonotonic endowments,"  In this paper we present formulas for the valuation of debt and equity of
firms in a financial network under comonotonic endowments. We demonstrate that
the comonotonic setting provides a lower bound to the price of debt under
Eisenberg-Noe financial networks with consistent marginal endowments. Such
financial networks encode the interconnection of firms through debt claims. The
proposed pricing formulas consider the realized, endogenous, recovery rate on
debt claims. Special consideration will be given to the setting in which firms
only invest in a risk-free bond and a common risky asset following a geometric
Brownian motion.
"
Option Pricing in Illiquid Markets with Jumps,"  The classical linear Black--Scholes model for pricing derivative securities
is a popular model in financial industry. It relies on several restrictive
assumptions such as completeness, and frictionless of the market as well as the
assumption on the underlying asset price dynamics following a geometric
Brownian motion. The main purpose of this paper is to generalize the classical
Black--Scholes model for pricing derivative securities by taking into account
feedback effects due to an influence of a large trader on the underlying asset
price dynamics exhibiting random jumps. The assumption that an investor can
trade large amounts of assets without affecting the underlying asset price
itself is usually not satisfied, especially in illiquid markets. We generalize
the Frey--Stremme nonlinear option pricing model for the case the underlying
asset follows a Levy stochastic process with jumps. We derive and analyze a
fully nonlinear parabolic partial-integro differential equation for the price
of the option contract. We propose a semi-implicit numerical discretization
scheme and perform various numerical experiments showing influence of a large
trader and intensity of jumps on the option price.
"
Reverse Quantum Annealing Approach to Portfolio Optimization Problems,"  We investigate a hybrid quantum-classical solution method to the
mean-variance portfolio optimization problems. Starting from real financial
data statistics and following the principles of the Modern Portfolio Theory, we
generate parametrized samples of portfolio optimization problems that can be
related to quadratic binary optimization forms programmable in the analog
D-Wave Quantum Annealer 2000Q. The instances are also solvable by an
industry-established Genetic Algorithm approach, which we use as a classical
benchmark. We investigate several options to run the quantum computation
optimally, ultimately discovering that the best results in terms of expected
time-to-solution as a function of number of variables for the hardest instances
set are obtained by seeding the quantum annealer with a solution candidate
found by a greedy local search and then performing a reverse annealing
protocol. The optimized reverse annealing protocol is found to be more than 100
times faster than the corresponding forward quantum annealing on average.
"
Robust XVA,"  We introduce an arbitrage-free framework for robust valuation adjustments. An
investor trades a credit default swap portfolio with a risky counterparty, and
hedges credit risk by taking a position in the counterparty bond. The investor
does not know the expected rate of return of the counterparty bond, but he is
confident that it lies within an uncertainty interval. We derive both upper and
lower bounds for the XVA process of the portfolio, and show that these bounds
may be recovered as solutions of nonlinear ordinary differential equations. The
presence of collateralization and closeout payoffs leads to fundamental
differences with respect to classical credit risk valuation. The value of the
super-replicating portfolio cannot be directly obtained by plugging one of the
extremes of the uncertainty interval in the valuation equation, but rather
depends on the relation between the XVA replicating portfolio and the close-out
value throughout the life of the transaction.
"
The Price of BitCoin: GARCH Evidence from High Frequency Data,"  This is the first paper that estimates the price determinants of BitCoin in a
Generalised Autoregressive Conditional Heteroscedasticity framework using high
frequency data. Derived from a theoretical model, we estimate BitCoin
transaction demand and speculative demand equations in a GARCH framework using
hourly data for the period 2013-2018. In line with the theoretical model, our
empirical results confirm that both the BitCoin transaction demand and
speculative demand have a statistically significant impact on the BitCoin price
formation. The BitCoin price responds negatively to the BitCoin velocity,
whereas positive shocks to the BitCoin stock, interest rate and the size of the
BitCoin economy exercise an upward pressure on the BitCoin price.
"
Mean-Field Games with Differing Beliefs for Algorithmic Trading,"  Even when confronted with the same data, agents often disagree on a model of
the real-world. Here, we address the question of how interacting heterogenous
agents, who disagree on what model the real-world follows, optimize their
trading actions. The market has latent factors that drive prices, and agents
account for the permanent impact they have on prices. This leads to a large
stochastic game, where each agents' performance criteria is computed under a
different probability measure. We analyse the mean-field game (MFG) limit of
the stochastic game and show that the Nash equilibria is given by the solution
to a non-standard vector-valued forward-backward stochastic differential
equation. Under some mild assumptions, we construct the solution in terms of
expectations of the filtered states. We prove the MFG strategy forms an
\epsilon-Nash equilibrium for the finite player game. Lastly, we present a
least-squares Monte Carlo based algorithm for computing the optimal control and
illustrate the results through simulation in market where agents disagree on
the model.
"
Explicit solutions to utility maximization problems in a regime-switching market model via Laplace transforms,"  We study the problem of utility maximization from terminal wealth in which an
agent optimally builds her portfolio by investing in a bond and a risky asset.
The asset price dynamics follow a diffusion process with regime-switching
coefficients modeled by a continuous-time finite-state Markov chain. We
consider an investor with a Constant Relative Risk Aversion (CRRA) utility
function. We deduce the associated Hamilton-Jacobi-Bellman equation to
construct the solution and the optimal trading strategy and verify optimality
by showing that the value function is the unique constrained viscosity solution
of the HJB equation. By means of a Laplace transform method, we show how to
explicitly compute the value function and illustrate the method with the two-
and three-states cases. This method is interesting in its own right and can be
adapted in other applications involving hybrid systems and using other types of
transforms with basic properties similar to the Laplace transform.
"
Atomic Swaptions: Cryptocurrency Derivatives,"  The atomic swap protocol allows for the exchange of cryptocurrencies on
different blockchains without the need to trust a third-party. However, market
participants who desire to hold derivative assets such as options or futures
would also benefit from trustless exchange. In this paper I propose the atomic
swaption, which extends the atomic swap to allow for such exchanges. Crucially,
atomic swaptions do not require the use of oracles. I also introduce the margin
contract, which provides the ability to create leveraged and short positions.
Lastly, I discuss how atomic swaptions may be routed on the Lightning Network.
"
Portfolio diversification and model uncertainty: a robust dynamic mean-variance approach,"  This paper is concerned with a multi-asset mean-variance portfolio selection
problem under model uncertainty. We develop a continuous time framework for
taking into account ambiguity aversion about both expected return rates and
correlation matrix of the assets, and for studying the effects on portfolio
diversification. We prove a separation principle for the associated robust
control problem, which allows to reduce the determination of the optimal
dynamic strategy to the parametric computation of the minimal risk premium
function. Our results provide a justification for under-diversification, as
documented in empirical studies. We explicitly quantify the degree of
under-diversification in terms of correlation and Sharpe ratio ambiguity. In
particular, we show that an investor with a poor confidence in the expected
return estimation does not hold any risky asset, and on the other hand, trades
only one risky asset when the level of ambiguity on correlation matrix is
large. This extends to the continuous-time setting the results obtained by
Garlappi, Uppal and Wang [13], and Liu and Zeng [24] in a one-period model. JEL
Classification: G11, C61 MSC Classification: 91G10, 91G80, 60H30
"
Econometric modelling and forecasting of intraday electricity prices,"  In the following paper we analyse the ID$_3$-Price on German Intraday
Continuous Electricity Market using an econometric time series model. A
multivariate approach is conducted for hourly and quarter-hourly products
separately. We estimate the model using lasso and elastic net techniques and
perform an out-of-sample very short-term forecasting study. The model's
performance is compared with benchmark models and is discussed in detail.
Forecasting results provide new insights to the German Intraday Continuous
Electricity Market regarding its efficiency and to the ID$_3$-Price behaviour.
The supplementary materials are available online.
"
The effect of prudence on the optimal allocation in possibilistic and mixed models,"  In this paper two portfolio choice models are studied: a purely possibilistic
model, in which the return of a risky asset is a fuzzy number, and a mixed
model in which a probabilistic background risk is added. For the two models an
approximate formula of the optimal allocation is computed, with respect to the
possibilistic moments associated with fuzzy numbers and the indicators of the
investor risk preferences (risk aversion, prudence).
"
Bi-Demographic Changes and Current Account using SVAR Modeling,"  The paper, as a new contribution, aims to explore the impacts of
bi-demographic structure on the current account and growth. By using a SVAR
modeling, we track the dynamic impacts between the underlying variables of the
Saudi economy. New insights have been developed to study the interrelations
between population growth, current account and economic growth inside the
neoclassical theory of population. The long-run net impact on economic growth
of the bi-population growth is negative, due to the typically lower skill sets
of the immigrant labor population. Besides, the negative long-run contribution
of immigrant workers to the current account growth largely exceeds that of
contributions from the native population, because of the increasing levels of
remittance outflows from the country. We find that a positive shock in
immigration leads to a negative impact on native active age ratio. Thus, the
immigrants appear to be more substitutes than complements for native workers.
"
Temporal Logistic Neural Bag-of-Features for Financial Time series Forecasting leveraging Limit Order Book Data,"  Time series forecasting is a crucial component of many important
applications, ranging from forecasting the stock markets to energy load
prediction. The high-dimensionality, velocity and variety of the data collected
in these applications pose significant and unique challenges that must be
carefully addressed for each of them. In this work, a novel Temporal Logistic
Neural Bag-of-Features approach, that can be used to tackle these challenges,
is proposed. The proposed method can be effectively combined with deep neural
networks, leading to powerful deep learning models for time series analysis.
However, combining existing BoF formulations with deep feature extractors pose
significant challenges: the distribution of the input features is not
stationary, tuning the hyper-parameters of the model can be especially
difficult and the normalizations involved in the BoF model can cause
significant instabilities during the training process. The proposed method is
capable of overcoming these limitations by a employing a novel adaptive scaling
mechanism and replacing the classical Gaussian-based density estimation
involved in the regular BoF model with a logistic kernel. The effectiveness of
the proposed approach is demonstrated using extensive experiments on a
large-scale financial time series dataset that consists of more than 4 million
limit orders.
"
Shattering the glass ceiling? How the institutional context mitigates the gender gap in entrepreneurship,"  We examine how the institutional context affects the relationship between
gender and opportunity entrepreneurship. To do this, we develop a multi-level
model that connects feminist theory at the micro-level to institutional theory
at the macro-level. It is hypothesized that the gender gap in opportunity
entrepreneurship is more pronounced in low-quality institutional contexts and
less pronounced in high-quality institutional contexts. Using data from the
Global Entrepreneurship Monitor (GEM) and regulation data from the economic
freedom of the world index (EFW), we test our predictions and find evidence in
support of our model. Our findings suggest that, while there is a gender gap in
entrepreneurship, these disparities are reduced as the quality of the
institutional context improves.
"
Ranking Causal Influence of Financial Markets via Directed Information Graphs,"  A non-parametric method for ranking stock indices according to their mutual
causal influences is presented. Under the assumption that indices reflect the
underlying economy of a country, such a ranking indicates which countries exert
the most economic influence in an examined subset of the global economy. The
proposed method represents the indices as nodes in a directed graph, where the
edges' weights are estimates of the pair-wise causal influences, quantified
using the directed information functional. This method facilitates using a
relatively small number of samples from each index. The indices are then ranked
according to their net-flow in the estimated graph (sum of the incoming weights
subtracted from the sum of outgoing weights). Daily and minute-by-minute data
from nine indices (three from Asia, three from Europe and three from the US)
were analyzed. The analysis of daily data indicates that the US indices are the
most influential, which is consistent with intuition that the indices
representing larger economies usually exert more influence. Yet, it is also
shown that an index representing a small economy can strongly influence an
index representing a large economy if the smaller economy is indicative of a
larger phenomenon. Finally, it is shown that while inter-region interactions
can be captured using daily data, intra-region interactions require more
frequent samples.
"
Portfolio Optimization for Cointelated Pairs: SDEs vs. Machine Learning,"  We investigate the problem of dynamic portfolio optimization in
continuous-time, finite-horizon setting for a portfolio of two stocks and one
risk-free asset. The stocks follow the Cointelation model. The proposed
optimization methods are twofold. In what we call an Stochastic Differential
Equation approach, we compute the optimal weights using mean-variance criterion
and power utility maximization. We show that dynamically switching between
these two optimal strategies by introducing a triggering function can further
improve the portfolio returns. We contrast this with the machine learning
clustering methodology inspired by the band-wise Gaussian mixture model. The
first benefit of the machine learning over the Stochastic Differential Equation
approach is that we were able to achieve the same results though a simpler
channel. The second advantage is a flexibility to regime change.
"
An Enhanced Initial Margin Methodology to Manage Warehoused Credit Risk,"  The use of CVA to cover credit risk is widely spread, but has its
limitations. Namely, dealers face the problem of the illiquidity of instruments
used for hedging it, hence forced to warehouse credit risk. As a result,
dealers tend to offer a limited OTC derivatives market to highly risky
counterparties. Consequently, those highly risky entities rarely have access to
hedging services precisely when they need them most. In this paper we propose a
method to overcome this limitation. We propose to extend the CVA risk-neutral
framework to compute an initial margin (IM) specific to each counterparty,
which depends on the credit quality of the entity at stake, transforming the
effective credit rating of a given netting set to AAA, regardless of the credit
rating of the counterparty. By transforming CVA requirement into IM ones, as
proposed in this paper, an institution could rely on the existing mechanisms
for posting and calling of IM, hence ensuring the operational viability of this
new form of managing warehoused risk. The main difference with the currently
standard framework is the creation of a Specific Initial Margin, that depends
in the credit rating of the counterparty and the characteristics of the netting
set in question. In this paper we propose a methodology for such transformation
in a sound manner, and hence this method overcomes some of the limitations of
the CVA framework.
"
Economic Implications of Blockchain Platforms,"  In an economy with asymmetric information, the smart contract in the
blockchain protocol mitigates uncertainty. Since, as a new trading platform,
the blockchain triggers segmentation of market and differentiation of agents in
both the sell and buy sides of the market, it recomposes the asymmetric
information and generates spreads in asset price and quality between itself and
a traditional platform. We show that marginal innovation and sophistication of
the smart contract have non-monotonic effects on the trading value in the
blockchain platform, its fundamental value, the price of cryptocurrency, and
consumers' welfare. Moreover, a blockchain manager who controls the level of
the innovation of the smart contract has an incentive to keep it lower than the
first best when the underlying information asymmetry is not severe, leading to
welfare loss for consumers.
"
Target volatility option pricing in lognormal fractional SABR model,"  We examine in this article the pricing of target volatility options in the
lognormal fractional SABR model. A decomposition formula by Ito's calculus
yields a theoretical replicating strategy for the target volatility option,
assuming the accessibilities of all variance swaps and swaptions. The same
formula also suggests an approximation formula for the price of target
volatility option in small time by the technique of freezing the coefficient.
Alternatively, we also derive closed formed expressions for a small volatility
of volatility expansion of the price of target volatility option. Numerical
experiments show accuracy of the approximations in a reasonably wide range of
parameters.
"
Reframing the S\&P500 Network of Stocks along the \nth{21} Century,"  Since the beginning of the new millennium, stock markets went through every
state from long-time troughs, trade suspensions to all-time highs. The
literature on asset pricing hence assumes random processes to be underlying the
movement of stock returns. Observed procyclicality and time-varying correlation
of stock returns tried to give the apparently random behavior some sort of
structure. However, common misperceptions about the co-movement of asset prices
in the years preceding the \emph{Great Recession} and the \emph{Global
Commodity Crisis}, is said to have even fueled the crisis' economic impact.
Here we show how a varying macroeconomic environment influences stocks'
clustering into communities. From a sample of 296 stocks of the S\&P 500 index,
distinct periods in between 2004 and 2011 are used to develop networks of
stocks. The Minimal Spanning Tree analysis of those time-varying networks of
stocks demonstrates that the crises of 2007-2008 and 2010-2011 drove the market
to clustered community structures in both periods, helping to restore the stock
market's ceased order of the pre-crises era. However, a comparison of the
emergent clusters with the \textit{General Industry Classification Standard}
conveys the impression that industry sectors do not play a major role in that
order.
"
Affine processes under parameter uncertainty,"  We develop a one-dimensional notion of affine processes under parameter
uncertainty, which we call non-linear affine processes. This is done as
follows: given a set of parameters for the process, we construct a
corresponding non-linear expectation on the path space of continuous processes.
By a general dynamic programming principle we link this non-linear expectation
to a variational form of the Kolmogorov equation, where the generator of a
single affine process is replaced by the supremum over all corresponding
generators of affine processes with parameters in the parameter set. This
non-linear affine process yields a tractable model for Knightian uncertainty,
especially for modelling interest rates under ambiguity.
We then develop an appropriate Ito-formula, the respective term-structure
equations and study the non-linear versions of the Vasicek and the
Cox-Ingersoll-Ross (CIR) model. Thereafter we introduce the non-linear
Vasicek-CIR model. This model is particularly suitable for modelling interest
rates when one does not want to restrict the state space a priori and hence the
approach solves this modelling issue arising with negative interest rates.
"
Empirical Survival Jensen-Shannon Divergence as a Goodness-of-Fit Measure for Maximum Likelihood Estimation and Curve Fitting,"  The coefficient of determination, known as $R^2$, is commonly used as a
goodness-of-fit criterion for fitting linear models. $R^2$ is somewhat
controversial when fitting nonlinear models, although it may be generalised on
a case-by-case basis to deal with specific models such as the logistic model.
Assume we are fitting a parametric distribution to a data set using, say, the
maximum likelihood estimation method. A general approach to measure the
goodness-of-fit of the fitted parameters, which we advocate herein, is to use a
nonparametric measure for model comparison between the raw data and the fitted
model. In particular, for this purpose we put forward the {\em Survival
Jensen-Shannon divergence} ($SJS$) and its empirical counterpart (${\cal
E}SJS$) as a metric which is bounded, and is a natural generalisation of the
Jensen-Shannon divergence. We demonstrate, via a straightforward procedure
making use of the ${\cal E}SJS$, that it can be used as part of maximum
likelihood estimation or curve fitting as a measure of goodness-of-fit,
including the construction of a confidence interval for the fitted parametric
distribution. Furthermore, we show the validity of the proposed method with
simulated data, and three empirical data sets of interest to researchers in
sociophysics and econophysics.
"
How spread changes affect the order book: Comparing the price responses of order deletions and placements to trades,"  We observe the effects of the three different events that cause spread
changes in the order book, namely trades, deletions and placement of limit
orders. By looking at the frequencies of the relative amounts of price changing
events, we discover that deletions of orders open the bid-ask spread of a stock
more often than trades do. We see that once the amount of spread changes due to
deletions exceeds the amount of the ones due to trades, other observables in
the order book change as well. We then look at how these spread changing events
affect the prices of stocks, by means of the price response. We not only see
that the self-response of stocks is positive for both spread changing trades
and deletions and negative for order placements, but also cross-response to
other stocks and therefore the market as a whole. In addition, the
self-response function of spread-changing trades is similar to that of all
trades. This leads to the conclusion that spread changing deletions and order
placements have a similar effect on the order book and stock prices over time
as trades.
"
Game-Theoretic Capital Asset Pricing in Continuous Time,"  We derive formulas for the performance of capital assets in continuous time
from an efficient market hypothesis, with no stochastic assumptions and no
assumptions about the beliefs or preferences of investors. Our efficient market
hypothesis says that a speculator with limited means cannot beat a particular
index by a substantial factor. Our results include a formula that resembles the
classical CAPM formula for the expected simple return of a security or
portfolio.
This version of the article was essentially written in December 2001 but
remains a working paper.
"
Seasonal Stochastic Volatility and the Samuelson Effect in Agricultural Futures Markets,"  We introduce a multi-factor stochastic volatility model for commodities that
incorporates seasonality and the Samuelson effect. Conditions on the seasonal
term under which the corresponding volatility factor is well-defined are given,
and five different specifications of the seasonality pattern are proposed. We
calculate the joint characteristic function of two futures prices for different
maturities in the risk-neutral measure. The model is then presented under the
physical measure, and its state-space representation is derived, in order to
estimate the parameters with the Kalman filter for time series of corn, cotton,
soybean, sugar and wheat futures from 2007 to 2017. The seasonal model
significantly outperforms the nested non-seasonal model in all five markets,
and we show which seasonality patterns are particularly well-suited in each
case. We also confirm the importance of correctly modelling the Samuelson
effect in order to account for futures with different maturities. Our results
are clearly confirmed in a robustness check carried out with an alternative
dataset of constant maturity futures for the same agricultural markets.
"
An optimization approach to adaptive multi-dimensional capital management,"  Firms should keep capital to offer sufficient protection against the risks
they are facing. In the insurance context methods have been developed to
determine the minimum capital level required, but less so in the context of
firms with multiple business lines including allocation. The individual capital
reserve of each line can be represented by means of classical models, such as
the conventional Cramr-Lundberg model, but the challenge lies in soundly
modelling the correlations between the business lines. We propose a simple yet
versatile approach that allows for dependence by introducing a common
environmental factor. We present a novel Bayesian approach to calibrate the
latent environmental state distribution based on observations concerning the
claim processes. The calibration approach is adjusted for an environmental
factor that changes over time. The convergence of the calibration procedure
towards the true environmental state is deduced. We then point out how to
determine the optimal initial capital of the different business lines under
specific constraints on the ruin probability of subsets of business lines. Upon
combining the above findings, we have developed an easy-to-implement approach
to capital risk management in a multi-dimensional insurance risk model.
"
Multilevel nested simulation for efficient risk estimation,"  We investigate the problem of computing a nested expectation of the form
$\mathbb{P}[\mathbb{E}[X|Y]
\!\geq\!0]\!=\!\mathbb{E}[\textrm{H}(\mathbb{E}[X|Y])]$ where $\textrm{H}$ is
the Heaviside function. This nested expectation appears, for example, when
estimating the probability of a large loss from a financial portfolio. We
present a method that combines the idea of using Multilevel Monte Carlo (MLMC)
for nested expectations with the idea of adaptively selecting the number of
samples in the approximation of the inner expectation, as proposed by (Broadie
et al., 2011). We propose and analyse an algorithm that adaptively selects the
number of inner samples on each MLMC level and prove that the resulting MLMC
method with adaptive sampling has an $\mathcal{O}\left(
\varepsilon^{-2}|\log\varepsilon|^2 \right)$ complexity to achieve a root
mean-squared error $\varepsilon$. The theoretical analysis is verified by
numerical experiments on a simple model problem. We also present a stochastic
root-finding algorithm that, combined with our adaptive methods, can be used to
compute other risk measures such as Value-at-Risk (VaR) and Conditional
Value-at-Risk (CVaR), with the latter being achieved with
$\mathcal{O}\left(\varepsilon^{-2}\right)$ complexity.
"
Evaluating regulatory reform of network industries: a survey of empirical models based on categorical proxies,"  Proxies for regulatory reforms based on categorical variables are
increasingly used in empirical evaluation models. We surveyed 63 studies that
rely on such indices to analyze the effects of entry liberalization,
privatization, unbundling, and independent regulation of the electricity,
natural gas, and telecommunications sectors. We highlight methodological issues
related to the use of these proxies. Next, taking stock of the literature, we
provide practical advice for the design of the empirical strategy and discuss
the selection of control and instrumental variables to attenuate endogeneity
problems undermining identification of the effects of regulatory reforms.
"
A simple mathematical model for unemployment: a case study in Portugal with optimal control,"  We propose a simple mathematical model for unemployment. Despite its
simpleness, we claim that the model is more realistic and useful than recent
models available in the literature. A case study with real data from Portugal
supports our claim. An optimal control problem is formulated and solved, which
provides some non-trivial and interesting conclusions.
"
A Dynamic Model of Central Counterparty Risk,"  We introduce a dynamic model of the default waterfall of derivatives CCPs and
propose a risk sensitive method for sizing the initial margin (IM), and the
default fund (DF) and its allocation among clearing members. Using a Markovian
structure model of joint credit migrations, our evaluation of DF takes into
account the joint credit quality of clearing members as they evolve over time.
Another important aspect of the proposed methodology is the use of the time
consistent dynamic risk measures for computation of IM and DF. We carry out a
comprehensive numerical study, where, in particular, we analyze the advantages
of the proposed methodology and its comparison with the currently prevailing
methods used in industry.
"
Intensity estimation of transaction arrivals on the intraday electricity market,"  In the following paper we present a simple intensity estimation method of
transaction arrivals on the intraday electricity market. Assuming the
interarrival times distribution, we utilize a maximum likelihood estimation.
The method's performance is briefly tested using German Intraday Continuous
data. Despite the simplicity of the method, the results are encouraging. The
supplementary materials containing the R-codes and the data are attached to
this paper.
"
A bootstrap test to detect prominent Granger-causalities across frequencies,"  Granger-causality in the frequency domain is an emerging tool to analyze the
causal relationship between two time series. We propose a bootstrap test on
unconditional and conditional Granger-causality spectra, as well as on their
difference, to catch particularly prominent causality cycles in relative terms.
In particular, we consider a stochastic process derived applying independently
the stationary bootstrap to the original series. Our null hypothesis is that
each causality or causality difference is equal to the median across
frequencies computed on that process. In this way, we are able to disambiguate
causalities which depart significantly from the median one obtained ignoring
the causality structure. Our test shows power one as the process tends to
non-stationarity, thus being more conservative than parametric alternatives. As
an example, we infer about the relationship between money stock and GDP in the
Euro Area via our approach, considering inflation, unemployment and interest
rates as conditioning variables. We point out that during the period 1999-2017
the money stock aggregate M1 had a significant impact on economic output at all
frequencies, while the opposite relationship is significant only at high
frequencies.
"
Optimal make-take fees for market making regulation,"  We consider an exchange who wishes to set suitable make-take fees to attract
liquidity on its platform. Using a principal-agent approach, we are able to
describe in quasi-explicit form the optimal contract to propose to a market
maker. This contract depends essentially on the market maker inventory
trajectory and on the volatility of the asset. We also provide the optimal
quotes that should be displayed by the market maker. The simplicity of our
formulas allows us to analyze in details the effects of optimal contracting
with an exchange, compared to a situation without contract. We show in
particular that it leads to higher quality liquidity and lower trading costs
for investors.
"
The equivalence of two tax processes,"  We introduce two models of taxation, the latent and natural tax processes,
which have both been used to represent loss-carry-forward taxation on the
capital of an insurance company. In the natural tax process, the tax rate is a
function of the current level of capital, whereas in the latent tax process,
the tax rate is a function of the capital that would have resulted if no tax
had been paid. Whereas up to now these two types of tax processes have been
treated separately, we show that, in fact, they are essentially equivalent.
This allows a unified treatment, translating results from one model to the
other. Significantly, we solve the question of existence and uniqueness for the
natural tax process, which is defined via an integral equation. Our results
clarify the existing literature on processes with tax.
"
A factor-model approach for correlation scenarios and correlation stress-testing,"  In 2012, JPMorgan accumulated a USD~6.2 billion loss on a credit derivatives
portfolio, the so-called `London Whale', partly as a consequence of
de-correlations of non-perfectly correlated positions that were supposed to
hedge each other. Motivated by this case, we devise a factor model for
correlations that allows for scenario-based stress testing of correlations. We
derive a number of analytical results related to a portfolio of homogeneous
assets. Using the concept of Mahalanobis distance, we show how to identify
adverse scenarios of correlation risk. In addition, we demonstrate how
correlation and volatility stress tests can be combined. As an example, we
apply the factor-model approach to the ""London Whale"" portfolio and determine
the value-at-risk impact from correlation changes. Since our findings are
particularly relevant for large portfolios, where even small correlation
changes can have a large impact, a further application would be to stress test
portfolios of central counterparties, which are of systemically relevant size.
"
On the martingale property in the rough Bergomi model,"  We consider a class of fractional stochastic volatility models (including the
so-called rough Bergomi model), where the volatility is a superlinear function
of a fractional Gaussian process. We show that the stock price is a true
martingale if and only if the correlation $\rho$ between the driving Brownian
motions of the stock and the volatility is nonpositive. We also show that for
each $\rho<0$ and $m> \frac{1}{1-\rho^2}$, the $m$-th moment of the stock
price is infinite at each positive time.
"
A Backward Simulation Method for Stochastic Optimal Control Problems,"  A number of optimal decision problems with uncertainty can be formulated into
a stochastic optimal control framework. The Least-Squares Monte Carlo (LSMC)
algorithm is a popular numerical method to approach solutions of such
stochastic control problems as analytical solutions are not tractable in
general. This paper generalizes the LSMC algorithm proposed in Shen and Weng
(2017) to solve a wide class of stochastic optimal control models. Our
algorithm has three pillars: a construction of auxiliary stochastic control
model, an artificial simulation of the post-action value of state process, and
a shape-preserving sieve estimation method which equip the algorithm with a
number of merits including bypassing forward simulation and control
randomization, evading extrapolating the value function, and alleviating
computational burden of the tuning parameter selection. The efficacy of the
algorithm is corroborated by an application to pricing equity-linked insurance
products.
"
Classes of elementary function solutions to the CEV model. I,"  The CEV model subsumes some of the previous option pricing models. An
important parameter in the model is the parameter b, the elasticity of
volatility. For b=0, b=-1/2, and b=-1 the CEV model reduces respectively to the
BSM model, the square-root model of Cox and Ross, and the Bachelier model. Both
in the case of the BSM model and in the case of the CEV model it has become
traditional to begin a discussion of option pricing by starting with the
vanilla European calls and puts. In the case of BSM model simpler solutions are
the log and power solutions. These contracts, despite the simplicity of their
mathematical description, are attracting increasing attention as a trading
instrument. Similar simple solutions have not been studied so far in a
systematic fashion for the CEV model. We use Kovacic's algorithm to derive, for
all half-integer values of b, all solutions ""in quadratures"" of the CEV
ordinary differential equation. These solutions give rise, by separation of
variables, to simple solutions to the CEV partial differential equation. In
particular, when b=...,-5/2,-2,-3/2,-1, 1, 3/2, 2, 5/2,..., we obtain four
classes of denumerably infinite elementary function solutions, when b=-1/2 and
b=1/2 we obtain two classes of denumerably infinite elementary function
solutions, whereas, when b=0 we find two elementary function solutions. In the
derived solutions we have also dispensed with the unnecessary assumption made
in the the BSM model asserting that the underlying asset pays no dividends
during the life of the option.
"
A dynamic network model to measure exposure diversification in the Austrian interbank market,"  We propose a statistical model for weighted temporal networks capable of
measuring the level of heterogeneity in a financial system. Our model focuses
on the level of diversification of financial institutions; that is, whether
they are more inclined to distribute their assets equally among partners, or if
they rather concentrate their commitment towards a limited number of
institutions. Crucially, a Markov property is introduced to capture time
dependencies and to make our measures comparable across time. We apply the
model on an original dataset of Austrian interbank exposures. The temporal span
encompasses the onset and development of the financial crisis in 2008 as well
as the beginnings of European sovereign debt crisis in 2011. Our analysis
highlights an overall increasing trend for network homogeneity, whereby core
banks have a tendency to distribute their market exposures more equally across
their partners.
"
Fast mean-reversion asymptotics for large portfolios of stochastic volatility models,"  We consider a large portfolio limit where the asset prices evolve according
certain stochastic volatility models with default upon hitting a lower barrier.
When the asset prices and the volatilities are correlated via systemic Brownian
Motions, that limit exist and it is described by a SPDE on the positive
half-space with Dirichlet boundary conditions which has been studied in
\cite{HK17}. We study the convergence of the total mass of a solution to this
stochastic initial-boundary value problem when the mean-reversion coefficients
of the volatilities are multiples of a parameter that tends to infinity. When
the volatilities of the volatilities are multiples of the square root of the
same parameter, the convergence is extremely weak. On the other hand, when the
volatilities of the volatilities are independent of this exploding parameter,
the volatilities converge to their means and we can have much better
approximations. Our aim is to use such approximations to improve the accuracy
of certain risk-management methods in markets where fast volatility
mean-reversion is observed.
"
Game-theoretic dynamic investment model with incomplete information: futures contracts,"  Over the past few years, the futures market has been successfully developing
in the North-West region. Futures markets are one of the most effective and
liquid-visible trading mechanisms. A large number of buyers are forced to
compete with each other and raise their prices. A large number of sellers make
them reduce prices. Thus, the gap between the prices of offers of buyers and
sellers is reduced due to high competition, and this is a good criterion for
the liquidity of the market. This high degree of liquidity contributed to the
fact that futures trading took such an important role in commerce and finance.
A multi-step, non-cooperative n persons game is formalized and studied
"
Portfolio Optimization with Nondominated Priors and Unbounded Parameters,"  We consider classical Merton problem of terminal wealth maximization in
finite horizon. We assume that the drift of the stock is following
Ornstein-Uhlenbeck process and the volatility of it is following GARCH(1)
process. In particular, both mean and volatility are unbounded. We assume that
there is Knightian uncertainty on the parameters of both mean and volatility.
We take that the investor has logarithmic utility function, and solve the
corresponding utility maximization problem explicitly. To the best of our
knowledge, this is the first work on utility maximization with unbounded mean
and volatility in Knightian uncertainty under nondominated priors.
"
Replica Analysis for Maximization of Net Present Value,"  In this paper, we use replica analysis to determine the investment strategy
that can maximize the net present value for portfolios containing multiple
development projects. Replica analysis was developed in statistical mechanical
informatics and econophysics to evaluate disordered systems, and here we use it
to formulate the maximization of the net present value as an optimization
problem under budget and investment concentration constraints. Furthermore, we
confirm that a common approach from operations research underestimates the true
maximal net present value as the maximal expected net present value by
comparing our results with the maximal expected net present value as derived in
operations research. Moreover, it is shown that the conventional method for
estimating the net present value does not consider variance in the cash flow.
"
Accounting Noise and the Pricing of CoCos,"  Contingent Convertible bonds (CoCos) are debt instruments that convert into
equity or are written down in times of distress. Existing pricing models assume
conversion triggers based on market prices and on the assumption that markets
can always observe all relevant firm information. But all Cocos issued so far
have triggers based on accounting ratios and/or regulatory intervention. We
incorporate that markets receive information through noisy accounting reports
issued at discrete time instants, which allows us to distinguish between market
and accounting values, and between automatic triggers and regulator-mandated
conversions. Our second contribution is to incorporate that coupon payments are
contingent too: their payment is conditional on the Maximum Distributable
Amount not being exceeded. We examine the impact of CoCo design parameters,
asset volatility and accounting noise on the price of a CoCo; and investigate
the interaction between CoCo design features, the capital structure of the
issuing bank and their implications for risk taking and investment incentives.
Finally, we use our model to explain the crash in CoCo prices after Deutsche
Bank's profit warning in February 2016.
"
Exploiting Investors Social Network for Stock Prediction in China's Market,"  Recent works have shown that social media platforms are able to influence the
trends of stock price movements. However, existing works have majorly focused
on the U.S. stock market and lacked attention to certain emerging countries
such as China, where retail investors dominate the market. In this regard, as
retail investors are prone to be influenced by news or other social media,
psychological and behavioral features extracted from social media platforms are
thought to well predict stock price movements in the China's market. Recent
advances in the investor social network in China enables the extraction of such
features from web-scale data. In this paper, on the basis of tweets from
Xueqiu, a popular Chinese Twitter-like social platform specialized for
investors, we analyze features with regard to collective sentiment and
perception on stock relatedness and predict stock price movements by employing
nonlinear models. The features of interest prove to be effective in our
experiments.
"
The role of complex analysis in modeling economic growth,"  Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.
"
"Optimal Investment, Demand and Arbitrage under Price Impact","  This paper studies the optimal investment problem with random endowment in an
inventory-based price impact model with competitive market makers. Our goal is
to analyze how price impact affects optimal policies, as well as both pricing
rules and demand schedules for contingent claims. For exponential market makers
preferences, we establish two effects due to price impact: constrained trading,
and non-linear hedging costs. To the former, wealth processes in the impact
model are identified with those in a model without impact, but with constrained
trading, where the (random) constraint set is generically neither closed nor
convex. Regarding hedging, non-linear hedging costs motivate the study of
arbitrage free prices for the claim. We provide three such notions, which
coincide in the frictionless case, but which dramatically differ in the
presence of price impact. Additionally, we show arbitrage opportunities, should
they arise from claim prices, can be exploited only for limited position sizes,
and may be ignored if outweighed by hedging considerations. We also show that
arbitrage inducing prices may arise endogenously in equilibrium, and that
equilibrium positions are inversely proportional to the market makers'
representative risk aversion. Therefore, large positions endogenously arise in
the limit of either market maker risk neutrality, or a large number of market
makers.
"
Optimal Timing to Trade Along a Randomized Brownian Bridge,"  This paper studies an optimal trading problem that incorporates the trader's
market view on the terminal asset price distribution and uninformative noise
embedded in the asset price dynamics. We model the underlying asset price
evolution by an exponential randomized Brownian bridge (rBb) and consider
various prior distributions for the random endpoint. We solve for the optimal
strategies to sell a stock, call, or put, and analyze the associated delayed
liquidation premia. We solve for the optimal trading strategies numerically and
compare them across different prior beliefs. Among our results, we find that
disconnected continuation/exercise regions arise when the trader prescribe a
two-point discrete distribution and double exponential distribution.
"
Mean Reverting Portfolios via Penalized OU-Likelihood Estimation,"  We study an optimization-based approach to con- struct a mean-reverting
portfolio of assets. Our objectives are threefold: (1) design a portfolio that
is well-represented by an Ornstein-Uhlenbeck process with parameters estimated
by maximum likelihood, (2) select portfolios with desirable characteristics of
high mean reversion and low variance, and (3) select a parsimonious portfolio,
i.e. find a small subset of a larger universe of assets that can be used for
long and short positions. We present the full problem formulation, a
specialized algorithm that exploits partial minimization, and numerical
examples using both simulated and empirical price data.
"
Consumption smoothing in the working-class households of interwar Japan,"  I analyze Osaka factory worker households in the early 1920s, whether
idiosyncratic income shocks were shared efficiently, and which consumption
categories were robust to shocks. While the null hypothesis of full
risk-sharing of total expenditures was rejected, factory workers maintained
their households, in that they paid for essential expenditures (rent,
utilities, and commutation) during economic hardship. Additionally, children's
education expenditures were possibly robust to idiosyncratic income shocks. The
results suggest that temporary income is statistically significantly increased
if disposable income drops due to idiosyncratic shocks. Historical documents
suggest microfinancial lending and saving institutions helped mitigate
risk-based vulnerabilities.
"
How to model fake news,"  Over the past three years it has become evident that fake news is a danger to
democracy. However, until now there has been no clear understanding of how to
define fake news, much less how to model it. This paper addresses both these
issues. A definition of fake news is given, and two approaches for the
modelling of fake news and its impact in elections and referendums are
introduced. The first approach, based on the idea of a representative voter, is
shown to be suitable to obtain a qualitative understanding of phenomena
associated with fake news at a macroscopic level. The second approach, based on
the idea of an election microstructure, describes the collective behaviour of
the electorate by modelling the preferences of individual voters. It is shown
through a simulation study that the mere knowledge that pieces of fake news may
be in circulation goes a long way towards mitigating the impact of fake news.
"
Internal migration and education: A cross-national comparison,"  Migration the main process shaping patterns of human settlement within and
between countries. It is widely acknowledged to be integral to the process of
human development as it plays a significant role in enhancing educational
outcomes. At regional and national levels, internal migration underpins the
efficient functioning of the economy by bringing knowledge and skills to the
locations where they are needed. It is the multi-dimensional nature of
migration that underlines its significance in the process of human development.
Human mobility extends in the spatial domain from local travel to international
migration, and in the temporal dimension from short-term stays to permanent
relocations. Classification and measurement of such phenomena is inevitably
complex, which has severely hindered progress in comparative research, with
very few large-scale cross-national comparisons of migration. The linkages
between migration and education have been explored in a separate line of
inquiry that has predominantly focused on country-specific analyses as to the
ways in which migration affects educational outcomes and how educational
attainment affects migration behaviour. A recurrent theme has been the
educational selectivity of migrants, which in turn leads to an increase of
human capital in some regions, primarily cities, at the expense of others.
Questions have long been raised as to the links between education and migration
in response to educational expansion, but have not yet been fully answered
because of the absence, until recently, of adequate data for comparative
analysis of migration. In this paper, we bring these two separate strands of
research together to systematically explore links between internal migration
and education across a global sample of 57 countries at various stages of
development, using data drawn from the IPUMS database.
"
Quantifying macroeconomic expectations in stock markets using Google Trends,"  Among other macroeconomic indicators, the monthly release of U.S.
unemployment rate figures in the Employment Situation report by the U.S. Bureau
of Labour Statistics gets a lot of media attention and strongly affects the
stock markets. I investigate whether a profitable investment strategy can be
constructed by predicting the likely changes in U.S. unemployment before the
official news release using Google query volumes for related search terms. I
find that massive new data sources of human interaction with the Internet not
only improves U.S. unemployment rate predictability, but can also enhance
market timing of trading strategies when considered jointly with macroeconomic
data. My results illustrate the potential of combining extensive behavioural
data sets with economic data to anticipate investor expectations and stock
market moves.
"
Mid-price estimation for European corporate bonds: a particle filtering approach,"  In most illiquid markets, there is no obvious proxy for the market price of
an asset. The European corporate bond market is an archetypal example of such
an illiquid market where mid-prices can only be estimated with a statistical
model. In this OTC market, dealers / market makers only have access, indeed, to
partial information about the market. In real-time, they know the price
associated with their trades on the dealer-to-dealer (D2D) and dealer-to-client
(D2C) markets, they know the result of the requests for quotes (RFQ) they
answered, and they have access to composite prices (e.g., Bloomberg CBBT). This
paper presents a Bayesian method for estimating the mid-price of corporate
bonds by using the real-time information available to a dealer. This method
relies on recent ideas coming from the particle filtering (PF) / sequential
Monte-Carlo (SMC) literature.
"
Nonparametric Bayesian volatility learning under microstructure noise,"  Aiming at financial applications, we study the problem of learning the
volatility under market microstructure noise. Specifically, we consider noisy
discrete time observations from a stochastic differential equation and develop
a novel computational method to learn the diffusion coefficient of the
equation. We take a nonparametric Bayesian approach, where we model the
volatility function a priori as piecewise constant. Its prior is specified via
the inverse Gamma Markov chain. Sampling from the posterior is accomplished by
incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs
sampler. Good performance of the method is demonstrated on two representative
synthetic data examples. Finally, we apply the method on the EUR/USD exchange
rate dataset.
"
From Azma supermartingales of finite honest times to optional semimartingales of class-($$),"  Given a finite honest time, we derive representations for the additive and
multiplicative decomposition of it's Azma supermartingale in terms of
optional supermartingales and its running supremum. We then extend the notion
of semimartingales of class-$(\Sigma)$ to optional semimartingales with jumps
in its finite variation part, allowing one to establish formulas similar to the
Madan-Roynette-Yor option pricing formulas for larger class of processes.
Finally, we introduce the optional multiplicative systems associated with
positive submartingales and apply them to construct random times with given
Azma supermartingale.
"
Asymmetric Connectedness of Fears in the U.S. Financial Sector,"  We study how shocks to the forward-looking expectations of investors buying
call and put options transmit across the financial system. We introduce a new
contagion measure, called asymmetric fear connectedness (AFC), which captures
the information related to ""fear"" on the two sides of the options market and
can be used as a forward-looking systemic risk monitoring tool. The decomposed
connectedness measures provide timely predictive information for near-future
macroeconomic conditions and uncertainty indicators, and they contain
additional valuable information that is not included in the aggregate
connectedness measure. The role of a positive/negative ""fear""
transmitter/receiver emerges clearly when we focus more closely on
idiosyncratic events for financial institutions. We identify banks that are
predominantly positive/negative receivers of ""fear"", as well as banks that
positively/negatively transmit ""fear"" in the financial system.
"
Complex Valued Risk Diversification,"  Risk diversification is one of the dominant concerns for portfolio managers.
Various portfolio constructions have been proposed to minimize the risk of the
portfolio under some constrains including expected returns. We propose a
portfolio construction method that incorporates the complex valued principal
component analysis into the risk diversification portfolio construction. The
proposed method is verified to outperform the conventional risk parity and risk
diversification portfolio constructions.
"
Universal features of price formation in financial markets: perspectives from Deep Learning,"  Using a large-scale Deep Learning approach applied to a high-frequency
database containing billions of electronic market quotes and transactions for
US equities, we uncover nonparametric evidence for the existence of a universal
and stationary price formation mechanism relating the dynamics of supply and
demand for a stock, as revealed through the order book, to subsequent
variations in its market price. We assess the model by testing its
out-of-sample predictions for the direction of price moves given the history of
price and order flow, across a wide range of stocks and time periods. The
universal price formation model is shown to exhibit a remarkably stable
out-of-sample prediction accuracy across time, for a wide range of stocks from
different sectors. Interestingly, these results also hold for stocks which are
not part of the training sample, showing that the relations captured by the
model are universal and not asset-specific.
The universal model --- trained on data from all stocks --- outperforms, in
terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear
models trained on time series of any given stock, showing that the universal
nature of price formation weighs in favour of pooling together financial data
from various stocks, rather than designing asset- or sector-specific models as
commonly done. Standard data normalizations based on volatility, price level or
average spread, or partitioning the training data into sectors or categories
such as large/small tick stocks, do not improve training results. On the other
hand, inclusion of price and order flow history over many past observations is
shown to improve forecasting performance, showing evidence of path-dependence
in price dynamics.
"
